# Deploying and Using Huggingface embeddings

To host Hugging Face embeddings we are using the
[text-embeddings-inference](https://github.com/huggingface/text-embeddings-inference)
image by Hugging Face.

## How to deploy

Before deploying Hugging Face embeddings, we need to make sure that the vector size generated by the new embeddings is the same as pgvector vector size in `PGVECTOR_VECTOR_SIZE`. If not, we can
update the vector size by uncommenting it in `.core_backend.env` and updating it to the new
value (cf [Configuring AAQ](../../deployment/config-options.md)). Note that if the database is already set up using a different `PGVECTOR_VECTOR_SIZE` value, this will not work unless the database is
destroyed and created again.

To deploy huggingface embeddings, follow the deployment instructions in [Quick Setup](../../deployment/quick-setup.md) with the following changes

On **Step 4:**:

1. Edit `.litellm_proxy.env` by uncommenting `HUGGINGFACE_MODEL`, `HUGGINGFACE_EMBEDDINGS_API_KEY`
   and `HUGGINGFACE_EMBEDDINGS_ENDPOINT`. `HUGGINGFACE_MODEL` and `HUGGINGFACE_EMBEDDINGS_API_KEY` and
   should be updated according to need, but the default `HUGGINGFACE_EMBEDDINGS_ENDPOINT` should
   work with docker compose.

??? warning "If using an arm64 device, a docker image should be built locally before deployment."

    This can be done by running the make command: `make build-embeddings-arm`. Also,
    the variable `EMBEDDINGS_IMAGE_NAME` should be uncommented in `.core_backend.env`"

On **Step 6:** _Run docker-compose_, run the following command instead:

```bash
docker compose -f docker-compose.yml -f docker-compose.dev.yml \
    --profile huggingface-embeddings -p aaq-stack up -d --build
```

### Development environment setup

To [set up your development environment](../../develop/setup.md) with Hugging Face embeddings, you can start the container
manually by navigating to `ask-a-question` repository root and executing the following make command:

```bash
make setup-embeddings
```

If you are using an arm device, you can first build the image using:

```bash
make build-embeddings-arm
```

then:

```bash
make setup-embeddings-arm
```


!!! warning "Before running the commands above, you must export environment variables `HUGGINGFACE_MODEL` and `HUGGINGFACE_EMBEDDINGS_API_KEY`."

The embeddings API endpoint by default is at: http://localhost:8080.

---

## Use Hugging Face embeddings with LiteLLM Proxy

To use Hugging Face embeddings instead of OpenAI embeddings, you can replace OpenAI
embeddings in `litellm_proxy_config.yaml`.

This can be done by uncommenting the second embeddings model:

```
# - model_name: embeddings
#   litellm_params:
#     model: huggingface/huggingface-embeddings # model name not important
#     api_key: "os.environ/HUGGINGFACE_EMBEDDINGS_API_KEY" #pragma: allowlist secret
#     api_base: "os.environ/HUGGINGFACE_EMBEDDINGS_ENDPOINT"
```

The first embeddings model should be commented out unless using Hugging Face embeddings
as a back up to openai embeddings.

## Also see

1. [Quick Setup]("../../deployment/quick-setup.md")
2. [Configuring AAQ]("../../deployment/config-options.md")
