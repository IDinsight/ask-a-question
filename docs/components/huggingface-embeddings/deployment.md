# Deploying and Using Huggingface embeddings

To host huggingface embeddings we are using the
[text-embeddings-inference](https://github.com/huggingface/text-embeddings-inference)
by Hugging Face.

## How to deploy

Before deploying huggingface embeddings, we need to make sure that the vector size generated by the new embeddings is the same as pgvector vector size in `PGVECTOR_VECTOR_SIZE`. If not, we can
update the vector size by uncommenting it in `.core_backend.env` and updating it to the new
value. Note that if the database is already existing, this will not work unless the database is
destroyed and created again.

To deploy huggingface embeddings, follow the deployment instructions in [Quick Setup]("../../deployment/quick-setup.md").

On **Step 4:**:

1. Edit `.litellm_proxy.env` by uncommenting `HUGGINGFACE_MODEL`, `HUGGINGFACE_EMBEDDINGS_API_KEY`
   and `HUGGINGFACE_EMBEDDINGS_ENDPOINT`. `HUGGINGFACE_MODEL` and `HUGGINGFACE_EMBEDDINGS_API_KEY` and
   should be updated accordin to need, but the default `HUGGINGFACE_EMBEDDINGS_ENDPOINT` should
   work with docker compose.

??? warning "If using an arm64 device, a docker image should be built locally before deployment."

    This can be done by running the make command: `make build-embeddings-arm`. Also,
    the variable `EMBEDDINGS_IMAGE_NAME` should be uncommented in `.core_backend.env`"

On **Step 6:** _Run docker-compose_, run the following command instead:

```bash
docker compose -f docker-compose.yml -f docker-compose.dev.yml \
    --profile huggingface-embeddings -p aaq-stack up -d --build
```

### Dev setup

To [set up your development environment](../../develop/setup.md) with Hugging Face embeddings, you can start the container
manually using:

```bash
make setup-embeddings
```

If you are using an arm device, you can first build the image using:

```bash
make build-embeddings-arm
```

then:

```bash
make setup-embeddings-arm
```

??? info "Before running the commands above, you need to make sure to export the environment variables"

    This can be done using:

    ```bash
    export HUGGINGFACE_MODEL=<add-model-name>
    export HUGGINGFACE_EMBEDDINGS_API_KEY=<add-token>
    ```

The embeddings api endpoint by default is at: http://localhost:8080.

---

## Use huggingface embeddings

To use huggingface embeddings instead of OpenAI embeddings, you can replace OpenAI
embeddings in `litellm_proxy_config.yaml`.

This can be done by uncommenting the second embeddings model:

```
# - model_name: embeddings
#   litellm_params:
#     model: huggingface/huggingface-embeddings # model name not important
#     api_key: "os.environ/HUGGINGFACE_EMBEDDINGS_API_KEY" #pragma: allowlist secret
#     api_base: "os.environ/HUGGINGFACE_EMBEDDINGS_ENDPOINT"
```

The first embeddings model should be commented out unless using huggingface embeddings
as a back up to openai embeddings.

## Also see

1. [Quick Setup]("../../deployment/quick-setup.md")
2. [Configuring AAQ]("../../deployment/config-options.md")
