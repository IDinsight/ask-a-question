{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"No-code, easy-to-setup and reliable RAG plugin for chatbots <p> Ask A Question is a free and open-source tool created to help non-profit organizations, governments in developing nations, and social sector organizations utilize Large Language Models for responding to citizen inquiries in their native languages.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Responsible &amp; Safe AI    Implements guardrailed AI that is ethical, transparent, and secure</p> </li> <li> <p> Local Languages (Coming Soon)    Multilingual model support to enhance accessibility and experience</p> </li> <li> <p> Scalable &amp; Easy to Deploy   Containerized app that can be deployed anywhere with minimal setup</p> </li> <li> <p> Voice    Ask questions and receive answers using voice memos</p> </li> </ul> <p> LLM-powered search: Answers questions to database content using LLM embeddings.</p> <p> LLM responses : Craft a custom reponse to users using LLM chat</p> <p> Chat manager integration : Integrate with Turn.io, Glific, Typebot and more</p> <p> Manage content : Use the Admin App to add, edit, and delete content in the database</p> <p> Flag urgent messages : Identify messages that are urgent based on your rules</p> <p> See Full Roadmap  </p> <p>Looking for other features?</p> <p>If you are a developing country government, NGO or a social sector organisation, we'd love to hear what features you'd like to see. Raise an issue with <code>[FEATURE REQUEST]</code> before the title to start the conversation.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p> The project is funded by Google.org through the AI for Global Goals grant.</p> <p> Built and powered by IDinsight.</p> <p>IDinsight uses data and evidence to help leaders combat poverty worldwide. Our collaborations deploy a large analytical toolkit to help clients design better policies, rigorously test what works, and use evidence to implement effectively at scale. We place special emphasis on using the right tool for the right question, and tailor our rigorous methods to the real-world constraints of decision-makers. IDinsight works with governments, foundations, NGOs, multilaterals and businesses across Africa and Asia. We work in all major sectors including health, education, agriculture, governance, digital ID, financial access, and sanitation. We have offices in Dakar, Lusaka, Manila, Nairobi, New Delhi, Rabat, and Remote.  www.idinsight.org </p>"},{"location":"contact_us/","title":"Team","text":""},{"location":"contact_us/#point-of-contact","title":"Point of Contact","text":"<ul> <li><p> Carlos Samey    Tech Lead carlos.samey@idinsight.org </p></li> </ul>"},{"location":"contact_us/#full-team","title":"Full Team","text":"<ul> <li> <p><p> Sid Ravinutala     Chief Data Scientist sid.ravinutala@idinsight.org </p></p> </li> <li> <p><p> Suzin You   Data Scientist suzin.you@idinsight.org </p></p> </li> <li> <p><p> Amir Emami   Data Scientist amir.emami@idinsight.org </p></p> </li> <li> <p><p> Tony Zhao   Data Scientist tony.zhen@idinsight.org </p></p> </li> <li> <p><p> Mark Botterill   Data Scientist mark.botterill@idinsight.org </p></p> </li> </ul> <p>We are part of IDinsight's DSEM team</p> <p>Read more about DSEM and our work here.</p>"},{"location":"roadmap/","title":"Roadmap","text":"Quarter Feature Status Description Q4 2023 FastAPI Refactor Refactored to an all-components-in-one-repo codebase Embeddings-based search Match user questions to content in the database using embeddings Q1 2024 RAG responses Craft a custom response to the question using LLM based on retrieved content in the database Guardrails Keep LLM responses friendly and strictly context-based Q2 2024 Message Triaging Tag user messages with intents &amp; flag urgency Multi-user with Google log-in You can now have 1 AAQ deployment with multiple users, each with their own content DB Support for Turn.io, Glific integration Add AAQ to popular chat flow builders in social sector, like Turn.io and Glific Content Tags Add tags to your content for easy browsing (and more to come!) Q3 2024 Analytics for Feedback and Content See content use, questions that receive poor feedback, missing content, and more Voice notes support Automatic Speech Recognition for audio message to content matching Multi-turn chat Refine or clarify user question through conversation. Engineering Dashboard Monitor uptime, response rates, throughput HTTP response codes Q4 2024 Personalization and contextualization Use contextual information to improve responses Multimedia content Respond with not just text but images and audio as well. A/B Testing Test and decide content that works better for users <p>Key  : Completed  : Under development  : Queued : Yet to be scoped</p> <p>Beyond 2024</p> <ul> <li>Multi-tenant architecture</li> <li>User configurable Guardrails (in Admin app)</li> <li>Feedback-based model/LLM fine tuning</li> <li>Safety tests, fairness reports and ethical reviews</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>There are two ways to interact with the service:</p> <ol> <li>Accessing the API endpoints</li> <li>The Admin App</li> </ol>"},{"location":"usage/#api-endpoints","title":"API endpoints","text":"<p>To get answers from your database of contents, you can use the <code>/search</code> endpoint. This endpoint returns the following:</p> <ul> <li>Search results: Finds the most similar content in the database using cosine distance between embeddings.</li> <li>(Optionally) LLM generated response: Crafts a custom response using LLM chat using the most similar content.</li> </ul> <p>You can also add your contents programatically using API endpoints. See docs or SwaggerUI at <code>https://&lt;DOMAIN&gt;/api/docs</code> or <code>https://&lt;DOMAIN&gt;/docs</code> for more details and other API endpoints.</p>"},{"location":"usage/#embeddings-search","title":"Embeddings search","text":"<pre><code>curl -X 'POST' \\\n  'https://[DOMAIN]/api/search' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;BEARER TOKEN&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query_text\": \"how are you?\",\n  \"generate_llm_response\": false,\n  \"query_metadata\": {}\n}'\n</code></pre>"},{"location":"usage/#llm-response","title":"LLM response","text":"<pre><code>curl -X 'POST' \\\n  'https://[DOMAIN]/api/search' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;BEARER TOKEN&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query_text\": \"this is my question\",\n  \"generate_llm_response\": true,\n  \"query_metadata\": {}\n}'\n</code></pre>"},{"location":"usage/#admin-app","title":"Admin app","text":"<p>You can access the admin console at</p> <pre><code>https://[DOMAIN]/\n</code></pre> <p>On the Admin app, you can:</p> <ul> <li> Manage content and urgency rules</li> <li> Use the test sidebars to test the Question-Answering and Urgency Detection services</li> <li> View dashboards</li> </ul>"},{"location":"blog/","title":"Latest updates","text":""},{"location":"blog/2024/01/12/no-more-hallucinations/","title":"No more hallucinations","text":"<p>Last week we rolled out another safety feature - checking consistency of the response from the LLM with the content it is meant to be using to generate it. This shoud catch hallucinations or when LLM uses it's pre-training to answer a question. But it also catches any prompt injection or jailbreaking - if it somehow got through our other checks.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#checking-alignment-of-llm-response","title":"Checking alignment of LLM Response","text":"<p>Despite clear prompting, LLMs hallucinate. And sometimes use their large training set to answer a question instead of solely using the context provided.</p> <p>One of the endpoints that AAQ presents is Search. The process diagram on the page will be kept up to date on how the service works but here is what it looks like at the time of writing:</p> <p></p> <p>Steps 12 and its response, 13 check if the statements being generated by the LLM are consitent with the content it is meant to use to generate it.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#using-alignscore","title":"Using AlignScore","text":"<p>We can use GPT4-turbo and it does remarkably well on our test data. But there may be reasons - from data governance and privacy rules to costs - for not sending the data to OpenAI. One option is to use your own locally hosted LLM.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#doc-references","title":"Doc references","text":"<ul> <li>Locally hosted LLMs</li> <li>Search</li> </ul>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/","title":"Did someone say Dashboard?","text":"<p>Getting insights from your app has been made easy with the introduction of a questions dashboard page.</p>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/#content-dashboard","title":"Content dashboard","text":"<p>On the Admin App, you can now get statistics from your question answering service with the new dashboard displaying information such as:</p> <ul> <li>The number of questions answered during the current month</li> <li>The number of questions with positive feedback</li> <li>The number of questions asked during the past 6 months.</li> </ul> <p>The dashboard is in its first version and we are planning on adding more information such as Urgency Detection related statistics, and content related statistics. </p>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/#doc-references","title":"Doc references","text":"<ul> <li>QA Service</li> </ul>"},{"location":"blog/2024/06/17/new-feature-detected-tags/","title":"New feature detected: Tags","text":"<p>Filtering contents has been made easier with tags.</p>"},{"location":"blog/2024/06/17/new-feature-detected-tags/#manage-tags","title":"Manage tags","text":"<p>On the Admin App, it is now possible to add tags to contents to categorise them.</p> <p>You can create a tag, add it the content, or delete it directly within the edit content page. The \"Tags\" bar shown on the image below will allow you to quickly manage tags.</p> <p></p> <p>We also added the possibility of filtering contents using tags for a seemless content management experience. On the landing page, you can use the Tags bar to filter contents based on tags. </p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/","title":"Revamping the dashboard","text":"<p>In the last month or so, we've had support from Google fellows to pinch hit on a few items. Ahn Mac, a UI/UX designer at Google, helped design a new dashboard that we are very excited about. The first page, \"Overview\", is now ready for you to check out.</p> <p>Here's a screenshot:</p> <p></p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/#why-did-we-build-this","title":"Why did we build this?","text":"<p>The ethos of AAQ is that you care deeply about your users and are continuously refining and adding content to improve user experience. A live dashboard is the first step. It allows you to understand the usage of your solution, your users, and their feedback.</p> <p>Also, it wouldn't be an IDinsight product without a decent dashboard!</p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/#doc-references","title":"Doc references","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"blog/2024/08/14/a-new-ai-powered-dashboard-page/","title":"A new AI-powered dashboard page","text":"<p>Last month we rolled out the \"Overview\" page of the dashboard. Today, we are excited to share the \"Performance\" page. This includes a detailed view of the performance of your content, and AI-powered suggestions on how to improve it.</p> <p>Here's it is in action</p> <p></p>"},{"location":"blog/2024/08/14/a-new-ai-powered-dashboard-page/#why-did-we-build-this","title":"Why did we build this?","text":"<p>In a world with too many \"set-it-and-forget-it\" RAG solutions, we decided to build a solution that is designed to be continuously improved. We believe that a quality service requires deep engagement with the users and their feedback.</p> <p>The \"Performance\" page allows for just that. It lets you see which contents are being shared, and the feedback each is receiving. Using AI, we are able to summarize user feedback and suggest ways to improve the content.</p> <p>As before, a special thanks to Anh Mac from Google for the UI/UX design.</p>"},{"location":"blog/2024/08/14/a-new-ai-powered-dashboard-page/#doc-references","title":"Doc references","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/","title":"Revamped feedback endpoints","text":"<p>There are now two new endpoints for feedback:</p> <ol> <li><code>POST /response-feedback</code> - Allows you to capture feedback for the overall response returned by either of the Question-Answering APIs.</li> <li><code>POST /content-feedback</code> - Allows you to capture feedback for a specific piece of content.</li> </ol> <p>These can be used in chat managers to collect feedback after answers are shown.</p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#sentiment-and-text","title":"Sentiment and Text","text":"<p>For both of these endpoints, you are able to provide either sentiment (positive, negative) or text feedback, or both.</p> <p>See your deployment's OpenAPI documentation at <code>https://[DOMAIN]/api/docs</code> for more details.</p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#content-cards-show-feedback","title":"Content cards show feedback","text":"<p>The positive and negative feedback captured for the content can also be seen in the \"Read\" modal for each content card in the Admin App.</p> <p></p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#doc-references","title":"Doc references","text":"<ul> <li>Response Feedback</li> <li>Content Feedback</li> </ul>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/","title":"Tracing your AAQ calls with Langfuse","text":"<p>By integrating Langfuse, a popular LLM observability tool with a generous free tier, you can now track all LLM calls made via AAQ.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#whats-in-a-trace","title":"What's in a Trace?","text":"<p>With Langfuse enabled, AAQ is set up to trace each query to the <code>POST /search</code> endpoint.</p> <p></p> <p>Each query is represented as a Trace. If you click on the Trace ID, you can view the details of the trace. Here is an example for a <code>/search</code> call with <code>generate_llm_response</code> set to <code>true</code>:</p> <p></p> <p>On the right, there are Generations associated with this trace. In AAQ, each generation is each call to the LiteLLM Proxy Server. You can view the series of input checks, RAG (\"get_similar_content_async\" and \"openai/generate-response\") and one output check we perform (you can learn more about our Guardrails here). The generation names come from the model names used in your LiteLLM Proxy Server Config.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#why-does-aaq-need-observability","title":"Why does AAQ need observability?","text":"<p>As we begin piloting AAQ in various use cases, we wanted to be able to track LLM calls so that we can debug, analyze, and improve AAQ's question-answering ability. We are using it to test different prompt templates and guardrails. If you are interested in getting your hands dirty with AAQ's codebase, we imagine this will be useful to you. (Langfuse has a generous free tier and is self-hostable!)</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#so-how-do-i-use-it","title":"So how do I use it?","text":"<p>Sign up to Langfuse, and set the following environment variables in the backend app to get started.</p> <pre><code>export LANGFUSE=True\nexport LANGFUSE_PUBLIC_KEY=pk-...\nexport LANGFUSE_SECRET_KEY=sk-...\nexport LANGFUSE_HOST=https://cloud.langfuse.com # optional based on your Langfuse host\n</code></pre> <p>See more in Config options - Tracing with Langfuse.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#whats-next","title":"What's next?","text":"<p>We want to explore the rich set of features that Langfuse offers, such as evaluation and scoring. One concrete next step is to trace AAQ's Feedback endpoint using Langfuse's Scores.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#docs-references","title":"Docs references","text":"<ul> <li>Config options - Tracing with Langfuse</li> <li>Dev setup</li> <li>LiteLLM Proxy Server</li> </ul>"},{"location":"blog/2024/01/12/improved-docs/","title":"Improved docs!","text":"<p>First, we have added this section that you are currently reading. Each week we'll post what we've rolled out - new features, bug fixes, and performance improvements.</p> <p>The rest of the docs have now also been restructured to make it easy to parse.</p>"},{"location":"blog/2024/01/12/improved-docs/#now-with-cards","title":"Now with cards!","text":"<p>A lot of the index pages now show cards like the one shown below. These should make it easy to grasp the content in section in a glance.</p> <p></p>"},{"location":"blog/2024/01/12/improved-docs/#process-flow-diagrams","title":"Process Flow Diagrams","text":"<p>Search page now shows process flow diagrams. It should make it a lot easier to understand what is happening under the hood when you call either of these endopints.</p> <p></p>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/","title":"Adding a model proxy server","text":"<p>Instead of being handled directly in our code, our model calls are now routed through a LiteLLM Proxy server. This lets us change models on the fly and have retries, fallbacks, budget tracking, and more.</p>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#why-change","title":"Why change?","text":"<p>LiteLLM Proxy introduces a streamlined approach for handling various large language models (LLMs) through a single interface. We can now manage model endpoints and configurations via a proxy server, replacing the previous method of hard-coding them into the environment of our app.</p> <p>The benefits of this setup:</p> <ul> <li>Simplifies codebase by centralizing the model configurations</li> <li>Provides the flexibility to switch or update models without altering the core application logic - we can even add and remove models through the proxy's API!</li> <li>Multiple instances of AAQ can use the same model server</li> </ul> <p>We now configure models in a <code>config.yaml</code> file, allowing the proxy to route requests to different LLMs (commercial or self-hosted - full list here). See <code>deployment/docker-compose/litellm_config.yaml</code> for an example.</p> <p>The LiteLLM Proxy server also has some extra useful features:</p> <ul> <li>Consistent input/output formats across different models</li> <li>Fallback mechanisms for error handling</li> <li>Detailed logging and connectivity to Langfuse and others</li> <li>Tracking of token usage and spending.</li> <li>Asynchronous handling of requests and caching.</li> </ul>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#any-downsides","title":"Any downsides?","text":"<p>Potential downsides include:</p> <ul> <li>Dependency on the LiteLLM project for updates for new models and parameters</li> <li>A possible increase in latency</li> <li>A new Docker container in our stack which, despite its name, it not \"lite\"!</li> </ul>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#docs-references","title":"Docs references","text":"<ul> <li>LLM Proxy Server</li> <li>Dev setup</li> </ul>"},{"location":"blog/2024/03/22/hello-material-ui/","title":"Hello Material UI","text":"<p>We've switched to MaterialUI: Cleaner, easier to build and maintain, more familiar.</p>"},{"location":"blog/2024/03/22/hello-material-ui/#why-change","title":"Why change?","text":"<p>Design best practices are \"best practices\" for having a proven track record of creating engaging, intuitive, and effective user experiences. Google has encapsulated this in the creation of MaterialUI \u2014 a design language well-infused with the principles of good design. Embracing these principles, we're thrilled to announce a significant shift in our project's Admin App: the transition to using MaterialUI components.</p> <p>This shift is a step towards cleaner maintainable code and a commitment to providing simpler and more familiar user experiences.</p> Old UI MUI Main page Edit page <p>We're already starting to see some benefits of moving to Material UI:</p> <ul> <li> <p>Cleaner Codebase:   MaterialUI components are modular and customizable, and has allowed us to achieve a polished look with less custom CSS.</p> </li> <li> <p>Easier to Build and Maintain:   MaterialUI's extensive component library has significantly reduced our development time. Components like buttons, nav bars, and switches come with a variety of options that are easily customizable.</p> </li> <li> <p>Familiarity and Consistency:   MaterialUI is a design language familiar to millions of users worldwide, thanks to Google's widespread implementation across its products. This is helping us lower the learning curve for new users and is ensuring consistency across devices.</p> </li> </ul>"},{"location":"blog/2024/02/09/nginx-out-caddy-in/","title":"Nginx out, Caddy in","text":"<p>By swapping out Nginx for Caddy, we substantially simplified the deployment steps and the architecture - which means fewer docker containers to run and manage.</p> <p>Previously, we were using NGINX and then manually running a script to issue certificates from Let's Encrypt. We were also running a container to refresh Let\u2019s Encrypt certificates. And then sharing volumes between this container and the nginx container. This article from 2018 shows you the setup. It was a bit of a mess.</p> <p>The other issue was that this process wouldn't work when running locally - for example when you are developing. Your domain would be <code>localhost</code> and Let's Encrypt can't issue certificates for it. So we had to come up with a different process for local dev where we were issuing self-signed certs.</p>"},{"location":"blog/2024/02/09/nginx-out-caddy-in/#welcome-caddy","title":"Welcome Caddy","text":"<p>Oscar, our Google.org AI advisor's first advice when he saw our architecture was to switch to Caddy. Here are the benefits:</p> <ol> <li>It requests and refresh certs from Let's Encrypt.</li> <li>If your domain is localhost, it knows to issue its own certificate.</li> <li>A much smaller and simpler config file.</li> <li>You can use environment variables.</li> </ol> <p>So now our local setup process is the same as prod and requires one fewer containers. Amazing!</p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/","title":"Check out the new Playground","text":"<p>Admin app now has a new Playground page where you can test out the FAQ matching and LLM response endpoints!</p> <p>Here's a screenshot:</p> <p></p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/#why-did-we-build-this","title":"Why did we build this?","text":"<p>Content managers can now test out how the retrieval APIs will perform when new content is added - without leaving the Admin App.</p> <p>By clicking on <code>&lt;json&gt;</code> at the bottom, they can also see the raw JSON response sent back by the server. This include debugging information that may be useful in understanding behaviour.</p> <p></p>"},{"location":"blog/2024/03/19/ditching-qdrant-for-pgvector/","title":"Ditching Qdrant for PgVector","text":"<p>In our latest infrastructure update, we decided to transition from Qdrant to pgvector for managing our vector databases. This move is part of our ongoing effort to reduce cost and simplify AAQ\u2019s architecture.</p> <p>This means: - we no longer require a separate Qdrant server. With the pgvector extension, vector data can now be stored in PostgreSQL along with other transactional data. - Seamless transactions between vector data and transactional data will allow for the integration of new features such as multilingual support.</p>"},{"location":"blog/2024/03/19/ditching-qdrant-for-pgvector/#why-this-change","title":"Why this change?","text":"<p>While Qdrant served us well as a dedicated vector database, integrating it with our existing PostgreSQL setup introduced complexity and maintenance overhead. Operating Qdrant alongside PostgreSQL meant managing two distinct database systems with their own infrastructure, architecture, and requirements. Integrating both databases into our codebase required additional integration layers, complicating our codebase.</p> <p>As we already were using Postgresql, pgvector caught our attention with this article as a promising solution that integrates vector database capabilities directly into our existing database. Here's why we decided to go for pgvector: - Simplified Architecture: By adopting pgvector, we significantly reduced the complexity of our data infrastructure. Vector and relational data now reside within the same database, eliminating the need for separate systems. - Improved Response Time: Direct integration with PostgreSQL enhances performance by eliminating the overhead of communicating between separate databases.</p> <p>Moving to pgvector not only benefits our team in terms of reduced complexity and better resource utilization but also lays the groundwork for future innovations such as the multilingual support which should be coming soon.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/","title":"One Endpoint to Rule Them All","text":"<p>We refactored our two question-answering endpoints into a single one called <code>/search</code> for clarity and ease of use.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#why-change","title":"Why change?","text":"<p>We realised our two <code>/embeddings-search</code> and <code>/llm-response</code> endpoints were a bit confusing, and since they performed very similar tasks we combined them into one for ease of use.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#whats-new","title":"What's new?","text":"<p>We now have a single endpoint called <code>/search</code> which always returns the top contents in the database that most closely matched your query and can optionally return an LLM-generated resposne to your question.</p> <p></p> <p>We've also simplified the parameters in our response model so it's easier to use. We will now respond with something like this:</p> <pre><code>{\n    \"query_id\": 1,\n    \"llm_response\": \"Example LLM response \"\n    \"(null if generate_llm_response is False)\",\n    \"search_results\": {\n        \"0\": {\n            \"title\": \"Example content title\",\n            \"text\": \"Example content text\",\n            \"id\": 23,\n            \"distance\": 0.1,\n        },\n        \"1\": {\n            \"title\": \"Another example content title\",\n            \"text\": \"Another example content text\",\n            \"id\": 12,\n            \"distance\": 0.2,\n        },\n    },\n    \"feedback_secret_key\": \"secret-key-12345-abcde\",\n    \"debug_info\": {\"example\": \"debug-info\"},\n}\n</code></pre>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#docs-references","title":"Docs references","text":"<ul> <li>Search</li> </ul>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/","title":"Turn.io Integration Now Available: Your Turn to Play with it","text":"<p>As a question-answering plugin service, Ask A Question requires what we call \"chat managers\" - platforms that help you build end-user chat flows and integrate with channels like WhatsApp.</p> <p>To make this integration process easier, we published a Turn.io Playbook. Playbooks are reusable, pre-built chat flows that can be shared with and remixed by other teams in the chat-for-impact community.</p> <p>Find it here. You may need to sign up to see it.</p>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/#how-do-i-use-it","title":"How do I use it?","text":"<p>Just click on \"Activate Journey\" and you will see this flow imported in your Journeys.</p> <p>Next, replace the <code>&lt;API_KEY&gt;</code> in the code card with your own API key and you should be good to go. If you are using your own deployment, you will also need to change the URL of the API call.</p>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/#stuck","title":"Stuck?","text":"<p>Learn all about Playbooks on Turn's official blog.</p> <p>And don't hesitate to write to us at aaq@idinsight.org!</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/","title":"Introducing Urgency Detection","text":"<p>You may wish to handle urgent messages differently. For example, when deploying a question answering service in a health context, you may wish to refer the user to their nearest health center, or escalate it immediately to a human operator.</p> <p>We introduce a new endpoint and new page in the Admin App to enable this.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#defining-urgency-rules","title":"Defining urgency rules","text":"<p>Using the Admin App, you can define your rules in natural language. For example, here are a few rules borrowed directly from the CDC website on Urgent Maternal Warning Signs:</p> <p></p> <p>It's as simple as that. You don't need to train a model (though you can if you want to. See \"Or write your own\" below).</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#using-the-urgency-detection-endpoint","title":"Using the urgency detection endpoint","text":"<p>You should refer to your Swagger UI/OpenAPI documentation for details but here is a screenshot for us lazy ones:</p> <p></p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#pick-your-method","title":"Pick your method","text":"<p>As of this blog post, there are two ways to determine urgency:</p> <ol> <li>Cosine Distance - It uses the cosine distance between the    input and the urgency rules to determine urgency. It's simple and fast, but may not be    as accurate as the next method.</li> <li>LLM Entailment - This calls an LLM to determine if the message matches the    urgency rules. It's more accurate, but slower. Also, since it's making a call to the LLM, it    is more expensive than the cosine distance method.</li> </ol> <p>See setup sections on how to configure these.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#or-write-your-own","title":"Or write your own","text":"<p>May be you are not happy with either of these and want to try out that new entailment model. All you need to do is define your function with this signature:</p> <pre><code>@urgency_classifier\nasync def your_fancy_method(\n    asession: AsyncSession,\n    urgency_query: UrgencyQuery,\n) -&gt; UrgencyResponse:\n</code></pre> <p>and you can update the <code>URGENCY_CLASSIFIER</code> environment variable to <code>your_fancy_method</code>.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#using-a-different-model","title":"Using a different model","text":"<p>Reminder that we setup a proxy server to make it easy to switch between models. If you want to use a different model, you can host it and update the <code>LITELLM_MODEL_URGENCY_DETECT</code> environment variable to point to your model.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#doc-references","title":"Doc references","text":"<ul> <li>LLM Proxy Server</li> <li>Urgency Detection</li> <li>Managing Urgency Rules</li> </ul>"},{"location":"blog/2024/09/09/from-thought-to-speech-introducing-aaqs-new-voice-service/","title":"From Thought to Speech: Introducing AAQ's New Voice Service","text":"<p>We're excited to unveil our new End-to-End Speech Service for AAQ! This powerful component allows you to seamlessly ask your questions on your device and recieve a spoken response back. Powered by advanced AI it integrates Text-to-Speech and Speech-to-Text capabilities, allowing you to easily create dynamic voice experiences through AAQ.</p> <p>Take a look at it it in action, integrating with a typical WhatsApp chat via the Typebot chatflow manager:</p> <p></p> <p>A demonstration using the E2E speech pipeline to ask AAQ \"What should I eat during pregnancy?\""},{"location":"blog/2024/09/09/from-thought-to-speech-introducing-aaqs-new-voice-service/#why-did-we-build-this","title":"Why did we build this?","text":"<p>As voice-driven interactions become the norm for providing accessibility and ease of use, we recognized the need to extend beyond text-based solutions.</p> <p>In many developing countries, voice communication is often preferred over text for a variety of reasons. Voice is inherently more intuitive, making it accessible to individuals with limited exposure to technology. Additionally, in regions with lower literacy rates, relying on text-based interactions can present barriers. By focusing on voice, we provide a solution that feels natural, immediate, and inclusive.</p> <p>Our in-house Speech Service uses cutting-edge AI models, enabling real-time speech synthesis and transcription with remarkable accuracy across multiple languages, giving users a seamless and consistent voice experience.</p>"},{"location":"blog/2024/09/09/from-thought-to-speech-introducing-aaqs-new-voice-service/#how-it-works-the-voice-search-endpoint","title":"How It Works: The <code>/voice-search</code> Endpoint","text":"<p>At the heart of AAQ's speech capabilities lies the <code>/voice-search</code> endpoint. This powerful API is the gateway to accessing our Speech Service. Here's a glimpse into how it operates:</p> <p></p> <p>The <code>/voice-search</code> endpoint expects a URL of a voice file stored in any public cloud storage. It processes this audio input and returns a structured response, enabling enabling hassle-free integration of voice interactions into your applications.</p> <p>Here's an example of the response model returned by the endpoint:</p> <pre><code>{\n    \"query_id\": 1,\n    \"llm_response\": \"Example LLM response \",\n    \"search_results\": {\n        \"0\": {\n            \"title\": \"Example content title\",\n            \"text\": \"Example content text\",\n            \"id\": 23,\n            \"distance\": 0.1,\n        },\n        \"1\": {\n            \"title\": \"Another example content title\",\n            \"text\": \"Another example content text\",\n            \"id\": 12,\n            \"distance\": 0.2,\n        },\n    },\n    \"feedback_secret_key\": \"secret-key-12345-abcde\",\n    \"debug_info\": {\"example\": \"debug-info\"},\n    \"tts_filepath\": \"https://storage.googleapis.com/example-bucket/random_uuid_filename.mp3\"\n}\n</code></pre> <p>This straightforward yet powerful interface allows developers to easily incorporate voice search capabilities into their AAQ-powered applications, opening up a world of possibilities for voice-driven interactions.</p>"},{"location":"blog/2024/09/09/from-thought-to-speech-introducing-aaqs-new-voice-service/#future-enhancements","title":"Future Enhancements","text":"<p>We're committed to continuously improving our Speech Service. Here are some exciting changes on our roadmap:</p> <ol> <li>Faster Inference Times: We're working on optimizing our AI models and infrastructure to reduce latency and provide even quicker responses.</li> <li>Expanded Language Support: We aim to serve underrepresented languages, enhancing accessibility for global users by significantly increasing our language offerings.</li> <li>Enhanced Customization Options: We aim to introduce more customization features, allowing users to fine-tune voice characteristics like accent, speed, and emotion to better suit their specific use cases.</li> </ol> <p>Stay tuned for these updates as we strive to make our Speech Service even more powerful and user-friendly!</p>"},{"location":"blog/2024/09/09/from-thought-to-speech-introducing-aaqs-new-voice-service/#docs-references","title":"Docs references","text":"<ul> <li>Voice Service</li> </ul>"},{"location":"components/","title":"Components","text":"<p>In this section you can find the different components within AAQ.</p>"},{"location":"components/#user-facing-components","title":"User-facing Components","text":"<p>There are 5 main components in Ask-A-Question.</p> <ul> <li> <p> The Admin App</p> <p>Manage content in the database. Manage urgency detection rules. Test the service's performance. Explore usage dashboards.</p> <p> More info</p> </li> <li> <p> Workspaces</p> <p>Create dedicated workspaces for your data and users.</p> <p> More info</p> </li> <li> <p> Multi-turn Chat</p> <p>Engage in multi-turn question-answering sessions with your data.</p> <p> More info</p> </li> <li> <p> The Question-Answering Service</p> <p>Integrate with these API endpoints to answer questions from your users.</p> <p> More info</p> </li> <li> <p> The Urgency Detection Service</p> <p>Integrate with this API endpoint to tag messages as urgent or not.</p> <p> More info</p> </li> </ul>"},{"location":"components/#internal-components","title":"Internal Components","text":"<ul> <li> <p> Model Proxy Server</p> <p>AAQ uses the LiteLLM Proxy Server for managing LLM and embedding calls, allowing you to use any LiteLLM supported model (including self-hosted ones).</p> <p> More info</p> </li> <li> <p> Self-hosted Hugging Face Embeddings Model</p> <p>(Optional) A dockerised Hugging Face text embeddings model to create vectors   and perform vector search.</p> <p> More info</p> </li> <li> <p> Voice Service</p> <p>(Optional) Supports both in-house and cloud-based solutions for Automatic Speech Recognition (ASR) and Text-to-Speech (TTS)</p> <p> More info</p> </li> </ul>"},{"location":"components/admin-app/","title":"The Admin App","text":"<ul> <li> <p> Question Answering Contents</p> <p>Allows you to view, create, edit, delete, or test content.</p> <p> More info</p> </li> <li> <p> Urgency Detection Rules</p> <p>Allows you to view, create, edit, delete, or test urgency rules.</p> <p> More info</p> </li> <li> <p> Dashboard</p> <p>Allows you to see statistics like which content is most frequently being used, or the feedback from users on responses provided by the service.</p> <p> More info</p> </li> </ul>"},{"location":"components/admin-app/#accessing-the-admin-app","title":"Accessing the Admin app","text":"<p>If you have the application running, you can access the admin app at:</p> <pre><code>https://[DOMAIN]/\n</code></pre> <p>or if you are using the dev setup:</p> <pre><code>http://localhost:3000/\n</code></pre>"},{"location":"components/admin-app/dashboard/","title":"Dashboard","text":"<p>The Dashboard provides real-time analytics on the performance of your solution. There are four time filters available: Last 24 hours, Last week, Last month, and Last year.</p> <p>The dashboard is divided into three sections: Overview, Performance, and Content Gaps.</p>"},{"location":"components/admin-app/dashboard/#overview","title":"Overview","text":"<p>This landing page of the dashboard provides a high-level summary of the performance of your solution.</p> <p></p>"},{"location":"components/admin-app/dashboard/#performance","title":"Performance","text":"<p>This section show how well your content is performing. You can sort the content by the number of times it was shared with your users, upvotes or downvotes, and even the trend (is it getting more or less popular).</p> <p></p> <p>Clicking on the any of the content in the table opens a detailed view of the content's performance. This also provides an AI generated summary of the feedback users have provided and suggestion on how to improve the content.</p> <p></p>"},{"location":"components/admin-app/dashboard/#ai-features","title":"AI Features","text":"<p>Several parts of the dashboard use AI to provide LLM summaries and accurate topic labels. However, if data privacy is a concern a version of the dashboard that requires zero LLM calls (while maintaing all other features) can be activated by setting <code>DISABLE_DASHBOARD_LLM=True</code> in your core_backend.env</p>"},{"location":"components/admin-app/dashboard/#content-gaps","title":"Content Gaps","text":"<p> Stay tuned for the \"Content Gaps\" section.</p>"},{"location":"components/admin-app/dashboard/#want-to-see-more","title":"Want to see more?","text":"<p>Is there a metric or a feature you'd like to see on the dashboard? We'd love to hear from you. Contact us!</p>"},{"location":"components/admin-app/question-answering/","title":"Managing Content used for Question Answering","text":"<p>The Admin app allows you to view, add, edit, or delete content in the database. It also allows you to test the service with questions to ensure the correct responses are fetched before releasing the service to your users.</p> <p>Once logged in, you should see the following screen:</p> <p></p> <p>When you click the \"Test\" button, you can use the sidebar to try your questions.</p> <p></p>"},{"location":"components/admin-app/question-answering/#upcoming-features","title":"Upcoming features","text":"<ul> <li> Add multiple languages for each content</li> <li> Allow metadata to be captured for each content</li> </ul>"},{"location":"components/admin-app/urgency-rules/","title":"Managing Urgency Rules","text":"<p>The Admin app allows you to view, add, edit, or delete  in the database. It also allows you to test the service with messages to ensure the classifications seem correct.</p> <p>Once logged in, navigate to \"Urgency Detection\" from the menu and you should see the following screen</p> <p></p> <p>You can add, edit, delete, or test urgency rules from this screen.</p> <p>When you click the \"Test\" button, you can use the sidebar to try your messages.</p> <p></p>"},{"location":"components/huggingface-embeddings/","title":"Hugging Face Embeddings","text":"<p>Get more control of your data by using open-source embeddings from Hugging Face</p> <p>By default, AAQ uses OpenAI text embeddings to generate text vectors. But we give the option of deploying and hosting custom Hugging Face embedding models.</p> <p>Note</p> <p>Hugging Face has an extensive list of embeddings models, some lightweight and some heavy. Please carefully choose the model that will fit your use case best.</p> <ul> <li> <p> How to use</p> <p>Deploying the custom Hugging Face embeddings model and configuring AAQ to use Hugging Face embeddings.</p> <p> More info</p> </li> </ul>"},{"location":"components/huggingface-embeddings/how-to-use/","title":"How to use Hugging Face embeddings","text":"<p>To host Hugging Face embeddings we use text-embeddings-inference image by Hugging Face.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#prerequisite-steps","title":"Prerequisite steps","text":""},{"location":"components/huggingface-embeddings/how-to-use/#step-0-update-your-litellm-proxy-config","title":"Step 0. Update your LiteLLM Proxy config","text":"<p>To use Hugging Face embeddings instead of OpenAI embeddings, you can replace OpenAI embeddings in <code>litellm_proxy_config.yaml</code>.</p> <p>This can be done by uncommenting the second embeddings model:</p> <pre><code># - model_name: embeddings\n#   litellm_params:\n#     model: huggingface/huggingface-embeddings # model name not important\n#     api_key: \"os.environ/HUGGINGFACE_EMBEDDINGS_API_KEY\" #pragma: allowlist secret\n#     api_base: \"os.environ/HUGGINGFACE_EMBEDDINGS_ENDPOINT\"\n</code></pre> <p>The first embeddings model should be commented out unless using Hugging Face embeddings as a back up to OpenAI embeddings.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#step-1-set-pgvector_vector_size-environment-variable","title":"Step 1. Set <code>PGVECTOR_VECTOR_SIZE</code> environment variable","text":"<p>Make sure that <code>PGVECTOR_VECTOR_SIZE</code> is set to be the vector size generated by your Hugging Face embedding of choice. This should be set in <code>.core_backend.env</code> (cf. Configuring AAQ).</p> <p>Note that if the database is already set up using a different <code>PGVECTOR_VECTOR_SIZE</code> value, this will not work unless the database is destroyed and created again.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#deploying-hugging-face-embeddings","title":"Deploying Hugging Face Embeddings","text":"<p>Make sure you've performed the prerequisite steps before proceeding.</p> <p>To deploy Hugging Face embeddings, follow the deployment instructions in Quick Setup with the following additional steps</p> <p>On Step 4: Configure LiteLLM Proxy server, edit <code>.litellm_proxy.env</code> by setting the following variables:</p> <ul> <li><code>HUGGINGFACE_MODEL</code>: your Hugging Face model of choice.</li> <li><code>HUGGINGFACE_EMBEDDINGS_API_KEY</code>: API key for Hugging Face Embeddings API</li> <li><code>HUGGINGFACE_EMBEDDINGS_ENDPOINT</code>: API endpoint URL for Hugging Face Embeddings     container. The default value should work with docker compose.</li> </ul> If using an arm64 device, a docker image should be built locally before deployment. <p>This can be done by running the make command: <code>make build-embeddings-arm</code>. Also, the variable <code>EMBEDDINGS_IMAGE_NAME</code> should be uncommented in <code>.core_backend.env</code>.</p> <p>On Step 6: Run docker-compose, add <code>--profile huggingface-embeddings</code> to the docker compose command:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    --profile huggingface-embeddings -p aaq-stack up -d --build\n</code></pre>"},{"location":"components/huggingface-embeddings/how-to-use/#setting-up-hugging-face-embeddings-for-development","title":"Setting up Hugging Face embeddings for development","text":"<p>Make sure you've performed the prerequisite steps before proceeding.</p> <p>To set up your development environment with Hugging Face embeddings, you can start the container manually by navigating to <code>ask-a-question</code> repository root and executing the following make command:</p> <pre><code>make setup-embeddings\n</code></pre> <p>If you are using an arm device, you can first build the image using:</p> <pre><code>make build-embeddings-arm\n</code></pre> <p>then:</p> <pre><code>make setup-embeddings-arm\n</code></pre> <p>Before running the commands above, you must export environment variables <code>HUGGINGFACE_MODEL</code> and <code>HUGGINGFACE_EMBEDDINGS_API_KEY</code>.</p> <p>The embeddings API endpoint by default is at: http://localhost:5000.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#also-see","title":"Also see","text":"<ol> <li>Quick Setup</li> <li>Configuring AAQ</li> </ol>"},{"location":"components/litellm-proxy/","title":"LLM Proxy Server","text":""},{"location":"components/litellm-proxy/#what-is-it","title":"What is it?","text":"<p>AAQ uses the LiteLLM Proxy Server for managing LLM calls, allowing you to use any LiteLLM supported model including self-hosted ones.</p> <p>This proxy server runs as a separate Docker container with configs read from a <code>config.yaml</code> file, where you can set the appropriate model names and endpoints for each LLM task.</p>"},{"location":"components/litellm-proxy/#example-config","title":"Example config","text":"<p>You can see an example <code>litellm_proxy_config.yaml</code> file below. In our backend code, we refer to the models by their custom task <code>model_name</code> (e.g. \"generate-response\"), but which actual LLM model each call is routed to is set here.</p> <pre><code>model_list:\n  - model_name: embeddings\n    litellm_params:\n      model: text-embedding-ada-002\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: default\n    litellm_params:\n      model: gpt-4-0125-preview\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: generate-response\n    litellm_params:\n      model: gpt-4-0125-preview\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: detect-language\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: translate\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: paraphrase\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: safety\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: alignscore\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\nlitellm_settings:\n  num_retries: 3\n  request_timeout: 100\n  telemetry: False\n</code></pre> <p>See the Contributing Setup and Docker Compose Setup for how this service is run in our stack.</p>"},{"location":"components/litellm-proxy/#also-see","title":"Also see","text":"<ul> <li>Latest Updates: Adding a model proxy server</li> </ul>"},{"location":"components/multi-turn-chat/","title":"Multi-turn Chat","text":"<p>The multi-turn chat endpoint, <code>/chat</code>, allows you to engage in multi-turn conversations with your data. This endpoint manages your chat session, including the context of the conversation and the history of questions and answers, and integrates directly with the <code>/search</code> endpoint to provide contextualized answers to your questions. An API key is required for this endpoint since an LLM is used to generate contextualized responses based on the conversation history.</p>"},{"location":"components/multi-turn-chat/#overview","title":"Overview","text":"<p>Multi-turn conversations allow users to continue their dialogue with the LLM agent and receive responses that are contextualized on the conversation history. Each conversation is referenced by a unique <code>session_id</code> and consists of user messages and appropriate LLM responses. We use Redis, an in-memory data store, to retrieve and update the conversation history for a given session each time the <code>/chat</code> endpoint is invoked.</p>"},{"location":"components/multi-turn-chat/#multi-turn-conversation-procedure","title":"Multi-turn Conversation Procedure","text":"<ol> <li>Users send their question to the <code>/chat</code> endpoint. If the request does not include a session ID, then a random session ID will be generated for the chat session. This unique ID is tied to the user for the duration of the conversation session.</li> <li>The <code>session_id</code> is used to initialize the chat history as follows:<ol> <li>If the session exists in Redis, then the existing conversation history is    retrieved and the new message from the user is appended to the conversation.</li> <li>If the session does not exist, then a new conversation history is started. The   new conversation will have a default system message that serves as a guideline   for the LLM behavior during the chat process (detailed below) and the first user   message. Thus, existing conversation histories will also have the same default   system message as a guideline.</li> </ol> </li> <li>At a minimum, all conversation histories will include a default system message and at least one user message. In general, conversation histories will include the default system message and the interleaved messages and responses between the user and the LLM agent.</li> <li>The default system message instructs the LLM to generate a query to retrieve information from a vector database that contains information that can be used to answer the user\u2019s question/concern. In addition, the LLM is also instructed to determine the Type of Message as follows:<ol> <li>Follow-up Message: These are messages that build upon the conversation so far   and/or seeks more clarifying information on a previously discussed   question/concern.</li> <li>New Message: These are messages that introduces a new topic that was not   previously discussed in the conversation.</li> </ol> </li> <li>Based on the Type of Message, the LLM then generates a suitable query to execute against the vector database in order to retrieve relevant information that can answer the user\u2019s question/concern.<ol> <li>Upon receiving the information from the vector database, we can either present    the information as is (with optional re-ranking) or use the LLM to choose the    top N most relevant pieces of information and perform abstractive/extractive    summarization.</li> </ol> </li> <li>The final response from the LLM is presented to the user and the conversation history is updated in Redis for the next invocation of the /chat endpoint.</li> </ol>"},{"location":"components/multi-turn-chat/#how-multi-turn-conversations-are-managed","title":"How Multi-turn Conversations are Managed","text":"<ol> <li>Initialize the Redis client: A Redis client is initialized and is used to cache the conversation history for each session.</li> <li>Initialize the conversation history: The conversation history can be explicitly initialized each time the /chat endpoint is invoked. The Redis client and the <code>session_id</code> is required to initialize the conversation history. We can also reset the conversation history before continuing the conversation.</li> <li>Initialize the chat parameters for the session: Text generation parameters for the LLM model responsible for managing the multi-turn chat session are initialized.</li> <li>Start/continue the conversation: The user then proceeds to start a new conversation or continue the conversation from a previous session. With each turn, the conversation history is updated in Redis and the latest response is sent back to the user.</li> </ol>"},{"location":"components/qa-service/","title":"Question-Answering Service","text":"<p><code>/search</code> is the flagship endpoint that your application can integrate with:</p> <ul> <li> <p> Search</p> <p>Allows searching through the content database using embeddings similarity and optionally creates an LLM-generated answer to the user's question.</p> <p> More info</p> </li> </ul> <p><code>/Voice-search</code> is the Optional End to End Speech endpoint that your application can integrate with:</p> <ul> <li> <p> Voice-Search</p> <p>Enables voice-based searching by accepting an audio file URL. Supports both in-house and Cloud based Solutions for transcription and Speech Synthesis. Returns an LLM-generated response as both text and an audio file URL.</p> <p> More info</p> </li> </ul>"},{"location":"components/qa-service/#capturing-feedback","title":"Capturing Feedback","text":"<p>The service also provides two endpoint to capture feedback from users. One to capture feedback for the response returned and the other for the content items retrieved.</p> <p>For both of these, you can provide feedback as sentiment (\"positive\", \"negative\"), as text, or as both.</p> <ul> <li> <p> Response Feedback</p> <p>Allows users to provide feedback on the response generated by the either Semantic Search or LLM Response.</p> <p> More info</p> </li> <li> <p> Content Feedback</p> <p>Allows users to provide feedback on the content items returned by the Semantic Search.</p> <p> More info</p> </li> </ul>"},{"location":"components/qa-service/#swaggerui","title":"SwaggerUI","text":"<p>If you have the application running, you can access the SwaggerUI at</p> <pre><code>https://[DOMAIN]/api/docs\n</code></pre> <p>or if you are using the dev setup:</p> <pre><code>http://localhost:8000/docs\n</code></pre>"},{"location":"components/qa-service/#upcoming","title":"Upcoming","text":"<ul> <li> Chat capability</li> </ul>"},{"location":"components/qa-service/content-feedback/","title":"Content Feedback","text":"<p>This service allows you to provide feedback for individual content items in the database. The feedback can be used to improve the quality of the content and the search results.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/response-feedback/","title":"Response Feedback","text":"<p>This service captures feedback for the response return by Semantic Search or LLM Response.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/search/","title":"Search","text":"<p>This service returns the contents from the database with the most similar vector embeddings to the question and optionally also uses an LLM to construct a custom answer to the user's question using the retrieved contents.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/search/#process-flow-without-llm-response-generation","title":"Process flow without LLM response generation","text":"<pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Translate text\n  LLM-&gt;&gt;AAQ: &lt;Translated text&gt;\n  AAQ-&gt;&gt;LLM: Paraphrase question\n  LLM-&gt;&gt;AAQ: &lt;Paraphrased question&gt;\n  AAQ-&gt;&gt;Vector DB: Request M most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;M contents with similarity score&gt;\n  AAQ-&gt;&gt;Cross-encoder: Re-rank to get top N contents\n  Cross-encoder-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;User: Return JSON of N contents\n</code></pre>"},{"location":"components/qa-service/search/#process-flow-with-llm-response-generation","title":"Process flow with LLM response generation","text":"<pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Check for safety\n  LLM-&gt;&gt;AAQ: &lt;Safety Classification&gt;\n  AAQ-&gt;&gt;Vector DB: Request N most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;Cross-encoder: Re-rank to get top N contents\n  Cross-encoder-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;LLM: Given contents, construct response in user's language to question\n  LLM-&gt;&gt;AAQ: &lt;LLM response&gt;\n  AAQ-&gt;&gt;LLM: Check if LLM response is consistent with contents\n  LLM-&gt;&gt;AAQ: &lt;Consistency score&gt;\n  AAQ-&gt;&gt;User: Return JSON of LLM response and N contents\n</code></pre>"},{"location":"components/qa-service/voice-search/","title":"Voice-Search","text":"<p>This service accepts an audio file URL, transcribes the speech to text, processes the query similar to the text-based search, and returns both a text response and an audio response URL.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/voice-search/#process-flow-for-end-to-end-speech-generation","title":"Process flow for End to End Speech Generation","text":"<pre><code>sequenceDiagram\n    autonumber\n    User-&gt;&gt;AAQ: Audio file URL\n    AAQ-&gt;&gt;Cloud Storage: save audio to Cloud Storage\n    AAQ-&gt;&gt;Speech-to-Text: Transcribe audio\n    Speech-to-Text-&gt;&gt;AAQ: Transcribed text\n    AAQ-&gt;&gt;LLM: Identify language\n    LLM-&gt;&gt;AAQ: &lt;Language&gt;\n    AAQ-&gt;&gt;LLM: Paraphrase question\n    LLM-&gt;&gt;AAQ: &lt;Paraphrased question&gt;\n    AAQ-&gt;&gt;Vector DB: Request N most similar contents in DB\n    Vector DB-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n    AAQ-&gt;&gt;Cross-encoder: Re-rank to get top N contents\n    Cross-encoder-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n    AAQ-&gt;&gt;LLM: Generate response based on top N contents\n    LLM-&gt;&gt;AAQ: &lt;Text response&gt;\n    AAQ-&gt;&gt;Text-to-Speech: Convert text response to audio\n    AAQ-&gt;&gt;Cloud Storage: Save audio file to Cloud Storage\n    Text-to-Speech-&gt;&gt;AAQ: Audio file URL\n    AAQ-&gt;&gt;User: Return JSON with text response, audio URL, and N contents</code></pre>"},{"location":"components/qa-service/voice-search/#voice-service-integration","title":"Voice Service Integration","text":"<p>For detailed information on how to integrate voice capabilities into your application using AAQ, including setup instructions for both in-house and cloud-based speech services, please refer to our Voice Service documentation. This documentation covers:</p> <ul> <li>Setting up the dockerized container for in-house ASR and TTS models</li> <li>Configuring Google Cloud Speech-to-Text and Text-to-Speech integration</li> <li>Best practices for voice input and output in your application</li> </ul> <p> Explore Voice Service Documentation</p>"},{"location":"components/urgency-detection/","title":"Urgency Detection","text":"<p>This service returns if the the message is urgent or not. There are currently two methods available to do this.</p>"},{"location":"components/urgency-detection/#method-1-cosine-distance","title":"Method 1: Cosine distance","text":"<ul> <li>Cost: </li> <li>Accuracy: </li> <li>Latency: </li> </ul> <p>This method uses the cosine distance between the input message and the urgency rules in the database. Since it only uses embeddings, it is fast and cheap to run.</p>"},{"location":"components/urgency-detection/#setup","title":"Setup","text":"<p>Set the following environment variables.</p> <ol> <li>Set <code>URGENCY_CLASSIFIER</code> environment variable to <code>cosine_distance_classifier</code>.</li> <li>Set <code>URGENCY_DETECTION_MAX_DISTANCE</code> environment variable. Any message with a cosine distance greater than this value will be tagged as urgent.</li> </ol> <p>You can do this either in the <code>.env</code> file or under <code>core_backend/app/urgency_detection/config.py</code>. See Configuring AAQ for more details.</p>"},{"location":"components/urgency-detection/#method-2-llm-entailment-classifier","title":"Method 2: LLM entailment classifier","text":"<ul> <li>Cost: </li> <li>Accuracy: </li> <li>Latency: </li> </ul> <p>This method calls an LLM to score the message against each of the urgency rules in the database.</p>"},{"location":"components/urgency-detection/#setup_1","title":"Setup","text":"<p>Set the following environment variables.</p> <ol> <li>Set <code>URGENCY_CLASSIFIER</code> environment variable to <code>llm_entailment_classifier</code>.</li> <li>Set <code>URGENCY_DETECTION_MIN_PROBABILITY</code> environment variable. The LLM returns the probability of a message being urgent. Any message with a probability greater than this value will be tagged as urgent.</li> </ol> <p>You can do this either in the <code>.env</code> file or under <code>core_backend/app/urgency_detection/config.py</code>. See Configuring AAQ for more details.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/urgency-detection/#more-details","title":"More details","text":"<ul> <li>Blog post on Urgency Detection.</li> <li>SwaggerUI</li> </ul>"},{"location":"components/voice-service/","title":"Voice Service","text":"<p>The Voice Service component provides voice interaction capabilities within the AAQ system. It supports both speech recognition (STT) and text-to-speech (TTS) functionalities through two primary methods:</p> <ol> <li>In-House Models: Utilize in-house, dockerized models for STT and TTS to process speech data locally.</li> <li>External APIs: Integrate with Google Cloud's Speech-to-Text and Text-to-Speech APIs for enhanced flexibility and accuracy.</li> </ol> <p>This documentation will guide you through setting up, configuring, and using the Voice Service in various scenarios.</p> <p>Note: To enable the <code>/voice-search</code> endpoint in the question-answer service, you need to set the <code>TOGGLE_VOICE</code> environment variable in <code>.core_backend.env</code> (cf. Configuring AAQ)</p> To use the speech service for manual setup and testing, you must install <code>ffmpeg</code> on your system. <ul> <li>Guide for MacOS</li> <li>Guide for Windows</li> <li>Guide for Linux</li> </ul> Using a combination of internal and external models <p>You have the flexibility to use both internal and external models simultaneously by setting the environment variables accordingly. If one of the environment variables is not set, the system will automatically default to the external model. For information on configuring and using external models, refer to our External APIs and In-house Models guide.</p> <ul> <li> <p> Using In-House Models</p> <p>Follow the steps to set up and use the in-house STT and TTS models.</p> <p> More info</p> </li> <li> <p> Using External APIs</p> <p>Learn how to integrate Google Cloud's Speech-to-Text and Text-to-Speech services.</p> <p> More info</p> </li> </ul>"},{"location":"components/voice-service/external-apis/","title":"How to use External Speech models","text":"<p>This guide outlines the process for using the external Google Cloud Speech-to-Text and Text-to-Speech speech-to-text and text-to-speech models within AAQ.</p>"},{"location":"components/voice-service/external-apis/#prerequisite-steps","title":"Prerequisite steps","text":""},{"location":"components/voice-service/external-apis/#configure-environment-variables","title":"Configure environment variables","text":"<p>To access the in-house models, ensure that the <code>CUSTOM_TTS_ENDPOINT</code> and <code>CUSTOM_STT_ENDPOINT</code> environment variables are not set (i.e. blank).</p> <p>You will also need to set the <code>GOOGLE_APPLICATION_CREDENTIALS</code> environment variable and make sure you have the <code>.gcp_credentials.json</code> file so that the you can access Google Cloud Services. These should be configured in the <code>.core_backend.env</code> and <code>.litellm_proxy.env</code> files respectively (cf. Configuring AAQ).</p>"},{"location":"components/voice-service/external-apis/#using-external-speech-models-in-deployment","title":"Using External Speech Models in Deployment","text":"<p>To deploy external speech models, simply follow the deployment instructions in the QuickSetup. No additional steps are needed.</p>"},{"location":"components/voice-service/external-apis/#setting-up-external-models-for-development","title":"Setting up External Models for Development","text":"<p>Follow these steps to set up your development environment for external speech models.</p> <p>Note: To use the Manual Setup method, you will need to add your <code>gcp_credentials</code> file manually in your local environment as below:</p> <ol> <li> <p>Place your <code>gcp_credentials.json</code> file inside the <code>core_backend/</code> folder.</p> </li> <li> <p>Run <code>export GOOGLE_APPLICATION_CREDENTIALS=\"core_backend/credentials.json\"</code> to set the environment variable.</p> </li> <li> <p>While in the root of the directory, run <code>python core_backend/main.py</code>.</p> </li> </ol> Do not navigate to <code>core_backend</code> folder using <code>cd core_backend</code> <p>If you do this, then you will also have to adjust <code>GOOGLE_APPLICATION_CREDENTIALS</code> to be <code>\"/credentials.json\"</code> as it's relative to your terminal.</p>"},{"location":"components/voice-service/external-apis/#additional-resources","title":"Additional Resources","text":"<ol> <li>How to use in-house speech models</li> <li>Quick Setup with Docker Compose</li> <li>Setting up your development environment</li> </ol>"},{"location":"components/voice-service/in-house-models/","title":"How to use in-house speech models","text":"<p>This guide outlines the process for hosting and using our custom in-house Speech-to-Text and Text-to-Speech models using our specialized Docker image.</p>"},{"location":"components/voice-service/in-house-models/#prerequisite-steps","title":"Prerequisite steps","text":""},{"location":"components/voice-service/in-house-models/#configure-environment-variables","title":"Configure environment variables","text":"<p>To properly set the <code>CUSTOM_TTS_ENDPOINT</code> and <code>CUSTOM_STT_ENDPOINT</code> environment variables, open the <code>.core_backend.env</code> file and locate the lines for these variables. If they're commented out, uncomment them and ensure their values are set to the correct endpoint URLs for your in-house TTS and STT models (cf. Configuring AAQ).</p>"},{"location":"components/voice-service/in-house-models/#using-in-house-speech-models-in-deployment","title":"Using In-house Speech Models in Deployment","text":"<p>To deploy in-house speech models, follow the deployment instructions in the QuickSetup with this additional step:</p> <p>In \"Step 5: Run docker-compose\", append <code>docker-compose.speech.yml -p</code> to the docker compose command as below:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -f \\\ndocker-compose.speech.yml -p aaq-stack up -d --build\n</code></pre>"},{"location":"components/voice-service/in-house-models/#setting-up-in-house-models-for-development","title":"Setting Up In-house Models for Development","text":"<p>Currently the in-house models only work with the Docker Compose Watch dev setup. Use the following command:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -f \\\ndocker-compose.speech.yml -p aaq-stack up -d --build\n</code></pre>"},{"location":"components/voice-service/in-house-models/#additional-resources","title":"Additional Resources","text":"<ol> <li>How to use External Speech models</li> <li>Quick Setup with Docker Compose</li> <li>Setting up your development environment</li> </ol>"},{"location":"components/workspaces/","title":"Workspaces","text":"<p>A workspace is a dedicated virtual environment that contains its own set of data and users. Workspaces allow you to create isolated environments for different projects or teams, and assign users with different roles to each workspace. This is useful for managing access to sensitive data, or for creating separate environments for development, testing, and production.</p>"},{"location":"components/workspaces/#background","title":"Background","text":"<p>Previous implementation of AAQ assigns every user as an admin user with read/write privileges. Each user is assigned to their own environment and can only access the contents (along with feedback, urgency rules, etc.) in their environment. In order for User 1 to share their contents with User 2, User 1 must give User 2 their credentials and, as a result, User 2 would have the same read/write privileges with User 1\u2019s content.</p> <p>The scenario above is undesirable for the following reasons:</p> <ol> <li> <p>Security risks: Sharing content requires sharing credentials. User 1 must share their credentials with every single user that they want to share their content with.</p> </li> <li> <p>Data risks: Sharing content with other users means that those users have admin privileges with the data. Each user can freely add, modify, and delete content without limitation.</p> </li> <li> <p>Resource sharing: When User 1 shares their environment with User 2, User 2 will use the same API daily quota and content quota as User 1. In addition, User 2 is free to make calls that uses LLMs without any constraints.</p> </li> </ol> <p>An ideal solution is one which addresses all of the issues above. That is, the solution should distinguish between different types of users, isolate content to its own dedicated environment, and better manage resource sharing. To accomplish this, our solution is as follows.</p>"},{"location":"components/workspaces/#workspace-solution","title":"Workspace Solution","text":"<p>A workspace is an isolated virtual environment that contains contents that can be accessed and modified by users assigned to that workspace. Workspaces must be unique but can contain duplicated content. Users can be assigned to one or more workspaces, with different roles in each workspace. In other words, there is a many-to-many relationship between users and workspaces.</p> <p>Users do not have assigned quotas or API keys; rather, a user's API keys and quotas are tied to those of the workspaces they belong to. Furthermore, users must be unique across all workspaces.</p> <p>There are currently 2 different types of users:</p> <ol> <li> <p>Read-Only: These users are assigned to workspaces and can only read the contents within their assigned workspaces. They cannot modify existing contents or add new contents to their workspaces, add or delete users from their workspaces, or delete workspaces. However, read-only users can create new workspaces.</p> </li> <li> <p>Admin: These users are assigned to workspaces and can read and modify the contents within their assigned workspaces. They can also add or delete users from their assigned workspaces and can also add new workspaces or delete their own workspaces (assuming that there is at least one admin user left in that workspace). Admin users have no control over workspaces that they are not assigned to.</p> </li> </ol> <p>Other user types are possible (e.g., a <code>Content-Only</code> user or <code>Dashboard-Only</code> user). Adding a new user type is relatively straightforward. However, it is advised to only add new user types when absolutely necessary as each new type requires additional backend logic.</p> <p>The workspace solution addresses:</p> <ol> <li>Security risks<ol> <li>Admins only have privileges in their assigned workspaces. An admin in Workspace    1 has no access to users or contents in Workspace 2.</li> <li>An admin of a workspace can add new users to their workspace without having to    share their own credentials.</li> <li>An admin can change the role of any user in their own workspaces.</li> <li>Each user is allowed to set up their own username and password, which are    universal across workspaces.</li> <li>The password of a user can only be changed by that user. This means that admins    cannot change the passwords of users in their workspaces. An admin is allowed to    change their own password.</li> <li>A user\u2019s name and default workspace (more details below) can be changed by the    admins of any workspaces that a user belongs to.</li> </ol> </li> <li>Data risks<ol> <li>An admin of a workspace can choose the user's role when adding users to their    workspace.</li> <li>An admin can also remove a user (including other admin users) from their     workspace.</li> <li>Each workspace must have at least 1 admin user. Removing the last admin user     from a workspace poses a data risk since an existing workspace with no users     means that ANY admin can add users to that workspace---this is essentially the     scenario when an admin creates a new workspace and then proceeds to add users     to that newly created workspace. However, existing workspaces can have content;     thus, we disable the ability to remove the last admin user from a workspace.</li> <li>A workspace cannot be deleted. Deleting a workspace is currently not     allowed since the process involves removing users from the workspace, possibly     reassigning those users to other default workspaces, and deleting/archiving     artifacts such as content, tags, urgency rules, and feedback.</li> </ol> </li> <li>Resource sharing<ol> <li>When a new workspace is created, the user that creates the workspace is      automatically assigned as an admin user in that workspace. They can then add      other users to the workspace, including other admin users.</li> <li>Admins set the API daily quota and content quota of a workspace when the      workspace is created. These quotas can only be updated by the workspace admins.</li> <li>The API key is now tied to workspaces rather than individual users. Only      admins can generate a new API key.</li> <li>Costly resources (such as making calls to LLM providers) can be limited to      certain user types (e.g., <code>LLM</code> users).</li> </ol> </li> </ol>"},{"location":"components/workspaces/#major-changes-10000-foot-view","title":"Major Changes (10,000 Foot View)","text":"<p>The old AAQ design uses the user's ID as the unique key for authentication, creating access tokens, and filtering users, content, tags, etc. In other words, each user was assigned to their own \u201cworkspace\u201d with their user ID as the unique identifier for that workspace.</p> <p>The new design effectively replaces every use of user ID with workspace ID and takes the user's role in the current workspace into account when accessing certain endpoints. In effect, operations that modify artifacts can only be done by admin users. Read-only users can only view existing artifacts.</p> <p>There are 2 new tables:</p> <ol> <li><code>WorkspaceDB</code>: This table manages workspace information, such as API and content quotas</li> <li><code>UserWorkspaceDB</code>: This table manages the relationship between users and workspaces, including the user's role in each workspace and a user's default workspace.</li> </ol>"},{"location":"deployment/architecture/","title":"Architecture","text":"<p>We use docker-compose to orchestrate containers with a reverse proxy that manages all incoming traffic to the service. The database and LiteLLM proxy are only accessed by the core app.</p> <p> </p>"},{"location":"deployment/config-options/","title":"Configuring AAQ","text":"<p>There are several aspects of AAQ you can configure:</p> <ol> <li> <p>Application configs through environment variables</p> <p>All required and optional environment variables are defined in <code>deployment/docker-compose/template.*.env</code> files. You will need to copy the templates into <code>.*.env</code> files.</p> <pre><code>cp template.base.env .base.env\ncp template.core_backend.env .core_backend.env\ncp template.litellm_proxy.env .litellm_proxy.env\n</code></pre> <p>To get a local setup running with docker compose, you won't need to change any values except for LLM credentials in <code>.litellm_proxy.env</code>.</p> <p>See the rest of this page for more information on the environment variables.</p> </li> <li> <p>LLM models in    <code>litellm_proxy_config.yaml</code></p> <p>This defines which LLM to use for which task. You may want to change the LLMs and specific calling parameters based on your needs.</p> </li> <li> <p>LLM prompts in    <code>llm_prompts.py</code></p> <p>While all prompts have been carefully selected to perform each task well, you can customize them to your need here.</p> </li> </ol> <p></p> <p>Understanding the template environment files <code>template.*.env</code></p> <p>For local testing and development, the values shoud work as is, except for LLM API credentials in <code>.litellm_proxy.env</code></p> <p>For production, make sure you confirm or update the ones marked \"change for production\" at the least.</p> <ol> <li>Secrets have been marked with \ud83d\udd12.</li> <li>All optional values have been commented out. Uncomment to customize for your own case.</li> </ol>"},{"location":"deployment/config-options/#aaq-wide-configurations","title":"AAQ-wide configurations","text":"<p>The base environment variables are shared by <code>caddy</code> (reverse proxy), <code>core_backend</code>, and <code>admin_app</code> during run time.</p> <p>If not done already, copy the template environment file to <code>.base.env</code></p> <pre><code>cd deployment/docker-compose/\ncp template.base.env .base.env\n</code></pre> <p>Then, edit the environment variables according to your need (guide on updating the template):</p> <code>deployment/docker-compose/template.base.env</code><pre><code>#### AAQ domain -- change for production ######################################\nDOMAIN=\"localhost\"\n# Example value: `example.domain.com`\n# This is the domain that admin_app will be hosted on. core_backend will be\n# hosted on ${DOMAIN}/${BACKEND_ROOT_PATH}.\n\nBACKEND_ROOT_PATH=\"/api\"\n# This is the path that core_backend will be hosted on.\n# Only change if you want to use a different backend root path.\n\n#### Google OAuth Client ID ###################################################\n# NEXT_PUBLIC_GOOGLE_LOGIN_CLIENT_ID=\"update-me\"\n# If you want to use Google OAuth, set the correct value for your production.\n# This value is used by core_backend and admin_app.\n\n#### Backend URL ##############################################################\nNEXT_PUBLIC_BACKEND_URL=\"https://${DOMAIN}${BACKEND_ROOT_PATH}\"\n# Do not change this value. This value is used by admin_app.\n# If not set, it will default to \"http://localhost:8000\" in the admin_app.\n</code></pre>"},{"location":"deployment/config-options/#configuring-the-backend-core_backend","title":"Configuring the backend (<code>core_backend</code>)","text":""},{"location":"deployment/config-options/#environment-variables-for-the-backend","title":"Environment variables for the backend","text":"<p>If not done already, copy the template environment file to <code>.core_backend.env</code> (guide on updating the template):</p> <pre><code>cd deployment/docker-compose/\ncp template.core_backend.env .core_backend.env\n</code></pre> <p>The <code>core_backend</code> uses the following required and optional (commented out) environment variables.</p> <code>deployment/docker-compose/template.core_backend.env</code><pre><code># If not set, default values are loaded from core_backend/app/**/config.py files\n\n#### \ud83d\udd12 Postgres variables -- change for production ###########################\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres  #pragma: allowlist secret\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=postgres\n\n#### \ud83d\udd12 Admin user -- change for production ###################################\nADMIN_USERNAME=\"admin\"\nADMIN_PASSWORD=\"fullaccess\" #pragma: allowlist secret\nADMIN_API_KEY=\"admin-key\"   #pragma: allowlist secret\n\n#### Admin user rate limits ###################################################\n# ADMIN_CONTENT_QUOTA=1000\n# ADMIN_API_DAILY_QUOTA=100\n\n#### \ud83d\udd12 JWT -- change for production ###########################################\nJWT_SECRET=\"jwt-secret\"    #pragma: allowlist secret\n\n#### Dashboard settings #######################################################\nDISABLE_DASHBOARD_LLM=False\n\n#### Redis  -- change for production ##########################################\nREDIS_HOST=\"redis://localhost:6379\"\n# For docker compose, use \"redis://redis:6379\"\n\n#### LiteLLM Proxy Server -- change for production ############################\nLITELLM_ENDPOINT=\"http://localhost:4000\"\n# For docker compose, use \"http://litellm_proxy:4000\"\n\n#### Variables for Huggingface embeddings container ###########################\n# If on ARM, you need to build the embeddings image manually using\n# `make build-embeddings-arm` from repository root and set the following variables\n#EMBEDDINGS_IMAGE_NAME=text-embeddings-inference-arm\n#PGVECTOR_VECTOR_SIZE=1024\n\n#### Speech APIs ###############################################################\n# CUSTOM_STT_ENDPOINT=http://speech_service:8001/transcribe\n# CUSTOM_TTS_ENDPOINT=http://speech_service:8001/synthesize\n\n#### Temporary folder for prometheus gunicorn multiprocess ####################\nPROMETHEUS_MULTIPROC_DIR=\"/tmp\"\n\n#### Application-wide content limits ##########################################\n# CHECK_CONTENT_LIMIT=True\n# DEFAULT_CONTENT_QUOTA=50\n\n#### Number of top content to return for /search. #############################\n# N_TOP_CONTENT=5\n\n#### Urgency detection variables ##############################################\n# URGENCY_CLASSIFIER=\"cosine_distance_classifier\"\n# Choose between `cosine_distance_classifier` and `llm_entailment_classifier`\n\n# URGENCY_DETECTION_MAX_DISTANCE=0.5\n# Only used if URGENCY_CLASSIFIER=cosine_distance_classifier\n\n# URGENCY_DETECTION_MIN_PROBABILITY=0.5\n# Only used if URGENCY_CLASSIFIER=llm_entailment_classifier\n\n#### LLM response alignment scoring ###########################################\n# ALIGN_SCORE_THRESHOLD=0.7\n\n#### LiteLLM tracing ##########################################################\nLANGFUSE=False\n\n# \ud83d\udd12 Keys\n# LANGFUSE_PUBLIC_KEY=\"pk-...\"\n# LANGFUSE_SECRET_KEY=\"sk-...\"  #pragma: allowlist secret\n# Set LANGFUSE=True to enable Langfuse logging, and set the keys.\n# See https://docs.litellm.ai/docs/observability/langfuse_integration for more\n# information.\n\n# Optional based on your Langfuse host:\n# LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n\n##### Google Cloud Storage Variables#############################################\n# GCS_SPEECH_BUCKET=\"aaq-speech-test\"\n# Set this variable up to your specific GCS bucket for storage and retrieval for Speech Workflow.\n\n#### Sentry ###################################################################\nSENTRY_DSN=\"https://...\"\n</code></pre>"},{"location":"deployment/config-options/#other-configurations-for-the-backend","title":"Other configurations for the backend","text":"<p>You can view all configurations that <code>core_backend</code> uses in <code>core_backend/app/*/config.py</code> files -- for example, <code>core_backend/app/config.py</code>.</p> Environment variables take precedence over the config file. <p>You'll see in the config files that we get parameters from the environment and if not found, we fall back on defaults provided. So any environment variables set will override any defaults you have set in the config file.</p>"},{"location":"deployment/config-options/#configuring-litellm-proxy-server-litellm_proxy","title":"Configuring LiteLLM Proxy Server (<code>litellm_proxy</code>)","text":""},{"location":"deployment/config-options/#litellm-proxy-server-configurations","title":"LiteLLM Proxy Server configurations","text":"<p>You can edit the default LiteLLM Proxy Server settings by updating <code>litellm_proxy_config.yaml</code>. Learn more about the server configuration in LiteLLM Proxy Server.</p>"},{"location":"deployment/config-options/#authenticating-litellm-proxy-server-to-llms","title":"Authenticating LiteLLM Proxy Server to LLMs","text":"<p>The <code>litellm_proxy</code> server uses the following required and optional (commented out) environment variables for authenticating to external LLM APIs (guide on updating the template).</p> <p>You will need to set up the correct credentials (API keys, etc.) for all LLM APIs declared in <code>litellm_proxy_config.yaml</code>. See LiteLLM's documentation for more information about authentication for different LLMs.</p> <code>deployment/docker-compose/template.litellm_proxy.env</code><pre><code># For every LLM API you decide to use, defined in litellm_proxy_config.yaml,\n# ensure you set up the correct authentication(s) here.\n\n#### \ud83d\udd12 Vertex AI auth -- change for production ###############################\n# Must be set if using VertexAI models\nGOOGLE_APPLICATION_CREDENTIALS=\"/app/credentials.json\"\n# Path to the GCP credentials file *within* litellm_proxy container.\n# This default value should work with docker compose.\n\nVERTEXAI_PROJECT=\"gcp-project-id-12345\"\nVERTEXAI_LOCATION=\"us-central1\"\nVERTEXAI_ENDPOINT=\"https://us-central1-aiplatform.googleapis.com\"\n# Vertex AI endpoint. Note that you may want to increase the request quota from\n# GCP's APIs console.\n\n#### \ud83d\udd12 OpenAI auth -- change for production ##################################\n# Must be set if using OpenAI APIs.\nOPENAI_API_KEY=\"sk-...\"\n\n#### \ud83d\udd12 Huggingface embeddings -- change for production #######################\n# HUGGINGFACE_MODEL=\"Alibaba-NLP/gte-large-en-v1.5\"\n# HUGGINGFACE_EMBEDDINGS_API_KEY=\"embeddings\"  #pragma: allowlist secret\n# HUGGINGFACE_EMBEDDINGS_ENDPOINT=\"http://huggingface-embeddings\"\n# This default endpoint value should work with docker compose.\n\n#### \ud83d\udd12 LiteLLM Proxy UI -- change for production #############################\n# UI_USERNAME=\"admin\"\n# UI_PASSWORD=\"admin\"\n</code></pre>"},{"location":"deployment/config-options/#configuring-optional-components","title":"Configuring optional components","text":"<p>See instructions for setting these in the documentation for the specific optional component at Optional components.</p>"},{"location":"deployment/quick-setup/","title":"Quick Setup with Docker Compose","text":""},{"location":"deployment/quick-setup/#quick-setup","title":"Quick setup","text":"<p>You need to have installed Docker</p> <p>Step 1: Clone the AAQ repository.</p> <pre><code>git clone git@github.com:IDinsight/ask-a-question.git\n</code></pre> <p>Step 2: Navigate to the <code>deployment/docker-compose/</code> subfolder.</p> <pre><code>cd deployment/docker-compose/\n</code></pre> <p>Step 3: Copy <code>template.*.env</code> files to <code>.*.env</code>:</p> <pre><code>cp template.base.env .base.env\ncp template.core_backend.env .core_backend.env\ncp template.litellm_proxy.env .litellm_proxy.env\n</code></pre> <p>Step 4: Configure LiteLLM Proxy server</p> <ol> <li>(optional) Edit <code>litellm_proxy_config.yaml</code> with LLM services you want to use. See    LiteLLM Proxy Server for more details.</li> <li>Update the API key(s) and authentication information in    <code>.litellm_proxy.env</code>. Make sure you set up the correct authentication for the LLM    services defined in <code>litellm_proxy_config.yaml</code>.</li> </ol> <p>Step 5: Run docker-compose</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack up -d --build\n</code></pre> <p>You can now view the AAQ admin app at <code>https://$DOMAIN/</code> (by default, this should be https://localhost/) and the API documentation at <code>https://$DOMAIN/api/docs</code> (you can also test the endpoints here).</p> <p>Step 6: Shutdown containers</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -p aaq-stack down\n</code></pre>"},{"location":"deployment/quick-setup/#ready-to-deploy","title":"Ready to deploy?","text":"<p>See Configuring AAQ to configure your app.</p>"},{"location":"develop/contributing/","title":"Contributing to AAQ","text":"<p>Thank you for your interest in contributing to AAQ!</p> <p>AAQ is an open source project started by data scientists at IDinsight and sponsored by Google.org. Everyone is welcome to contribute! </p> <p>Note</p> <p>If you want to set up the complete development environment for AAQ first, you can follow the setup instructions here. Otherwise, this page provides general guidelines on how to contribute to the project with a minimal setup.</p>"},{"location":"develop/contributing/#pull-requests-guide","title":"Pull requests guide","text":"<p>This section shows you how to raise a pull request to the project.</p>"},{"location":"develop/contributing/#make-a-fork","title":"Make a fork","text":"<ol> <li>Fork the project repository by clicking on the \"Fork\" button.</li> <li> <p>Clone the repo using:</p> <pre><code>git clone git@github.com:&lt;your GitHub handle&gt;/ask-a-question.git\n</code></pre> </li> <li> <p>Navigate to the project directory.</p> <pre><code>cd ask-a-question\n</code></pre> </li> </ol>"},{"location":"develop/contributing/#install-prerequisites","title":"Install prerequisites","text":"<p>Install conda.</p>"},{"location":"develop/contributing/#setup-your-virtual-python-environment","title":"Setup your virtual Python environment","text":"<p>You can automatically create a ready-to-go <code>aaq</code> conda environment with:</p> <pre><code>make fresh-env\nconda activate aaq\n</code></pre> <p>If you encounter errors while installing <code>psycopg2</code>, see here for troubleshooting.</p> <p>If you would like to set up your Python environment manually, you can follow the steps here.</p>"},{"location":"develop/contributing/#make-your-changes","title":"Make your changes","text":"<ol> <li> <p>Create a <code>feature</code> branch for your development changes:</p> <pre><code>git checkout -b feature\n</code></pre> </li> <li> <p>Run <code>mypy</code> with <code>mypy core_backend/app</code> (1)</p> </li> <li> <p>Then <code>git add</code> and <code>git commit</code> your changes:</p> <pre><code>git add modified_files\ngit commit\n</code></pre> </li> <li> <p>And then push the changes to your fork in GitHub</p> <pre><code>git push -u origin feature\n</code></pre> </li> <li> <p>Go to the GitHub web page of your fork of the AAQ repo. Click the <code>Pull request</code> button to send your changes to the project\u2019s maintainers for review. This will send a notification to the committers.</p> </li> </ol> <ol> <li><code>pre-commit</code> runs in its own virtual environment. Since <code>mypy</code> needs all the    packages installed, this would mean keeping a whole separate copy of your    environment just for it. That's too bulky so the pre-commit only checks    a few high-level typing things. You still need to run <code>mypy</code> directly to catch    all the typing issues.    If you forget, GitHub Actions 'Linting' workflow will pick up all the mypy errors.</li> </ol>"},{"location":"develop/contributing/#next-steps","title":"Next steps","text":"<p>If you haven't already, see Setup on how to set up the complete development environment for AAQ. Otherwise, you can check out how to test the AAQ codebase.</p>"},{"location":"develop/setup/","title":"Setting up your development environment","text":""},{"location":"develop/setup/#step-1-fork-the-repository","title":"Step 1: Fork the repository","text":"<p>Please fork the project repository by clicking on the \"Fork\" button. Then, clone the repo using your own GitHub handle:</p> <pre><code>git clone git@github.com:&lt;your GitHub handle&gt;/ask-a-question.git\n</code></pre> <p>For questions related to setup, please contact AAQ Support</p>"},{"location":"develop/setup/#step-2-configure-environment-variables","title":"Step 2: Configure environment variables","text":"<ol> <li> <p>Navigate to the <code>deployment/docker-compose</code> directory.</p> <pre><code>cd ask-a-question/deployment/docker-compose\n</code></pre> </li> <li> <p>Copy <code>template.*.env</code> into new files named <code>.*.env</code> within the same directory:</p> <pre><code>cp template.base.env .base.env\ncp template.core_backend.env .core_backend.env\ncp template.litellm_proxy.env .litellm_proxy.env\n</code></pre> </li> <li> <p>Update <code>.litellm_proxy.env</code> with LLM service credentials. This will be used by    LiteLLM Proxy Server to authenticate to LLM services.</p> <p>For local development setup, this is the only file you need to update to get started. For more information on the variables used here and other template environment files, see Configuring AAQ.</p> </li> <li> <p>(optional) Edit which LLMs are used in the    <code>litellm_proxy_config.yaml</code>.</p> </li> </ol>"},{"location":"develop/setup/#step-3-set-up-your-development-environment","title":"Step 3: Set up your development environment","text":"<p>Once you are done with steps 1 &amp; 2, there are two ways to set up your development environment for AAQ:</p> <ol> <li>Docker Compose Watch</li> <li>Manual</li> </ol> <p>You can view the pros and cons of each method at the bottom.</p>"},{"location":"develop/setup/#set-up-using-docker-compose-watch","title":"Set up using Docker Compose Watch","text":""},{"location":"develop/setup/#install-prerequisites","title":"Install prerequisites","text":"<ol> <li>Install Docker.</li> <li>If you are not using Docker Desktop, install Docker Compose with version &gt;=2.22 to use the <code>watch</code> command.</li> </ol>"},{"location":"develop/setup/#run-docker-compose-watch","title":"Run <code>docker compose watch</code>","text":"<p>If not done already, configure the environment variables in Step 2.</p> <ol> <li> <p>In the <code>deployment/docker-compose</code> directory, run</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack watch\n</code></pre> Here's what you should see if the above command executes successfully <pre><code>\u2714 Network aaq-stack_default            Created\n\u2714 Volume \"aaq-stack_db_volume\"         Created\n\u2714 Volume \"aaq-stack_caddy_data\"        Created\n\u2714 Volume \"aaq-stack_caddy_config\"      Created\n\u2714 Container aaq-stack-caddy-1          Started\n\u2714 Container aaq-stack-litellm_proxy-1  Started\n\u2714 Container aaq-stack-relational_db-1  Started\n\u2714 Container aaq-stack-core_backend-1   Started\n\u2714 Container aaq-stack-admin_app-1      Started\nWatch enabled\n</code></pre> <p>The app will now run and update with any changes made to the <code>core_backend</code> or <code>admin_app</code> folders.</p> <p>The admin app will be available on https://localhost and the backend API testing UI on https://localhost/api/docs.</p> </li> <li> <p>To stop AAQ, first exit the running app process in your terminal with <code>ctrl+c</code> and then run:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack down\n</code></pre> </li> </ol>"},{"location":"develop/setup/#set-up-manually","title":"Set up manually","text":""},{"location":"develop/setup/#install-prerequisites_1","title":"Install prerequisites","text":"<ol> <li>Install    conda.</li> <li> <p>Install Node.js v19.</p> Setting up a different NodeJS version <p>If you have a different NodeJS version installed already, you can switch to a different version by installing Node Version Manager and then executing</p> <pre><code>nvm install 19\nnvm use 19\n</code></pre> </li> <li> <p>Install Docker.</p> </li> <li>(optional) Install GNU Make. <code>make</code> is used to run commands (targets) in <code>Makefile</code>s and can be used to automate various setup procedures.</li> </ol>"},{"location":"develop/setup/#set-up-the-backend","title":"Set up the backend","text":"<ol> <li> <p>Navigate to <code>ask-a-question</code> repository root.</p> <pre><code>cd ask-a-question\n</code></pre> </li> <li> <p>Set up your Python environment by creating a new conda environment using <code>make</code>.</p> <pre><code>make fresh-env\n</code></pre> <p>This command will remove any existing <code>aaq</code> conda environment and create a new <code>aaq</code> conda environment with the required Python packages.</p> <p></p> Errors with <code>psycopg2</code>? <p><code>psycopg2</code> vs <code>psycopg2-binary</code>: For production use cases we should use <code>psycopg2</code> but for local development, <code>psycopg2-binary</code> suffices and is often easier to install. If you are getting errors from <code>psycopg2</code>, try installing <code>psycopg2-binary</code> instead.</p> <p>If you would like <code>psycopg2-binary</code> instead of <code>psycopg2</code>, run <code>make fresh-env psycopg_binary=true</code> instead (see below for why you may want this).</p> <p>See here for more details.</p> <p></p> Setting up the Python environment manually <p>Create virtual environment</p> <pre><code>conda create --name aaq python=3.10\n</code></pre> <p>Install Python packages</p> <pre><code>pip install -r core_backend/requirements.txt\npip install -r requirements-dev.txt\n</code></pre> <p>Install pre-commit</p> <p><code>pre-commit</code> is used to ensure that code changes  are formatted correctly. It is only necessary if you are planning on making  changes to the codebase.</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>Activate your <code>aaq</code> conda environment.</p> <pre><code>conda activate aaq\n</code></pre> </li> <li> <p>Set up the required Docker containers by running</p> <pre><code>make setup-dev\n</code></pre> <p>The command will start the following containers:</p> <ul> <li>PostgreSQL with pgvector extension</li> <li>LiteLLM Proxy Server</li> <li>Redis</li> </ul> Setting up the DB, LiteLLM Proxy Server, and Redis seprately <p>If you would like to set up each of these dependencies separately, check out the Makefile at repository root.</p> </li> <li> <p>Export the environment variables you defined earlier in Step    2 by running</p> <pre><code>set -a\nsource deployment/docker-compose/.base.env\nsource deployment/docker-compose/.core_backend.env\nset +a\n</code></pre> Saving environment variables in <code>aaq</code> conda environment <p>You can set environment variables by either running <code>conda env config vars set &lt;NAME&gt;=&lt;VALUE&gt;</code> for each required environment variable, or save them in the environment activation script.</p> </li> <li> <p>Start the backend app.</p> <pre><code>python core_backend/main.py\n</code></pre> Here's what you should see if the above command executes successfully <pre><code>INFO:     Will watch for changes in these directories: ['~/ask-a-question']\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [73855] using StatReload\nINFO:     Started server process [73858]\nINFO:     Waiting for application startup.\n07/15/2024 12:29:08 PM          __init__.py  79 : Application started\nINFO:     Application startup complete.\n</code></pre> <p>This will launch the application in \"reload\" mode (i.e., the app will automatically refresh everytime you make a change to one of the files).</p> <p>You can test the endpoint for the API documentation by going to http://localhost:8000/docs (the backend itself runs on http://localhost:8000).</p> </li> <li> <p>To stop the backend app, first exit the running app process in your terminal with <code>ctrl+c</code> and then run:</p> <pre><code>make teardown-dev\n</code></pre> </li> </ol>"},{"location":"develop/setup/#set-up-the-frontend","title":"Set up the frontend","text":"<ol> <li> <p>Navigate to <code>ask-a-question</code> repository root and activate the <code>aaq</code> virtual environment:</p> <pre><code>cd ask-a-question\n</code></pre> <pre><code>conda activate aaq\n</code></pre> </li> <li> <p>In a new terminal:</p> <pre><code>cd admin_app\nnvm use 19\nnpm install\nnpm run dev\n</code></pre> Here's what you should see if the above commands execute successfully <pre><code>&gt; admin-app@0.2.0 dev\n&gt; next dev\n\n\u25b2 Next.js 14.1.3\n- Local:        http://localhost:3000\n\n\u2713 Ready in 1233ms\n</code></pre> <p>This will install the required NodeJS packages for the admin app and start the admin app in <code>dev</code> (i.e., autoreload) mode. The admin app will now be accessible on http://localhost:3000/.</p> <p>You can login with either the default credentials (username: <code>admin</code>, password: <code>fullaccess</code>) or the ones you specified in <code>.core_backend.env</code>.</p> </li> <li> <p>To stop the admin app, exit the running app process in your terminal with <code>ctrl</code>+<code>c</code>.</p> </li> </ol>"},{"location":"develop/setup/#pros-and-cons-of-each-setup-method","title":"Pros and cons of each setup method","text":"Method Pros Cons Set up using docker compose watch <ul><li>Good for end-to-end testing</li><li>Local environment identical to production deployment</li><li>No need to setup local environment</li><li>Set environment variables and configs once</li></ul> <ul><li>Changes take 20-30s to be reflected in the app</li></ul> Set up manually <ul><li>Instant feedback from changes</li></ul> <ul><li>Requires more configuration before each run</li><li>Requires environment and dependencies to be set up correctly</li><ul>"},{"location":"develop/setup/#step-4-set-up-docs","title":"Step 4: Set up docs","text":"<p>Note</p> <p>Ensure that you are in the <code>ask-a-question</code> project directory and that you have activated the <code>aaq</code> virtual environment before proceeding.</p> <p>To host docs offline so that you can see documentation changes in real-time, run the following from <code>ask-a-question</code> repository root with an altered port (so that it doesn't interfere with the app's server):</p> <pre><code>mkdocs serve -a localhost:8080\n</code></pre>"},{"location":"develop/validation/","title":"Running validation","text":"<p>Currently, there is validation only for retrieval, i.e. <code>POST /search</code> endpoint with <code>\"generate_llm_response\": false</code></p> <p>To evaluate the performance of your model (along with your own configurations and guardrails), run the validation test(s) in <code>core_backend/validation</code>.</p>"},{"location":"develop/validation/#retrieval-search-validation","title":"Retrieval (<code>/search</code>) validation","text":"<p>We evaluate the \"performance\" of retrieval by computing \"Top K Accuracy\", which is defined as proportion of times the best matching answer was present in top K retrieved contents.</p>"},{"location":"develop/validation/#preparing-the-data","title":"Preparing the data","text":"<p>The test assumes the validation data contains a single label representing the best matching content, rather than a ranked list of all relevant content.</p> <p>An example validation data will look like</p> query label \"How?\" 0 \"When?\" 1 \"What year was it?\" 1 \"May I?\" 2 <p>An example content data will look like</p> content_text label \"Here's how.\" 0 \"It was 2024.\" 1 \"Yes\" 2"},{"location":"develop/validation/#setting-up","title":"Setting up","text":"<ol> <li>Create a new python environment:     <pre><code>conda create -n \"aaq-validate\" python=3.10\n</code></pre>     You can also copy the existing <code>aaq</code> environment.</li> <li>Install requirements. This assumes you are in project root <code>ask-a-question</code>.     <pre><code>conda activate aaq-validate\npip install -r core_backend/requirements.txt\npip install -r core_backend/validation/requirements.txt\n</code></pre></li> <li> <p>Set environment variables.</p> <ol> <li> <p>You must export the required environment variables. They are defined with default values in     <code>core_backend/validation/validation.env</code>. To ensure that these env variables are     set every time you activate <code>aaq-validate</code>, you can run the     following command for each variable:     <pre><code>conda env config vars set &lt;VARIABLE&gt;=&lt;VALUE&gt;\n</code></pre></p> </li> <li> <p>For optional ones, check out the defaults in <code>core_backend/app/configs/app_config.py</code>     and modify as per your own requirements. For example:     <pre><code>conda env config vars set LITELLM_MODEL_EMBEDDING=&lt;...&gt;\n</code></pre></p> </li> <li>If you are using an external LLM endpoint, e.g. OpenAI, make sure to export the     API key variable as well.     <pre><code>conda env config vars set OPENAI_API_KEY=&lt;Your OPENAI API key&gt;\n</code></pre></li> </ol> </li> </ol>"},{"location":"develop/validation/#running-retrieval-validation","title":"Running retrieval validation","text":"<p>In project root <code>ask-a-question</code> run the following command. (Perform any necessary    authentication steps you need to do, e.g. for AWS login).     <pre><code>cd ask-a-question\n\npython -m pytest core_backend/validation/validate_retrieval.py \\\n    --validation_data_path &lt;path&gt; \\\n    --content_data_path &lt;path&gt; \\\n    --validation_data_question_col &lt;name&gt; \\\n    --validation_data_label_col &lt;name&gt; \\\n    --content_data_label_col &lt;name&gt; \\\n    --content_data_text_col &lt;name&gt; \\\n    --notification_topic &lt;topic ARN, if using AWS SNS&gt; \\\n    --aws_profile &lt;aws SSO profile name, if required&gt; \\\n    -n auto -s\n</code></pre> <code>-n auto</code> allows multiprocessing to speed up the test, and <code>-s</code> ensures logging by     the test module is shown on your stdout.</p> <pre><code>For details of the command line arguments, see the \"Custom options\" section of the\noutput for the following command:\n```shell\npython -m pytest core_backend/validation/validate_retrieval.py --help\n```\n</code></pre>"},{"location":"develop/testing/core-backend-testing/","title":"Writing and running tests","text":"<p>If you are writing new features, you should also add unit tests. Tests are under <code>core_backend/tests</code>.</p>"},{"location":"develop/testing/core-backend-testing/#running-unit-tests","title":"Running unit tests","text":"<p>You need to have installed Docker</p> Don't run <code>pytest</code> directly <p>Unless you have updated your environment variables and started a testing instance of postgres, the tests will end up writing to your dev environment </p> <p>Run tests using:</p> <pre><code>make tests\n</code></pre> <p>This target starts up new postgres and redis containers for testing. It also sets the correct environment variables, runs <code>pytest</code>, and then destroys the containers once done.</p>"},{"location":"develop/testing/core-backend-testing/#debugging-unit-tests","title":"Debugging unit tests","text":"<p>Before debugging, run <code>make setup-test-db</code> within <code>core_backend</code> to launch new postgres container for testing and set the correct environment variables.</p> <p>After debugging, clean up the testing resources by calling <code>make teardown-test-db</code>.</p>"},{"location":"develop/testing/core-backend-testing/#configs-for-visual-studio-code","title":"Configs for Visual Studio Code","text":"<code>.vscode/launch.json</code> <p>Add the following configuration to your <code>.vscode/launch.json</code> file to set environment variables for debugging:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {  // configuration for debugging\n            \"name\": \"Python: Tests in current file\",\n            \"purpose\": [\"debug-test\"],\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"args\": [\"--color=yes\"],\n            \"envFile\": \"${workspaceFolder}/core_backend/tests/api/test.env\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": false\n        }\n    ]\n}\n</code></pre> <code>.vscode/settings.json</code> <p>Add the following configuration to <code>.vscode/settings.json</code> to set the correct pytest working directory and environment variables:</p> <pre><code>{\n    \"python.testing.cwd\": \"${workspaceFolder}/core_backend\",\n    \"python.testing.pytestArgs\": [\n        \"tests\",\n        \"--rootdir=${workspaceFolder}/core_backend\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true,\n    \"python.envFile\": \"${workspaceFolder}/core_backend/tests/api/test.env\"\n}\n</code></pre>"},{"location":"develop/testing/core-backend-testing/#running-optional-tests","title":"Running optional tests","text":"<p>There are some additional tests that are not run by default. Most of these either make call to OpenAI or depend on other components:</p> <ul> <li>Language Identification: Tests if the the solution is able to identify the language given a short sample text.</li> <li>Test if LLM response aligns with provided context: Tests for hallucinations by checking if the LLM response is supported by the provided context.</li> <li>Test safety: Tests for prompt injection and jailbreaking.</li> </ul> <p>These tests will require the LiteLLM Proxy server to be running (to accept LLM calls). You can run this by going to root and running:</p> <pre><code>make setup-llm-proxy\n</code></pre> <p>Then run the tests using:</p> <pre><code>cd core_backend\nmake setup-test-db\npython -m pytest -m rails\n</code></pre> <p>And when done:</p> <pre><code>make teardown-test-db\n</code></pre>"},{"location":"develop/testing/speech-service-testing/","title":"Writing and Running Tests for the In-house Speech Service","text":"<p>This guide outlines the process for writing, running, and debugging tests for the in-house Speech Service.</p>"},{"location":"develop/testing/speech-service-testing/#adding-new-tests","title":"Adding new tests","text":"<p>When developing new features for Speech Service, it's crucial to add corresponding unit tests. These tests should be placed in the following directory: <code>optional_components/speech_api/tests</code>.</p>"},{"location":"develop/testing/speech-service-testing/#executing-unit-tests","title":"Executing unit Tests","text":"<p>You need to have installed Docker</p> <p>To run the unit tests, simply use the following command:</p> <pre><code>make tests\n</code></pre> <p>This command handles all necessary steps, including:</p> <ol> <li>Building the Docker container for testing</li> <li>Running the tests inside the container</li> <li>Cleaning up after the tests are complete</li> </ol> <p>The entire process is automated, so you don't need to worry about setting up the environment or cleaning up afterwards.</p>"},{"location":"develop/testing/speech-service-testing/#debugging-unit-tests","title":"Debugging unit tests","text":"<p>For debugging, you can modify the process to run an interactive container:</p> <ol> <li>First, build the Docker image:</li> </ol> <p><pre><code>make setup-tests\n</code></pre> 2. Then, run an interactive container:</p> <p><pre><code>docker run -it --rm --name speech_test_container speech_test /bin/bash\n</code></pre> 3. Once inside the container, you can run tests manually:</p> <p><pre><code>pytest --color=yes -rPQ tests/*\n</code></pre> Or run specific tests as needed.</p> <ol> <li>Exit the container when done (the container will be automatically removed due to the <code>--rm</code> flag).</li> </ol>"},{"location":"develop/testing/speech-service-testing/#configs-for-visual-studio-code","title":"Configs for Visual Studio Code","text":"Add this to your <code>.vscode/launch.json</code> file: <p>Add the following configuration to your <code>.vscode/launch.json</code> file to set environment variables for debugging:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {  // configuration for debugging\n            \"name\": \"Python: Tests in current file\",\n            \"purpose\": [\"debug-test\"],\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"args\": [\"--color=yes\"],\n            \"envFile\": \"${workspaceFolder}/optional_components/tests/test.env\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": false\n        }\n    ]\n}\n</code></pre> Add this to your <code>.vscode/settings.json</code> file: <p>Add the following configuration to <code>.vscode/settings.json</code> to set the correct pytest working directory and environment variables:</p> <pre><code>{\n    \"python.testing.cwd\": \"${workspaceFolder}/optional_components/speech_api\",\n    \"python.testing.pytestArgs\": [\n        \"tests\",\n        \"--rootdir=${workspaceFolder}/optional_components/speech_api\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true,\n    \"python.envFile\": \"${workspaceFolder}/optional_components/speech_api/tests/test.env\"\n}\n</code></pre> <p>With these configurations in place, you'll be able to efficiently run and debug your Speech Service tests within Visual Studio Code.</p>"},{"location":"integrations/","title":"Integrations","text":"<p>In this section you can find the different integrations AAQ supports so far.</p>"},{"location":"integrations/#chat-managers","title":"Chat Managers","text":"<p>You can use the AAQ endpoints through various chat managers. Below are some examples:</p> <ul> <li> <p></p> <p>How to get Typebot running and connected to AAQ endpoints</p> <p> More info</p> </li> <li> <p> Botpress</p> <p>How to get Botpress v12 (OSS) running and connected to AAQ endpoints</p> <p> More info</p> </li> <li> <p></p> <p>How to connect a Turn.io Journey to AAQ endpoints</p> <p> More info</p> </li> <li> <p></p> <p>How to connect a Glific flow to AAQ endpoints</p> <p> More info</p> </li> </ul>"},{"location":"integrations/chat_managers/botpress_v12/","title":"Botpress v12  Setup Instructions","text":"<p>Below is an example of how to get Botpress v12 (OSS) running and connected to AAQ endpoints using a provided demo flow.</p> <p>Note: Botpress v12 is open-source and available to self-host but Botpress Cloud is a different closed-source product.</p>"},{"location":"integrations/chat_managers/botpress_v12/#demo-aaq-flow","title":"Demo AAQ flow","text":"<ol> <li>Once you have deployed Botpress v12 as per your requirements, go to the URL where the app is running</li> <li>Make an account and login</li> <li>Go to \"Create Bot\" and then \"Import Existing\" (you can set Bot ID to anything you want)</li> <li>Load the <code>.tgz</code> file given under <code>chat_managers/botpress_v12/</code> in the AAQ repo</li> <li> <p>Edit the \"API Call\" cards to reflect the AAQ endpoint URL that you have running *</p> <p>a. Click on the card</p> <p>b. Click on \"Edit skill\"</p> <p>c. Change the base of the URL at the top</p> <p>d. If you've changed the bearer token for the QA endpoints, you'll have to update the headers sections too</p> </li> <li> <p>Test the bot in the emulator</p> </li> </ol> * Errors with using <code>localhost</code> on the API Call skill? <p>If you're having trouble with localhost AAQ calls, try forwarding traffic through <code>ngrok</code> and using that for deployment of AAQ.</p> <ol> <li> <p>Install and configure ngrok</p> </li> <li> <p>Run <code>ngrok http https://localhost</code> to forward traffic</p> </li> <li> <p>In <code>deployment/.env</code> file, ensure you have</p> <pre><code>NEXT_PUBLIC_BACKEND_URL=https://[NGROK URL]/api\nBACKEND_ROOT_PATH=\"/api\"\n</code></pre> </li> <li> <p>In <code>deployment/.env.nginx</code> file ensure you have</p> <pre><code>DOMAIN=[NGROK URL]  # don't add https/http at the front\n</code></pre> </li> <li> <p>Change the base of the API Call skill so it looks like:</p> <pre><code>[NGROK URL]/api/search\n</code></pre> </li> </ol>"},{"location":"integrations/chat_managers/botpress_v12/#self-hosted-deployment","title":"Self-hosted deployment","text":""},{"location":"integrations/chat_managers/botpress_v12/#option-1-via-docker-compose-behind-caddy-with-https","title":"Option 1 - Via Docker Compose (behind Caddy with HTTPS)","text":"<p>Step 1: Navigate to <code>chat_managers/botpress_v12/deployment/</code></p> <p>Step 2: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables</p> <p>Step 3: Run docker compose</p> <pre><code>docker compose -p botpress-stack up -d --build\n</code></pre> <p>You can now access Botpress at <code>https://[DOMAIN]/</code></p> <p>Step 4: Shutdown containers</p> <pre><code>docker compose -p botpress-stack down\n</code></pre>"},{"location":"integrations/chat_managers/botpress_v12/#option-2-via-docker","title":"Option 2 - Via Docker","text":"<p>To install through Docker (recommended), follow the official Botpress v12 docs here. In short:</p> <ol> <li> <p>Get the image</p> <pre><code>docker pull botpress/server\n</code></pre> </li> <li> <p>Run the image</p> <pre><code>docker run -d --name=botpress -p 3000:3000 botpress/server\n</code></pre> </li> </ol>"},{"location":"integrations/chat_managers/botpress_v12/#option-3-via-executables","title":"Option 3 - Via executables","text":"<p>Follow the official docs here to set up Botpress v12 locally as per your OS.</p>"},{"location":"integrations/chat_managers/typebot/","title":"Setup Instructions","text":"<p>Below is a guide on how to connect to AAQ endpoints using a provided demo flow.</p> <p>You can either use the cloud-hosted Typebot service or self-host the application (see \"Deployment\" below).</p>"},{"location":"integrations/chat_managers/typebot/#demo-aaq-flow","title":"Demo AAQ flow","text":"<ol> <li>Go to your Typebot instance</li> <li>Make an account and login</li> <li>Go to \"Create a typebot\" and then \"Import a file\"</li> <li>Load the <code>.json</code> file given under <code>chat_managers/typebot/</code> in the AAQ repo</li> <li> <p>Edit the \"API Call\" cards to reflect the AAQ endpoint URL that you have running</p> <p>a. Click on the card b. Change the base of the URL at the top c. Add the bearer token in the Headers section.</p> </li> <li> <p>Test the bot in the emulator</p> </li> </ol>"},{"location":"integrations/chat_managers/typebot/#deployment","title":"Deployment","text":"<p>For self-hosting, you can either follow the official docs or follow our quick start below:</p> <p>Step 1: Navigate to <code>chat_managers/typebot/deployment/</code></p> <p>Step 2: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables</p> You must configure at least one login option while setting the environment variables. <p>We recommend either Github or Google authentication. See Typebot's docs for details.</p> <p>Step 3: Run docker compose</p> <pre><code>docker compose -p typebot-stack up -d --build\n</code></pre> <p>You can now access Typebot at <code>https://[DOMAIN]/</code></p> <p>Step 4: Shutdown containers</p> <pre><code>docker compose -p typebot-stack down\n</code></pre>"},{"location":"integrations/chat_managers/glific/glific/","title":"Setup Instructions","text":"<p>Below is a tutorial for how to load our FAQ template flow into Glific and connect it to your own AAQ endpoint.</p> <ol> <li> <p>Go to \"Flows\"</p> <p></p> </li> <li> <p>Click \"Import flow\"</p> <p></p> </li> <li> <p>Select a <code>.json</code> file given under <code>chat_managers/glific/</code> in the AAQ repo</p> <p></p> </li> <li> <p>Open the imported flow</p> <p></p> </li> <li> <p>Open the webhook card</p> <p></p> </li> <li> <p>Replace <code>&lt;INSERT_AAQ_URL&gt;</code> with your the URL to your AAQ instance</p> <p></p> </li> <li> <p>Go to headers and replace <code>&lt;INSERT_AAQ_API_KEY&gt;</code> value to your own AAQ API key.</p> <p> </p> </li> <li> <p>Test the flow in the \"Preview\" emulator</p> <p> </p> </li> </ol>"},{"location":"integrations/chat_managers/turn.io/turn/","title":"Setup Instructions","text":"<p>Below is an example of how to connect a Turn.io Journey to AAQ endpoints.</p> <ol> <li> <p>On your Turn.io page, go to the Journey menu.</p> <p></p> </li> <li> <p>Create New Journey.</p> <p></p> </li> <li> <p>Select \"From Scratch\" -&gt; \"Code\".</p> <p></p> </li> <li> <p>Type in your journey title and click \"Next\".</p> <p></p> </li> <li> <p>Copy and paste the contents of    chat_managers/turn.io/llm_response_flow_code_journey.txt in the AAQ repository into the    Journey's code area.</p> <p></p> </li> <li> <p>Replace <code>&lt;INSERT_AAQ_URL&gt;</code> and <code>&lt;INSERT_AAQ_API_KEY&gt;</code> values to your own AAQ URL and    API key.</p> <p></p> </li> <li> <p>Test the bot in the emulator.</p> <p></p> </li> </ol>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/page/2/","title":"Latest updates","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}