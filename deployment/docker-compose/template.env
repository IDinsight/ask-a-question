# IMPORTANT: all variables must be either set or commented out.
# If commented out, default values will be used from `core_backend/app/config.py`

#### Root domain (if not set, localhost will be used)
# DOMAIN=example.domain.com

#### Postgres variables  ######################################################
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=postgres

#### Secrets - change for production
# admin user
USER1_USERNAME="admin"
USER1_PASSWORD="fullaccess"
USER1_API_KEY="admin-key"
USER1_CONTENT_QUOTA=1000

# jwt
JWT_SECRET="jwt-secret"

#### Functionality variables
# N_TOP_CONTENT=

### limits
# CHECK_CONTENT_LIMIT=True
# DEFAULT_CONTENT_QUOTA=50

# Align Score (if ALIGN_SCORE_METHOD="AlignScore", must also set the ALIGN_SCORE_API)
ALIGN_SCORE_METHOD="LLM"
ALIGN_SCORE_THRESHOLD=0.7
# ALIGN_SCORE_API=http://alignScore:5001/alignscore_base

#### Backend paths
# if using a reverse proxy, set backend root path to /api here
BACKEND_ROOT_PATH="/api"
# if using a reverse proxy, the NEXT_PUBLIC_BACKEND_URL should be "https://$DOMAIN$BACKEND_ROOT_PATH"
# if not set, it will be set to "http://localhost:8000" in the front-end
NEXT_PUBLIC_BACKEND_URL="https://localhost/api"
# LiteLLM Proxy endpoint for the backend to call (must be container_name:port)
LITELLM_ENDPOINT="http://litellm_proxy:4000"
# Redis endpoint for the backend to call (must be container_name:port)
REDIS_HOST="redis://redis:6379"

# Temporary folder for prometheus gunicorn multiprocess
PROMETHEUS_MULTIPROC_DIR="/tmp"

#### Urgency detection variables
URGENCY_CLASSIFIER="llm_entailment_classifier" # or "cosine_distance_classifier"
URGENCY_DETECTION_MAX_DISTANCE=0.5 # only uses if using `cosine_distance_classifier`
URGENCY_DETECTION_MIN_PROBABILITY=0.5 # only uses if using `llm_entailment_classifier`

#### For login with Google
NEXT_PUBLIC_GOOGLE_LOGIN_CLIENT_ID="update-me"

#### Logging
## To enable langfuse logging, set LANGFUSE=True and set the keys
LANGFUSE=False
# LANGFUSE_PUBLIC_KEY="pk-..."
# LANGFUSE_SECRET_KEY="sk-..."
# optional based on your Langfuse host
# LANGFUSE_HOST="https://cloud.langfuse.com"

# Variables for LiteLLM Proxy container #######################################
# LiteLLM Proxy UI
UI_USERNAME="admin"
UI_PASSWORD="admin"
# For OpenAI (if using)
OPENAI_API_KEY="sk-..."
# For Google Vertex AI (if using)
GOOGLE_APPLICATION_CREDENTIALS="/path/to/.gcp-credentials.json"
VERTEXAI_PROJECT="gcp-project-id-12345"
VERTEXAI_LOCATION="us-central1"
VERTEXAI_ENDPOINT="https://us-central1-aiplatform.googleapis.com"
# Variables for connecting to local embeddings container (if using)
HUGGINGFACE_MODEL="Alibaba-NLP/gte-large-en-v1.5"
CUSTOM_EMBEDDINGS_API_KEY="embeddings"
CUSTOM_EMBEDDINGS_ENDPOINT="http://local-embeddings"

#Redis
REDIS_HOST="redis://redis:6379"

# Variables for local embeddings container ####################################
# If on ARM, you need to build the embeddings image manually using
# `make build-embeddings-arm` and set the following variables
# EMBEDDINGS_IMAGE_NAME=text-embeddings-inference-arm
# PGVECTOR_VECTOR_SIZE=1024
