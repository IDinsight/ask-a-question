# If not set, uses default values defined in core_backend/app/**/config.py files.

#### ðŸ”’ Postgres variables -- change for production ############################
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=postgres

#### ðŸ”’ Admin user -- change for production ####################################
ADMIN_USERNAME="admin"
ADMIN_PASSWORD="fullaccess"
ADMIN_API_KEY="admin-key"

#### Admin user rate limits ###################################################
# ADMIN_CONTENT_QUOTA=1000
# ADMIN_API_DAILY_QUOTA=100

#### ðŸ”’ JWT -- change for production ###########################################
JWT_SECRET="jwt-secret"

#### Redis  -- change for production ##########################################
REDIS_HOST="redis://localhost:6379"
# For docker compose, use "redis://redis:6379"

#### LiteLLM Proxy Server -- change for production ############################
LITELLM_ENDPOINT="http://localhost:4000"
# For docker compose, use "http://litellm_proxy:4000"

#### Variables for Huggingface embeddings container ###########################
# If on ARM, you need to build the embeddings image manually using
# `make build-embeddings-arm` from repository root and set the following variables
# EMBEDDINGS_IMAGE_NAME=text-embeddings-inference-arm
# PGVECTOR_VECTOR_SIZE=1024

#### Speech APIs ###############################################################
# SPEECH_ENDPOINT=http://speech_service:8001/transcribe
# PREFERRED_MODEL=small
# Choose from `base` or `small`

#### Temporary folder for prometheus gunicorn multiprocess ####################
PROMETHEUS_MULTIPROC_DIR="/tmp"

#### Application-wide content limits ##########################################
# CHECK_CONTENT_LIMIT=True
# DEFAULT_CONTENT_QUOTA=50

#### Number of top content to return for /search. #############################
# N_TOP_CONTENT=5

#### Urgency detection variables ##############################################
# URGENCY_CLASSIFIER="cosine_distance_classifier"
# Choose between `cosine_distance_classifier` and `llm_entailment_classifier`

# URGENCY_DETECTION_MAX_DISTANCE=0.5
# Only used if URGENCY_CLASSIFIER=cosine_distance_classifier

# URGENCY_DETECTION_MIN_PROBABILITY=0.5
# Only used if URGENCY_CLASSIFIER=llm_entailment_classifier

#### LLM response alignment scoring ###########################################
# ALIGN_SCORE_METHOD="LLM"
# ALIGN_SCORE_THRESHOLD=0.7
# ALIGN_SCORE_API=http://alignScore:5001/alignscore_base
# ALIGN_SCORE_API must be set if ALIGN_SCORE_METHOD="AlignScore"

#### LiteLLM tracing ##########################################################
LANGFUSE=False

# ðŸ”’ Keys
# LANGFUSE_PUBLIC_KEY="pk-..."
# LANGFUSE_SECRET_KEY="sk-..."
# Set LANGFUSE=True to enable Langfuse logging, and set the keys.
# See https://docs.litellm.ai/docs/observability/langfuse_integration for more information.

# Optional based on your Langfuse host:
# LANGFUSE_HOST="https://cloud.langfuse.com"
