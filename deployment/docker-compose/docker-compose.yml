services:
  core_backend:
    image: idinsight/aaq-backend:latest
    build:
      context: ../../core_backend
      dockerfile: Dockerfile
    command: >
      /bin/sh startup.sh
    restart: always
    volumes:
      - temp:/usr/src/aaq_backend/temp
      - ./.gcp_credentials.json:/app/credentials.json
    env_file:
      - .base.env
      - .core_backend.env
      - .litellm_proxy.env
    environment:
      - REDIS_HOST=redis://redis:6379
      - LITELLM_ENDPOINT=http://litellm_proxy:4000
    depends_on:
      - redis

    develop:
      watch:
        - action: rebuild
          path: ../../core_backend

  admin_app:
    image: idinsight/aaq-admin-app:latest
    build:
      context: ../../admin_app
      dockerfile: Dockerfile
    command: >
      node server.js
    depends_on:
      - core_backend
    restart: always
    env_file:
      - .base.env
    develop:
      watch:
        - action: rebuild
          path: ../../admin_app

  caddy:
    image: caddy:2.7.6
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "443:443/udp"
    volumes:
      - ./Caddyfile:/etc/caddy/Caddyfile
      - caddy_data:/data
      - caddy_config:/config
    env_file:
      - .base.env

  litellm_proxy:
    image: ghcr.io/berriai/litellm:main-v1.40.10
    restart: always
    env_file:
      - .litellm_proxy.env
    volumes:
      - ./litellm_proxy_config.yaml:/app/config.yaml
      - ./.gcp_credentials.json:/app/credentials.json
    ports: # Expose the port to port 4000 for debugging purposes
      - 4000:4000
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "4"]

  alignScore:
    image: idinsight/alignscore-base:latest
    profiles:
      - alignScore
      - optional-components
    build:
      context: ../../optional_components/alignScore
      dockerfile: Dockerfile
    command: >
      /usr/local/bin/python server.py --port=5001 --models=base
    restart: always

  huggingface-embeddings:
    # image either refers to locally built image or defaults to the one from the registry
    image: ${EMBEDDINGS_IMAGE_NAME:-ghcr.io/huggingface/text-embeddings-inference:cpu-1.5}
    profiles:
      - huggingface-embeddings
      - optional-components
    volumes:
      - $PWD/data:/data
    command:
      [
        "--model-id",
        "${HUGGINGFACE_MODEL}",
        "--api-key",
        "${HUGGINGFACE_EMBEDDINGS_API_KEY}",
      ]
    restart: always
    env_file:
      - .litellm_proxy.env
    develop:
      watch:
        - action: rebuild
          path: ../../optional_components/embeddings

  redis:
    image: "redis:6.0-alpine"
    ports: # Expose the port to port 6380 on the host machine for debugging
      - "6380:6379"
    restart: always

volumes:
  caddy_data:
  caddy_config:
  temp:
