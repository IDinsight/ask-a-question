# For every LLM API you decide to use, defined in litellm_proxy_config.yaml, ensure you
# set up the correct authentication(s) here.

#### ðŸ”’ Vertex AI auth -- change for production ################################
# Must be set if using VertexAI models
GOOGLE_APPLICATION_CREDENTIALS="/app/credentials.json"
# Path to the GCP credentials file *within* litellm_proxy container.
# This default value should work with docker compose.

VERTEXAI_PROJECT="gcp-project-id-12345"
VERTEXAI_LOCATION="us-central1"
VERTEXAI_ENDPOINT="https://us-central1-aiplatform.googleapis.com"
# Vertex AI endpoint. Note that you may want to increase the request quota from GCP's APIs
# console.

#### ðŸ”’ OpenAI auth -- change for production ###################################
# Must be set if using OpenAI APIs.
OPENAI_API_KEY="sk-..."

#### ðŸ”’ Huggingface embeddings -- change for production ########################
# HUGGINGFACE_MODEL="Alibaba-NLP/gte-large-en-v1.5"
# HUGGINGFACE_EMBEDDINGS_API_KEY="embeddings"  #pragma: allowlist secret
# HUGGINGFACE_EMBEDDINGS_ENDPOINT="http://huggingface-embeddings"
# This default endpoint value should work with docker compose.

#### ðŸ”’ LiteLLM Proxy UI -- change for production ##############################
# UI_USERNAME="admin"
# UI_PASSWORD="admin"
