{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"No-code, easy-to-setup and reliable RAG plugin for chatbots <p> Ask A Question is a free and open-source tool created to help non-profit organizations, governments in developing nations, and social sector organizations utilize Large Language Models for responding to citizen inquiries in their native languages.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Responsible &amp; Safe AI    Implements guardrailed AI that is ethical, transparent, and secure</p> </li> <li> <p> Local Languages (Coming Soon)    Multilingual model support to enhance accessibility and experience</p> </li> <li> <p> Scalable &amp; Easy to Deploy   Containerized app that can be deployed anywhere with minimal setup</p> </li> <li> <p> Voice (Coming Soon)    Ask questions and receive answers using voice memos</p> </li> </ul> <p> LLM-powered search: Answers questions to database content using LLM embeddings.</p> <p> LLM responses : Craft a custom reponse to users using LLM chat</p> <p> Chat manager integration : Integrate with Turn.io, Glific, Typebot and more</p> <p> Manage content : Use the Admin App to add, edit, and delete content in the database</p> <p> Flag urgent messages : Identify messages that are urgent based on your rules</p> <p> See Full Roadmap  </p> <p>Looking for other features?</p> <p>If you are a developing country government, NGO or a social sector organisation, we'd love to hear what features you'd like to see. Raise an issue with <code>[FEATURE REQUEST]</code> before the title to start the conversation.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p> The project is funded by Google.org through the AI for Global Goals grant.</p> <p> Built and powered by IDinsight.</p> <p>IDinsight uses data and evidence to help leaders combat poverty worldwide. Our collaborations deploy a large analytical toolkit to help clients design better policies, rigorously test what works, and use evidence to implement effectively at scale. We place special emphasis on using the right tool for the right question, and tailor our rigorous methods to the real-world constraints of decision-makers. IDinsight works with governments, foundations, NGOs, multilaterals and businesses across Africa and Asia. We work in all major sectors including health, education, agriculture, governance, digital ID, financial access, and sanitation. We have offices in Dakar, Lusaka, Manila, Nairobi, New Delhi, Rabat, and Remote.  www.idinsight.org </p>"},{"location":"contact_us/","title":"Team","text":""},{"location":"contact_us/#point-of-contact","title":"Point of Contact","text":"<ul> <li><p> Tanmay Verma    Product Manager tanmay.verma@idinsight.org </p></li> </ul>"},{"location":"contact_us/#full-team","title":"Full Team","text":"<ul> <li> <p><p> Sid Ravinutala      Director, Data Science sid.ravinutala@idinsight.org </p></p> </li> <li> <p><p> Suzin You    Tech Lead, Data Scientist suzin.you@idinsight.org </p></p> </li> <li> <p><p> Carlos Samey     Data Scientist carlos.samey@idinsight.org </p></p> </li> <li> <p><p> Amir Emami     Data Scientist amir.emami@idinsight.org </p></p> </li> </ul> <p>We are part of IDinsight's DSEM team</p> <p>Read more about DSEM and our work here.</p>"},{"location":"roadmap/","title":"Roadmap","text":"Quarter Feature Status Description Q4 2023 FastAPI Refactor Refactored to an all-components-in-one-repo codebase Embeddings-based search Match user questions to content in the database using embeddings Q1 2024 RAG responses Craft a custom response to the question using LLM based on retrieved content in the database Guardrails Keep LLM responses friendly and strictly context-based Q2 2024 Message Triaging Tag user messages with intents &amp; flag urgency Multi-user with Google log-in You can now have 1 AAQ deployment with multiple users, each with their own content DB Support for Turn.io, Glific integration Add AAQ to popular chat flow builders in social sector, like Turn.io and Glific Content Tags Add tags to your content for easy browsing (and more to come!) Q3 2024 Analytics for Feedback and Content See content use, questions that receive poor feedback, missing content, and more Voice notes support Automatic Speech Recognition for audio message to content matching Multi-turn chat Refine or clarify user question through conversation. Engineering Dashboard Monitor uptime, response rates, throughput HTTP response codes Q4 2024 Personalization and contextualization Use contextual information to improve responses Multimedia content Respond with not just text but images and audio as well. A/B Testing Test and decide content that works better for users <p>Key  : Completed  : Under development  : Queued : Yet to be scoped</p> <p>Beyond 2024</p> <ul> <li>Multi-tenant architecture</li> <li>User configurable Guardrails (in Admin app)</li> <li>Feedback-based model/LLM fine tuning</li> <li>Safety tests, fairness reports and ethical reviews</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>There are two ways to interact with the service:</p> <ol> <li>Accessing the API endpoints</li> <li>The Admin App</li> </ol>"},{"location":"usage/#api-endpoints","title":"API endpoints","text":"<p>To get answers from your database of contents, you can use the <code>/search</code> endpoint. This endpoint returns the following:</p> <ul> <li>Search results: Finds the most similar content in the database using cosine distance between embeddings.</li> <li>(Optionally) LLM generated response: Crafts a custom response using LLM chat using the most similar content.</li> </ul> <p>You can also add your contents programatically using API endpoints. See docs or SwaggerUI at <code>https://&lt;DOMAIN&gt;/api/docs</code> or <code>https://&lt;DOMAIN&gt;/docs</code> for more details and other API endpoints.</p>"},{"location":"usage/#embeddings-search","title":"Embeddings search","text":"<pre><code>curl -X 'POST' \\\n  'https://[DOMAIN]/api/search' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;BEARER TOKEN&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query_text\": \"how are you?\",\n  \"generate_llm_response\": false,\n  \"query_metadata\": {}\n}'\n</code></pre>"},{"location":"usage/#llm-response","title":"LLM response","text":"<pre><code>curl -X 'POST' \\\n  'https://[DOMAIN]/api/search' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;BEARER TOKEN&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query_text\": \"this is my question\",\n  \"generate_llm_response\": true,\n  \"query_metadata\": {}\n}'\n</code></pre>"},{"location":"usage/#admin-app","title":"Admin app","text":"<p>You can access the admin console at</p> <pre><code>https://[DOMAIN]/\n</code></pre> <p>On the Admin app, you can:</p> <ul> <li> Manage content for more details</li> <li> Use the playground to test the Question-Answering service</li> <li> View dashboards</li> </ul>"},{"location":"blog/","title":"Latest updates","text":""},{"location":"blog/2024/01/12/no-more-hallucinations/","title":"No more hallucinations","text":"<p>Last week we rolled out another safety feature - checking consistency of the response from the LLM with the content it is meant to be using to generate it. This shoud catch hallucinations or when LLM uses it's pre-training to answer a question. But it also catches any prompt injection or jailbreaking - if it somehow got through our other checks.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#checking-alignment-of-llm-response","title":"Checking alignment of LLM Response","text":"<p>Despite clear prompting, LLMs hallucinate. And sometimes use their large training set to answer a question instead of solely using the context provided.</p> <p>One of the endpoints that AAQ presents is Search. The process diagram on the page will be kept up to date on how the service works but here is what it looks like at the time of writing:</p> <p></p> <p>Steps 12 and its response, 13 check if the statements being generated by the LLM are consitent with the content it is meant to use to generate it.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#using-alignscore","title":"Using AlignScore","text":"<p>We can use GPT4-turbo and it does remarkably well on our test data. But there may be reasons - from data governance and privacy rules to costs - for not sending the data to OpenAI. One option is to use your own locally hosted LLM.</p> <p>The other option is to use AlignScore, a model built specifically for this use case. It can be deployes as another container that exposes an endpoint that AAQ can call to check consistency.</p> <p>See docs for how to do this.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#doc-references","title":"Doc references","text":"<ul> <li>Locally hosted LLMs</li> <li>Search</li> <li>AlignScore</li> </ul>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/","title":"Did someone say Dashboard?","text":"<p>Getting insights from your app has been made easy with the introduction of a questions dashboard page.</p>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/#content-dashboard","title":"Content dashboard","text":"<p>On the Admin App, you can now get statistics from your question answering service with the new dashboard displaying information such as:</p> <ul> <li>The number of questions answered during the current month</li> <li>The number of questions with positive feedback</li> <li>The number of questions asked during the past 6 months.</li> </ul> <p>The dashboard is in its first version and we are planning on adding more information such as Urgency Detection related statistics, and content related statistics. </p>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/#doc-references","title":"Doc references","text":"<ul> <li>QA Service</li> </ul>"},{"location":"blog/2024/06/17/new-feature-detected-tags/","title":"New feature detected: Tags","text":"<p>Filtering contents has been made easier with tags.</p>"},{"location":"blog/2024/06/17/new-feature-detected-tags/#manage-tags","title":"Manage tags","text":"<p>On the Admin App, it is now possible to add tags to contents to categorise them.</p> <p>You can create a tag, add it the content, or delete it directly within the edit content page. The \"Tags\" bar shown on the image below will allow you to quickly manage tags.</p> <p></p> <p>We also added the possibility of filtering contents using tags for a seemless content management experience. On the landing page, you can use the Tags bar to filter contents based on tags. </p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/","title":"Revamping the dashboard","text":"<p>In the last month or so, we've had support from Google fellows to pinch hit on a few items. Ahn Mac, a UI/UX designer at Google, helped design a new dashboard that we are very excited about. The first page, \"Overview\", is now ready for you to check out.</p> <p>Here's a screenshot:</p> <p></p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/#why-did-we-build-this","title":"Why did we build this?","text":"<p>The ethos of AAQ is that you care deeply about your users and are continuously refining and adding content to improve user experience. A live dashboard is the first step. It allows you to understand the usage of your solution, your users, and their feedback.</p> <p>Also, it wouldn't be an IDinsight product without a decent dashboard!</p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/#doc-references","title":"Doc references","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/","title":"Revamped feedback endpoints","text":"<p>There are now two new endpoints for feedback:</p> <ol> <li><code>POST /response-feedback</code> - Allows you to capture feedback for the overall response returned by either of the Question-Answering APIs.</li> <li><code>POST /content-feedback</code> - Allows you to capture feedback for a specific piece of content.</li> </ol> <p>These can be used in chat managers to collect feedback after answers are shown.</p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#sentiment-and-text","title":"Sentiment and Text","text":"<p>For both of these endpoints, you are able to provide either sentiment (positive, negative) or text feedback, or both.</p> <p>See your deployment's OpenAPI documentation at <code>https://[DOMAIN]/api/docs</code> for more details.</p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#content-cards-show-feedback","title":"Content cards show feedback","text":"<p>The positive and negative feedback captured for the content can also be seen in the \"Read\" modal for each content card in the Admin App.</p> <p></p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#doc-references","title":"Doc references","text":"<ul> <li>Response Feedback</li> <li>Content Feedback</li> </ul>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/","title":"Tracing your AAQ calls with Langfuse","text":"<p>By integrating Langfuse, a popular LLM observability tool with a generous free tier, you can now track all LLM calls made via AAQ.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#whats-in-a-trace","title":"What's in a Trace?","text":"<p>With Langfuse enabled, AAQ is set up to trace each query to the <code>POST /search</code> endpoint.</p> <p></p> <p>Each query is represented as a Trace. If you click on the Trace ID, you can view the details of the trace. Here is an example for a <code>/search</code> call with <code>generate_llm_response</code> set to <code>true</code>:</p> <p></p> <p>On the right, there are Generations associated with this trace. In AAQ, each generation is each call to the LiteLLM Proxy Server. You can view the series of input checks, RAG (\"get_similar_content_async\" and \"openai/generate-response\") and one output check we perform (you can learn more about our Guardrails here). The generation names come from the model names used in your LiteLLM Proxy Server Config.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#why-does-aaq-need-observability","title":"Why does AAQ need observability?","text":"<p>As we begin piloting AAQ in various use cases, we wanted to be able to track LLM calls so that we can debug, analyze, and improve AAQ's question-answering ability. We are using it to test different prompt templates and guardrails. If you are interested in getting your hands dirty with AAQ's codebase, we imagine this will be useful to you. (Langfuse has a generous free tier and is self-hostable!)</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#so-how-do-i-use-it","title":"So how do I use it?","text":"<p>Sign up to Langfuse, and set the following environment variables in the backend app to get started.</p> <pre><code>export LANGFUSE=True\nexport LANGFUSE_PUBLIC_KEY=pk-...\nexport LANGFUSE_SECRET_KEY=sk-...\nexport LANGFUSE_HOST=https://cloud.langfuse.com # optional based on your Langfuse host\n</code></pre> <p>See more in Config options - Tracing with Langfuse.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#whats-next","title":"What's next?","text":"<p>We want to explore the rich set of features that Langfuse offers, such as evaluation and scoring. One concrete next step is to trace AAQ's Feedback endpoint using Langfuse's Scores.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#docs-references","title":"Docs references","text":"<ul> <li>Config options - Tracing with Langfuse</li> <li>Dev setup</li> <li>LiteLLM Proxy Server</li> </ul>"},{"location":"blog/2024/01/12/improved-docs/","title":"Improved docs!","text":"<p>First, we have added this section that you are currently reading. Each week we'll post what we've rolled out - new features, bug fixes, and performance improvements.</p> <p>The rest of the docs have now also been restructured to make it easy to parse.</p>"},{"location":"blog/2024/01/12/improved-docs/#now-with-cards","title":"Now with cards!","text":"<p>A lot of the index pages now show cards like the one shown below. These should make it easy to grasp the content in section in a glance.</p> <p></p>"},{"location":"blog/2024/01/12/improved-docs/#process-flow-diagrams","title":"Process Flow Diagrams","text":"<p>Search page now shows process flow diagrams. It should make it a lot easier to understand what is happening under the hood when you call either of these endopints.</p> <p></p>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/","title":"Adding a model proxy server","text":"<p>Instead of being handled directly in our code, our model calls are now routed through a LiteLLM Proxy server. This lets us change models on the fly and have retries, fallbacks, budget tracking, and more.</p>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#why-change","title":"Why change?","text":"<p>LiteLLM Proxy introduces a streamlined approach for handling various large language models (LLMs) through a single interface. We can now manage model endpoints and configurations via a proxy server, replacing the previous method of hard-coding them into the environment of our app.</p> <p>The benefits of this setup:</p> <ul> <li>Simplifies codebase by centralizing the model configurations</li> <li>Provides the flexibility to switch or update models without altering the core application logic - we can even add and remove models through the proxy's API!</li> <li>Multiple instances of AAQ can use the same model server</li> </ul> <p>We now configure models in a <code>config.yaml</code> file, allowing the proxy to route requests to different LLMs (commercial or self-hosted - full list here). See <code>deployment/docker-compose/litellm_config.yaml</code> for an example.</p> <p>The LiteLLM Proxy server also has some extra useful features:</p> <ul> <li>Consistent input/output formats across different models</li> <li>Fallback mechanisms for error handling</li> <li>Detailed logging and connectivity to Langfuse and others</li> <li>Tracking of token usage and spending.</li> <li>Asynchronous handling of requests and caching.</li> </ul>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#any-downsides","title":"Any downsides?","text":"<p>Potential downsides include:</p> <ul> <li>Dependency on the LiteLLM project for updates for new models and parameters</li> <li>A possible increase in latency</li> <li>A new Docker container in our stack which, despite its name, it not \"lite\"!</li> </ul>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#docs-references","title":"Docs references","text":"<ul> <li>LLM Proxy Server</li> <li>Dev setup</li> </ul>"},{"location":"blog/2024/03/22/hello-material-ui/","title":"Hello Material UI","text":"<p>We've switched to MaterialUI: Cleaner, easier to build and maintain, more familiar.</p>"},{"location":"blog/2024/03/22/hello-material-ui/#why-change","title":"Why change?","text":"<p>Design best practices are \"best practices\" for having a proven track record of creating engaging, intuitive, and effective user experiences. Google has encapsulated this in the creation of MaterialUI \u2014 a design language well-infused with the principles of good design. Embracing these principles, we're thrilled to announce a significant shift in our project's Admin App: the transition to using MaterialUI components.</p> <p>This shift is a step towards cleaner maintainable code and a commitment to providing simpler and more familiar user experiences.</p> Old UI MUI Main page Edit page <p>We're already starting to see some benefits of moving to Material UI:</p> <ul> <li> <p>Cleaner Codebase:   MaterialUI components are modular and customizable, and has allowed us to achieve a polished look with less custom CSS.</p> </li> <li> <p>Easier to Build and Maintain:   MaterialUI's extensive component library has significantly reduced our development time. Components like buttons, nav bars, and switches come with a variety of options that are easily customizable.</p> </li> <li> <p>Familiarity and Consistency:   MaterialUI is a design language familiar to millions of users worldwide, thanks to Google's widespread implementation across its products. This is helping us lower the learning curve for new users and is ensuring consistency across devices.</p> </li> </ul>"},{"location":"blog/2024/02/09/nginx-out-caddy-in/","title":"Nginx out, Caddy in","text":"<p>By swapping out Nginx for Caddy, we substantially simplified the deployment steps and the architecture - which means fewer docker containers to run and manage.</p> <p>Previously, we were using NGINX and then manually running a script to issue certificates from Let's Encrypt. We were also running a container to refresh Let\u2019s Encrypt certificates. And then sharing volumes between this container and the nginx container. This article from 2018 shows you the setup. It was a bit of a mess.</p> <p>The other issue was that this process wouldn't work when running locally - for example when you are developing. Your domain would be <code>localhost</code> and Let's Encrypt can't issue certificates for it. So we had to come up with a different process for local dev where we were issuing self-signed certs.</p>"},{"location":"blog/2024/02/09/nginx-out-caddy-in/#welcome-caddy","title":"Welcome Caddy","text":"<p>Oscar, our Google.org AI advisor's first advice when he saw our architecture was to switch to Caddy. Here are the benefits:</p> <ol> <li>It requests and refresh certs from Let's Encrypt.</li> <li>If your domain is localhost, it knows to issue its own certificate.</li> <li>A much smaller and simpler config file.</li> <li>You can use environment variables.</li> </ol> <p>So now our local setup process is the same as prod and requires one fewer containers. Amazing!</p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/","title":"Check out the new Playground","text":"<p>Admin app now has a new Playground page where you can test out the FAQ matching and LLM response endpoints!</p> <p>Here's a screenshot:</p> <p></p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/#why-did-we-build-this","title":"Why did we build this?","text":"<p>Content managers can now test out how the retrieval APIs will perform when new content is added - without leaving the Admin App.</p> <p>By clicking on <code>&lt;json&gt;</code> at the bottom, they can also see the raw JSON response sent back by the server. This include debugging information that may be useful in understanding behaviour.</p> <p></p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/#doc-references","title":"Doc references","text":"<ul> <li>Playground</li> </ul>"},{"location":"blog/2024/03/19/ditching-qdrant-for-pgvector/","title":"Ditching Qdrant for PgVector","text":"<p>In our latest infrastructure update, we decided to transition from Qdrant to pgvector for managing our vector databases. This move is part of our ongoing effort to reduce cost and simplify AAQ\u2019s architecture.</p> <p>This means: - we no longer require a separate Qdrant server. With the pgvector extension, vector data can now be stored in PostgreSQL along with other transactional data. - Seamless transactions between vector data and transactional data will allow for the integration of new features such as multilingual support.</p>"},{"location":"blog/2024/03/19/ditching-qdrant-for-pgvector/#why-this-change","title":"Why this change?","text":"<p>While Qdrant served us well as a dedicated vector database, integrating it with our existing PostgreSQL setup introduced complexity and maintenance overhead. Operating Qdrant alongside PostgreSQL meant managing two distinct database systems with their own infrastructure, architecture, and requirements. Integrating both databases into our codebase required additional integration layers, complicating our codebase.</p> <p>As we already were using Postgresql, pgvector caught our attention with this article as a promising solution that integrates vector database capabilities directly into our existing database. Here's why we decided to go for pgvector: - Simplified Architecture: By adopting pgvector, we significantly reduced the complexity of our data infrastructure. Vector and relational data now reside within the same database, eliminating the need for separate systems. - Improved Response Time: Direct integration with PostgreSQL enhances performance by eliminating the overhead of communicating between separate databases.</p> <p>Moving to pgvector not only benefits our team in terms of reduced complexity and better resource utilization but also lays the groundwork for future innovations such as the multilingual support which should be coming soon.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/","title":"One Endpoint to Rule Them All","text":"<p>We refactored our two question-answering endpoints into a single one called <code>/search</code> for clarity and ease of use.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#why-change","title":"Why change?","text":"<p>We realised our two <code>/embeddings-search</code> and <code>/llm-response</code> endpoints were a bit confusing, and since they performed very similar tasks we combined them into one for ease of use.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#whats-new","title":"What's new?","text":"<p>We now have a single endpoint called <code>/search</code> which always returns the top contents in the database that most closely matched your query and can optionally return an LLM-generated resposne to your question.</p> <p></p> <p>We've also simplified the parameters in our response model so it's easier to use. We will now respond with something like this:</p> <pre><code>{\n    \"query_id\": 1,\n    \"llm_response\": \"Example LLM response \"\n    \"(null if generate_llm_response is False)\",\n    \"search_results\": {\n        \"0\": {\n            \"title\": \"Example content title\",\n            \"text\": \"Example content text\",\n            \"id\": 23,\n            \"distance\": 0.1,\n        },\n        \"1\": {\n            \"title\": \"Another example content title\",\n            \"text\": \"Another example content text\",\n            \"id\": 12,\n            \"distance\": 0.2,\n        },\n    },\n    \"feedback_secret_key\": \"secret-key-12345-abcde\",\n    \"debug_info\": {\"example\": \"debug-info\"},\n}\n</code></pre>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#docs-references","title":"Docs references","text":"<ul> <li>Search</li> </ul>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/","title":"Turn.io Integration Now Available: Your Turn to Play with it","text":"<p>As a question-answering plugin service, Ask A Question requires what we call \"chat managers\" - platforms that help you build end-user chat flows and integrate with channels like WhatsApp.</p> <p>To make this integration process easier, we published a Turn.io Playbook. Playbooks are reusable, pre-built chat flows that can be shared with and remixed by other teams in the chat-for-impact community.</p> <p>Find it here. You may need to sign up to see it.</p>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/#how-do-i-use-it","title":"How do I use it?","text":"<p>Just click on \"Activate Journey\" and you will see this flow imported in your Journeys.</p> <p>Next, replace the <code>&lt;API_KEY&gt;</code> in the code card with your own API key and you should be good to go. If you are using your own deployment, you will also need to change the URL of the API call.</p>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/#stuck","title":"Stuck?","text":"<p>Learn all about Playbooks on Turn's official blog.</p> <p>And don't hesitate to write to us at aaq@idinsight.org!</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/","title":"Introducing Urgency Detection","text":"<p>You may wish to handle urgent messages differently. For example, when deploying a question answering service in a health context, you may wish to refer the user to their nearest health center, or escalate it immediately to a human operator.</p> <p>We introduce a new endpoint and new page in the Admin App to enable this.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#defining-urgency-rules","title":"Defining urgency rules","text":"<p>Using the Admin App, you can define your rules in natural language. For example, here are a few rules borrowed directly from the CDC website on Urgent Maternal Warning Signs:</p> <p></p> <p>It's as simple as that. You don't need to train a model (though you can if you want to. See \"Or write your own\" below).</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#using-the-urgency-detection-endpoint","title":"Using the urgency detection endpoint","text":"<p>You should refer to your Swagger UI/OpenAPI documentation for details but here is a screenshot for us lazy ones:</p> <p></p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#pick-your-method","title":"Pick your method","text":"<p>As of this blog post, there are two ways to determine urgency:</p> <ol> <li>Cosine Distance - It uses the cosine distance between the    input and the urgency rules to determine urgency. It's simple and fast, but may not be    as accurate as the next method.</li> <li>LLM Entailment - This calls an LLM to determine if the message matches the    urgency rules. It's more accurate, but slower. Also, since it's making a call to the LLM, it    is more expensive than the cosine distance method.</li> </ol> <p>See setup sections on how to configure these.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#or-write-your-own","title":"Or write your own","text":"<p>May be you are not happy with either of these and want to try out that new entailment model. All you need to do is define your function with this signature:</p> <pre><code>@urgency_classifier\nasync def your_fancy_method(\n    asession: AsyncSession,\n    urgency_query: UrgencyQuery,\n) -&gt; UrgencyResponse:\n</code></pre> <p>and you can update the <code>URGENCY_CLASSIFIER</code> environment variable to <code>your_fancy_method</code>.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#using-a-different-model","title":"Using a different model","text":"<p>Reminder that we setup a proxy server to make it easy to switch between models. If you want to use a different model, you can host it and update the <code>LITELLM_MODEL_URGENCY_DETECT</code> environment variable to point to your model.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#doc-references","title":"Doc references","text":"<ul> <li>LLM Proxy Server</li> <li>Urgency Detection</li> <li>Managing Urgency Rules</li> </ul>"},{"location":"components/","title":"Components","text":"<p>In this section you can find the different components within AAQ.</p>"},{"location":"components/#user-facing-components","title":"User-facing Components","text":"<p>There are 3 main components in Ask-A-Question.</p> <ul> <li> <p> The Question-Answering Service</p> <p>Integrate with these API endpoints to answer questions from your users.</p> <p> More info</p> </li> <li> <p> The Urgency Detection Service</p> <p>Integrate with this API endpoint to tag messages as urgent or not.</p> <p> More info</p> </li> <li> <p> The Admin App</p> <p>Manage content in the database. Manage urgency detection rules. Test the service in the playground. Explore usage dashboards.</p> <p> More info</p> </li> </ul>"},{"location":"components/#internal-components","title":"Internal Components","text":"<ul> <li> <p> Model Proxy Server</p> <p>AAQ uses the LiteLLM Proxy Server for managing LLM and embedding calls, allowing you to use any LiteLLM supported model (including self-hosted ones).</p> <p> More info</p> </li> <li> <p> Custom Align Score Model</p> <p>(Optional) Use a custom dockerised AlignScore model to catch hallucination and check if LLM response is consistent with the context.</p> <p> More info</p> </li> </ul>"},{"location":"components/admin-app/","title":"The Admin App","text":"<ul> <li> <p> Manage content</p> <p>Allows you to view, create, edit, or delete content.</p> <p> More info</p> </li> <li> <p> Manage urgency rules</p> <p>Allows you to view, create, edit, or delete urgency rules.</p> <p> More info</p> </li> <li> <p> Test</p> <p>Allows you to experiment with the Question-Answering and Urgency Detection. You can use this to simulate a question being asked by a user within a chat window.</p> <p> More info</p> </li> <li> <p> Dashboard</p> <p>Allows you to see statistics like which content is most frequently being used, or the feedback from users on responses provided by the service.</p> <p> More info</p> </li> <li> <p> Engineer's Dashboard (Coming Soon)</p> <p>Shows you technical performance of the application like uptime, throughput, response time, and the number of responses by HTTP response codes</p> </li> </ul>"},{"location":"components/admin-app/#accessing-the-admin-app","title":"Accessing the Admin app","text":"<p>If you have the application running, you can access the admin app at:</p> <pre><code>https://[DOMAIN]/\n</code></pre> <p>or if you are using the dev setup:</p> <pre><code>http://localhost:3000/\n</code></pre>"},{"location":"components/admin-app/dashboard/","title":"Dashboard","text":"<p>The Dashboard provided real-time analytics on the performance of your solution. There are four time filters available: Last 24 hours, Last week, Last month, and Last year.</p> <p></p>"},{"location":"components/admin-app/dashboard/#stay-tuned","title":"Stay tuned","text":"<p>The \"Performance\" and \"Insights\" sections are coming soon. Stay tuned!</p>"},{"location":"components/admin-app/manage-content/","title":"Managing Content","text":"<p>The Admin app allows you to view, add, edit, or delete content in the database.</p> <p>Once logged in, you should see the following screen</p> <p></p>"},{"location":"components/admin-app/manage-content/#upcoming-features","title":"Upcoming features","text":"<ul> <li> Add multiple languages for each content</li> <li> Allow metadata to be captured for each content</li> </ul>"},{"location":"components/admin-app/manage-urgency-rules/","title":"Managing Urgency Rules","text":"<p>The Admin app allows you to view, add, edit, or delete  in the database.</p> <p>Once logged in, navigate to \"Urgency Rules\" from the menu and you should see the following screen</p> <p></p> <p>You can add, edit, or delete urgency rules from this screen.</p>"},{"location":"components/admin-app/playground/","title":"Playground","text":"<p>The Playground allows you to test the Question Answering service using a simple chat window.</p> <p> When accessing it for the first time, you will need to provide the Bearer token to access the backend APIs.</p>"},{"location":"components/align-score/","title":"What is Align Score?","text":"<p>\"AlignScore, a metric for automatic factual consistency evaluation of text pairs\" is introduced in AlignScore: Evaluating Factual Consistency with a Unified Alignment Function by Yuheng Zha, Yichi Yang, Ruichen Li and Zhiting Hu</p> <p>AlignScore returns a score between <code>0</code> (Not consistent) and <code>1</code> (Perfectly consistent) for if the given claims are consistent with the provided contexts.</p> <ul> <li> <p> Deployment</p> <p>Deploying the AlignScore service. Configuring AAQ to use AlignScore.</p> <p> More info</p> </li> <li> <p> Testing</p> <p>Testing how well AlignScore works for your context</p> <p> More info</p> </li> </ul>"},{"location":"components/align-score/deployment/","title":"Deploying and Using AlignScore","text":"<p>The authors of the AlignScore paper have made their code available on Github. We wrap their code (1) in a FastAPI application and expose APIs that can be called by AAQ. The FastAPI application can be deployed using a Docker container.</p> <ol> <li>Actually, we wrap a fork of their code</li> </ol>"},{"location":"components/align-score/deployment/#configure-aaq-to-use-alignscore","title":"Configure AAQ to use AlignScore","text":"<p>You can use AlignScore instead of the default LLM-based content validation by setting <code>ALIGN_SCORE_METHOD</code> to <code>AlignScore</code> in the <code>.env</code> file (1) in the <code>deployment/</code> folder.</p> <ol> <li>If you don't a .env file, make sure you have deployed the app as per instruction in Quick Setup</li> </ol> <pre><code>ALIGN_SCORE_METHOD=AlignScore\n</code></pre>"},{"location":"components/align-score/deployment/#other-configuration","title":"Other configuration","text":"<p>You can also change the threshold score below which the LLM response is considered not consistent with the provided context. <pre><code>ALIGN_SCORE_THRESHOLD=0.7\n</code></pre></p> <p>See Testing on how to decide on this threshold.</p>"},{"location":"components/align-score/deployment/#deploying-the-service","title":"Deploying the service","text":"<p>Follow the deployment instructions in Quick Setup.</p> <p>On Step 6: Run docker-compose, run the following command instead:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    --profile alignScore -p aaq-stack up -d --build\n</code></pre>"},{"location":"components/align-score/deployment/#dev-setup","title":"Dev setup","text":"<p>If you are using the Dev setup, you can start the container manually using:</p> <pre><code>cd optional_components/alignScore\ndocker build -t alignscore-base . # (1)\ndocker run -p 5001:5001 --name align-score-local --detach 'alignscore-base'\n</code></pre> <ol> <li>This step can take long time and even longer if you have a slow internet connection. So if you have already generated the image previously, you can skip this step. Alternatively, you can pull from ECR using <pre><code>docker pull \\\n    public.ecr.aws/j3r7b4k0/alignscore-base:latest\n</code></pre></li> </ol>"},{"location":"components/align-score/deployment/#also-see","title":"Also see","text":"<ol> <li>Quick Setup</li> <li>Configuring AAQ</li> </ol>"},{"location":"components/align-score/testing/","title":"Testing AlignScore","text":"<p>You may wish to check the performance of AlignScore against your task and context. You can also use the performance on the tests to set the best threshold score(1).</p> <ol> <li>See Deployment for what this is.</li> </ol> <p>In this section we describe how to setup the data, the infrastructure, and run the tests.</p>"},{"location":"components/align-score/testing/#setup-the-data","title":"Setup the data","text":"<p>We provide tests and some data to get you started but it highly recommended that you create some new test data that is realistics and appropriate to your context.</p> <p>The test data can be found in: <pre><code>tests/rails/data/llm_response_in_context.yaml\n</code></pre> This shows you the format expected by the tests.</p>"},{"location":"components/align-score/testing/#setup-the-infrastructure","title":"Setup the infrastructure","text":"<p>In order to run these test, you need to setup the AlignScore service. See instructions in Dev setup on how to setup a local container.</p>"},{"location":"components/align-score/testing/#run-the-tests","title":"Run the tests","text":"<p>Once you have data ready and the AlignScore container running, you are ready to run the tests:</p> <pre><code>cd ../../core_backend\npytest -m rails -k \"test_alignScore\" -vv\n</code></pre>"},{"location":"components/litellm-proxy/","title":"LLM Proxy Server","text":""},{"location":"components/litellm-proxy/#what-is-it","title":"What is it?","text":"<p>AAQ uses the LiteLLM Proxy Server for managing LLM calls, allowing you to use any LiteLLM supported model including self-hosted ones.</p> <p>This proxy server runs as a separate Docker container with configs read from a <code>config.yaml</code> file, where you can set the appropriate model names and endpoints for each LLM task.</p>"},{"location":"components/litellm-proxy/#example-config","title":"Example config","text":"<p>You can see an example <code>litellm_proxy_config.yaml</code> file below. In our backend code, we refer to the models by their custom task <code>model_name</code> (e.g. \"generate-response\"), but which actual LLM model each call is routed to is set here.</p> <pre><code>model_list:\n  - model_name: embeddings\n    litellm_params:\n      model: text-embedding-ada-002\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: default\n    litellm_params:\n      model: gpt-4-0125-preview\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: generate-response\n    litellm_params:\n      model: gpt-4-0125-preview\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: detect-language\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: translate\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: paraphrase\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: safety\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: alignscore\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\nlitellm_settings:\n  num_retries: 3\n  request_timeout: 100\n  telemetry: False\n</code></pre> <p>See the Contributing Setup and Docker Compose Setup for how this service is run in our stack.</p>"},{"location":"components/litellm-proxy/#also-see","title":"Also see","text":"<ul> <li>Latest Updates: Adding a model proxy server</li> </ul>"},{"location":"components/qa-service/","title":"Question-Answering Service","text":"<p><code>/search</code> is the flagship endpoint that your application can integrate with:</p> <ul> <li> <p> Search</p> <p>Allows searching through the content database using embeddings similarity and optionally creates an LLM-generated answer to the user's question.</p> <p> More info</p> </li> </ul>"},{"location":"components/qa-service/#capturing-feedback","title":"Capturing Feedback","text":"<p>The service also provides two endpoint to capture feedback from users. One to capture feedback for the response returned and the other for the content items retrieved.</p> <p>For both of these, you can provide feedback as sentiment (\"positive\", \"negative\"), as text, or as both.</p> <ul> <li> <p> Response Feedback</p> <p>Allows users to provide feedback on the response generated by the either Semantic Search or LLM Response.</p> <p> More info</p> </li> <li> <p> Content Feedback</p> <p>Allows users to provide feedback on the content items returned by the Semantic Search.</p> <p> More info</p> </li> </ul>"},{"location":"components/qa-service/#swaggerui","title":"SwaggerUI","text":"<p>If you have the application running, you can access the SwaggerUI at</p> <pre><code>https://[DOMAIN]/api/docs\n</code></pre> <p>or if you are using the dev setup:</p> <pre><code>http://localhost:8000/docs\n</code></pre>"},{"location":"components/qa-service/#upcoming","title":"Upcoming","text":"<ul> <li> Chat capability</li> </ul>"},{"location":"components/qa-service/content-feedback/","title":"Content Feedback","text":"<p>This service allows you to provide feedback for individual content items in the database. The feedback can be used to improve the quality of the content and the search results.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/response-feedback/","title":"Response Feedback","text":"<p>This service captures feedback for the response return by Semantic Search or LLM Response.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/search/","title":"Search","text":"<p>This service returns the contents from the database with the most similar vector embeddings to the question and optionally also uses an LLM to construct a custom answer to the user's question using the retrieved contents.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/search/#process-flow-without-llm-response-generation","title":"Process flow without LLM response generation","text":"<pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Translate text\n  LLM-&gt;&gt;AAQ: &lt;Translated text&gt;\n  AAQ-&gt;&gt;LLM: Paraphrase question\n  LLM-&gt;&gt;AAQ: &lt;Paraphrased question&gt;\n  AAQ-&gt;&gt;Vector DB: Request N most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;User: Return JSON of N contents\n</code></pre>"},{"location":"components/qa-service/search/#process-flow-with-llm-response-generation","title":"Process flow with LLM response generation","text":"<pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Check for safety\n  LLM-&gt;&gt;AAQ: &lt;Safety Classification&gt;\n  AAQ-&gt;&gt;Vector DB: Request N most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;LLM: Given contents, construct response in user's language to question\n  LLM-&gt;&gt;AAQ: &lt;LLM response&gt;\n  AAQ-&gt;&gt;LLM: Check if LLM response is consistent with contents\n  LLM-&gt;&gt;AAQ: &lt;Consistency score&gt;\n  AAQ-&gt;&gt;User: Return JSON of LLM response and N contents\n</code></pre>"},{"location":"components/qa-service/search/#optional-components","title":"Optional components","text":""},{"location":"components/qa-service/search/#align-score","title":"Align Score","text":"<p>In the Process Flow above, Step 12: Check if answer is consistent with content can be done using a custom AlignScore model instead of another LLM call.</p> <pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Check for safety\n  LLM-&gt;&gt;AAQ: &lt;Safety Classification&gt;\n  AAQ-&gt;&gt;Vector DB: Request N most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;LLM: Construct response to question given contents\n  LLM-&gt;&gt;AAQ: &lt;LLM response&gt;\n  AAQ-&gt;&gt;Custom AlignScore Model: Check if LLM response is consistent with contents\n  Custom AlignScore Model -&gt;&gt;AAQ: &lt;Consistency score&gt;\n  AAQ-&gt;&gt;User: Return JSON of LLM response and N contents\n</code></pre> <p>To use the custom AlignScore model, AAQ needs access to the AlignScore service. See documentation for how to setup the service and configure AAQ to call it.</p>"},{"location":"components/urgency-detection/","title":"Urgency Detection","text":"<p>This service returns if the the message is urgent or not. There are currently two methods available to do this.</p>"},{"location":"components/urgency-detection/#method-1-cosine-distance","title":"Method 1: Cosine distance","text":"<ul> <li>Cost: </li> <li>Accuracy: </li> <li>Latency: </li> </ul> <p>This method uses the cosine distance between the input message and the urgency rules in the database. Since it only uses embeddings, it is fast and cheap to run.</p>"},{"location":"components/urgency-detection/#setup","title":"Setup","text":"<p>Set the following environment variables.</p> <ol> <li>Set <code>URGENCY_CLASSIFIER</code> environment variable to <code>cosine_distance_classifier</code>.</li> <li>Set <code>URGENCY_DETECTION_MAX_DISTANCE</code> environment variable. Any message with a cosine distance greater than this value will be tagged as urgent.</li> </ol> <p>You can do this either in the <code>.env</code> file or under <code>core_backend/app/urgency_detection/config.py</code>. See Configuring AAQ for more details.</p>"},{"location":"components/urgency-detection/#method-2-llm-entailment-classifier","title":"Method 2: LLM entailment classifier","text":"<ul> <li>Cost: </li> <li>Accuracy: </li> <li>Latency: </li> </ul> <p>This method calls an LLM to score the message against each of the urgency rules in the database.</p>"},{"location":"components/urgency-detection/#setup_1","title":"Setup","text":"<p>Set the following environment variables.</p> <ol> <li>Set <code>URGENCY_CLASSIFIER</code> environment variable to <code>llm_entailment_classifier</code>.</li> <li>Set <code>URGENCY_DETECTION_MIN_PROBABILITY</code> environment variable. The LLM returns the probability of a message being urgent. Any message with a probability greater than this value will be tagged as urgent.</li> </ol> <p>You can do this either in the <code>.env</code> file or under <code>core_backend/app/urgency_detection/config.py</code>. See Configuring AAQ for more details.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/urgency-detection/#more-details","title":"More details","text":"<ul> <li>Blog post on Urgency Detection.</li> <li>SwaggerUI</li> </ul>"},{"location":"deployment/architecture/","title":"Architecture","text":"<p>We use docker-compose to orchestrate containers with a reverse proxy that manages all incoming traffic to the service. The database and LiteLLM proxy are only accessed by the core app.</p> <p> </p>"},{"location":"deployment/config-options/","title":"Configuring AAQ","text":"<p>There are compulsory configs you must set like secrets and API keys. These are all set in the .env files as shown in Steps 3 and 4 in Quick Setup.</p>"},{"location":"deployment/config-options/#other-parameters","title":"Other parameters","text":"<p>In addition to these, you can modify a bunch of other parameters by either:</p> <ul> <li>Setting environment variables in the <code>.env</code> file; or</li> <li>Updating the config directly in <code>core_backend/app/config.py</code>.</li> </ul> <p>For example, you set a different LLM to be used for each guardrail step.</p> Environment variables take precedence over the config file. <p>You'll see in the config files that we get parameters from the environment and if not found, we fall back on defaults provided. So any environment variables set will override any defaults you have set in the config file.</p>"},{"location":"deployment/config-options/#application-parameters","title":"Application parameters","text":"<p>You can find parameters than control the behaviour of the app at <code>core_backend/app/config.py</code></p> <p>For a number of optional components like offline models, you will need to update the parameters in this file.</p> <p>See instructions for setting these in the documentation for the specific optional component.</p>"},{"location":"deployment/config-options/#updating-llm-prompts","title":"Updating LLM prompts","text":"<p>You may wish to customize the prompts for your specific context. These are all found in <code>core_backend/app/llm_call/llm_prompts.py</code></p>"},{"location":"deployment/config-options/#tracing-with-langfuse","title":"Tracing with Langfuse","text":"<p>To connect your AAQ to Langfuse, <code>core_backend</code> container needs the following environment variables set:</p> <pre><code>LANGFUSE=True # by default, False\nLANGFUSE_PUBLIC_KEY=pk-...\nLANGFUSE_SECRET_KEY=sk-...\nLANGFUSE_HOST=https://cloud.langfuse.com # optional based on your Langfuse host\n</code></pre> <p>Also see LiteLLM's Langfuse Integration docs.</p>"},{"location":"deployment/quick-setup/","title":"Quick Setup with Docker Compose","text":"<p>You need to have installed Docker</p> <p>Step 1: Clone the AAQ repository.</p> <pre><code>git clone git@github.com:IDinsight/ask-a-question.git\n</code></pre> <p>Step 2: Navigate to the <code>deployment/docker-compose/</code> subfolder.</p> <p>Step 3: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables. If <code>DOMAIN</code> is commented out, it defaults to <code>localhost</code>.</p> <p>Step 4: If you want to change which LLMs are used, edit <code>litellm_proxy_config.yaml</code>. See LiteLLM Proxy Server for more details.</p> <p>Step 5: Run docker-compose</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack up -d --build\n</code></pre> <p>You can now view the AAQ admin app at <code>https://$DOMAIN/</code> (e.g. <code>https://localhost/</code>) and the API documentation at <code>https://$DOMAIN/api/docs</code> (you can also test the endpoints here).</p> <p>Step 6: Shutdown containers</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -p aaq-stack down\n</code></pre>"},{"location":"develop/contributing/","title":"Contributing to AAQ","text":"<p>Thank you for your interest in contributing to AAQ!</p> <p>AAQ is an open source project started by data scientists at IDinsight and sponsored by Google.org. Everyone is welcome to contribute! </p> <p>Note</p> <p>If you want to set up the complete development environment for AAQ first, you can follow the setup instructions here. Otherwise, this page provides general guidelines on how to contribute to the project with a minimal setup.</p>"},{"location":"develop/contributing/#pull-requests-guide","title":"Pull requests guide","text":"<p>This section shows you how to raise a pull request to the project.</p>"},{"location":"develop/contributing/#make-a-fork","title":"Make a fork","text":"<ol> <li>Fork the project repository by clicking on the \"Fork\" button.</li> <li> <p>Clone the repo using:</p> <pre><code>git clone git@github.com:&lt;your GitHub handle&gt;/ask-a-question.git\n</code></pre> </li> <li> <p>Navigate to the project directory.</p> <pre><code>cd ask-a-question\n</code></pre> </li> </ol>"},{"location":"develop/contributing/#install-prerequisites","title":"Install prerequisites","text":"<p>Install conda.</p>"},{"location":"develop/contributing/#setup-your-virtual-python-environment","title":"Setup your virtual Python environment","text":"<p>You can automatically create a ready-to-go <code>aaq</code> conda environment with:</p> <pre><code>make fresh-env\nconda activate aaq\n</code></pre> <p>If you encounter errors while installing <code>psycopg2</code>, see here for troubleshooting.</p> <p>If you would like to set up your Python environment manually, you can follow the steps here.</p>"},{"location":"develop/contributing/#make-your-changes","title":"Make your changes","text":"<ol> <li> <p>Create a <code>feature</code> branch for your development changes:</p> <pre><code>git checkout -b feature\n</code></pre> </li> <li> <p>Run <code>mypy</code> with <code>mypy core_backend/app</code> (1)</p> </li> <li> <p>Then <code>git add</code> and <code>git commit</code> your changes:</p> <pre><code>git add modified_files\ngit commit\n</code></pre> </li> <li> <p>And then push the changes to your fork in GitHub</p> <pre><code>git push -u origin feature\n</code></pre> </li> <li> <p>Go to the GitHub web page of your fork of the AAQ repo. Click the <code>Pull request</code> button to send your changes to the project\u2019s maintainers for review. This will send a notification to the committers.</p> </li> </ol> <ol> <li><code>pre-commit</code> runs in its own virtual environment. Since <code>mypy</code> needs all the    packages installed, this would mean keeping a whole separate copy of your    environment just for it. That's too bulky so the pre-commit only checks    a few high-level typing things. You still need to run <code>mypy</code> directly to catch    all the typing issues.    If you forget, GitHub Actions 'Linting' workflow will pick up all the mypy errors.</li> </ol>"},{"location":"develop/contributing/#next-steps","title":"Next steps","text":"<p>If you haven't already, see Setup on how to set up the complete development environment for AAQ. Otherwise, you can check out how to test the AAQ codebase.</p>"},{"location":"develop/setup/","title":"Setting up your development environment","text":"<p>There are two ways to set up your development environment for AAQ:</p> <ol> <li>Docker Compose Watch</li> <li>Manual</li> </ol> <p>You can view the pros and cons of each method at the bottom.</p> <p>Before you get started, please fork the project repository by clicking on the \"Fork\" button and then clone the repo using:</p> <pre><code>git clone git@github.com:&lt;your GitHub handle&gt;/ask-a-question.git\n</code></pre> <p>For questions related to setup, please contact AAQ Support</p>"},{"location":"develop/setup/#set-up-using-docker-compose-watch","title":"Set up using Docker Compose Watch","text":""},{"location":"develop/setup/#step-0-install-prerequisites","title":"Step 0: Install prerequisites","text":"<ol> <li>Install Docker.</li> <li>If you are not using Docker Desktop, install Docker Compose with version &gt;=2.22 to use the <code>watch</code> command.</li> </ol>"},{"location":"develop/setup/#step-1-configure-environment-variables","title":"Step 1: Configure environment variables","text":"<ol> <li> <p>Navigate to the <code>deployment/docker-compose</code> directory.</p> <pre><code>cd ask-a-question/deployment/docker-compose\n</code></pre> </li> <li> <p>Copy <code>template.env</code> to a new file named <code>.env</code> within the same directory, and set    the necessary variables. For local setup, you just need to set your own    <code>OPENAI_API_KEY</code> as the app can use default values for other environment variables    (check out the various <code>config.py</code> under <code>core_backend/app/</code> and its subdirectories.)</p> </li> <li> <p>(optional) Edit which LLMs are used in the <code>litellm_proxy_config.yaml</code>.</p> </li> </ol>"},{"location":"develop/setup/#step-2-run-docker-compose-watch","title":"Step 2: Run <code>docker compose watch</code>","text":"<ol> <li> <p>In the <code>deployment/docker-compose</code> directory, run</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -p aaq-stack watch\n</code></pre> Here's what you should see if the above command executes successfully <pre><code>\u2714 Network aaq-stack_default            Created\n\u2714 Volume \"aaq-stack_db_volume\"         Created\n\u2714 Volume \"aaq-stack_caddy_data\"        Created\n\u2714 Volume \"aaq-stack_caddy_config\"      Created\n\u2714 Container aaq-stack-caddy-1          Started\n\u2714 Container aaq-stack-litellm_proxy-1  Started\n\u2714 Container aaq-stack-relational_db-1  Started\n\u2714 Container aaq-stack-core_backend-1   Started\n\u2714 Container aaq-stack-admin_app-1      Started\nWatch enabled\n</code></pre> <p>The app will now run and update with any changes made to the <code>core_backend</code> or <code>admin_app</code> folders.</p> <p>The admin app will be available on https://localhost and the backend API testing UI on https://localhost/api/docs.</p> </li> <li> <p>To stop the admin app, first exit the running app process in your terminal with <code>ctrl+c</code> and then run:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -p aaq-stack down\n</code></pre> </li> </ol>"},{"location":"develop/setup/#set-up-manually","title":"Set up manually","text":""},{"location":"develop/setup/#step-0-install-prerequisites_1","title":"Step 0: Install prerequisites","text":"<ol> <li>Install    conda.</li> <li> <p>Install Node.js v19.</p> Setting up a different NodeJS version <p>If you have a different NodeJS version installed already, you can switch to a different version by installing Node Version Manager and then executing</p> <pre><code>nvm install 19\nnvm use 19\n</code></pre> </li> <li> <p>Install Docker.</p> </li> <li>Install PostgreSQL.</li> <li>(optional) Install GNU Make. <code>make</code> is used to run commands (targets) in <code>Makefile</code>s and can be used to automate various setup procedures.</li> </ol>"},{"location":"develop/setup/#step-1-set-up-the-backend","title":"Step 1: Set up the backend","text":"<p>Note</p> <p>Ensure that you are in the <code>ask-a-question</code> project directory before proceeding.</p> <ol> <li> <p>Set up your Python environment by creating a new conda environment using <code>make</code>.</p> <pre><code>make fresh-env\n</code></pre> <p>This command will remove any existing <code>aaq</code> conda environment and create a new <code>aaq</code> conda environment with the required Python packages.</p> <p></p> Errors with <code>psycopg2</code>? <p><code>psycopg2</code> vs <code>psycopg2-binary</code>: For production use cases we should use <code>psycopg2</code> but for local development, <code>psycopg2-binary</code> suffices and is often easier to install. If you are getting errors from <code>psycopg2</code>, try installing <code>psycopg2-binary</code> instead.</p> <p>If you would like <code>psycopg2-binary</code> instead of <code>psycopg2</code>, run <code>make fresh-env psycopg_binary=true</code> instead (see below for why you may want this).</p> <p>See here for more details.</p> <p></p> Setting up the Python environment manually <p>Create virtual environment</p> <pre><code>conda create --name aaq python=3.10\n</code></pre> <p>Install Python packages</p> <pre><code>pip install -r core_backend/requirements.txt\npip install -r requirements-dev.txt\n</code></pre> <p>Install pre-commit</p> <p><code>pre-commit</code> is used to ensure that code changes  are formatted correctly. It is only necessary if you are planning on making  changes to the codebase.</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>Activate your <code>aaq</code> conda environment.</p> <pre><code>conda activate aaq\n</code></pre> </li> <li> <p>Set required environment variables in your terminal.</p> <pre><code># 1. Required for core_backend\nexport PROMETHEUS_MULTIPROC_DIR=/tmp\n</code></pre> </li> <li> Set up optional environment variables <pre><code>1. To enable Google login\nexport NEXT_PUBLIC_GOOGLE_LOGIN_CLIENT_ID=&lt;YOUR_CLIENT_ID&gt;\n\n2. If you want to track using LANGFUSE\nexport LANGFUSE=True\nexport LANGFUSE_PUBLIC_KEY=pk-...\nexport LANGFUSE_SECRET_KEY=sk-...\n\n3. API keys for LLM models used in litellm_proxy_config.yaml. This step is only required if you are using aspects of AAQ that require calls to API providers.\nexport OPENAI_API_KEY=sk... # if using OpenAI\nexport GEMINI_API_KEY=... # if using Gemini\n\n4. Edit which LLMs are used in the `deployment/docker-compose/litellm_proxy_config.yaml`.\n</code></pre> </li> <li> <p> You can set custom login credentials for the frontend admin app by setting the following environment variables. The defaults can be found in <code>/core_backend/add_users_to_db.py</code>.</p> <pre><code># user 1\nexport USER1_USERNAME=\"user1\"\nexport USER1_PASSWORD=\"fullaccess\"\nexport USER1_API_KEY=\"user1-key\"\n\n# user 2\nexport USER2_USERNAME=\"user2\"\nexport USER2_PASSWORD=\"fullaccess\"\nexport USER2_API_KEY=\"user2-key\"\n</code></pre> </li> <li> <p>Set up the required Docker containers for the <code>PostgreSQL</code> database and the <code>LiteLLM</code> proxy server using <code>make</code>.</p> <pre><code>make setup-dev\n</code></pre> Setting up the <code>PostgreSQL</code> database and <code>LiteLLM</code> proxy server separately <p>The <code>make setup-dev</code> command should set up the <code>PostgreSQL</code> database docker container and <code>LiteLLM</code> proxy server automatically.  If you would like to set up each of these separately, here are the steps:</p> <p>NB: To run each of the <code>make</code> steps manually, you can check out the individual steps in each of the corresponding Make targets.</p> </li> <li> <p>Start the backend app.</p> <pre><code>python core_backend/main.py\n</code></pre> Here's what you should see if the above command executes successfully <pre><code>INFO:     Will watch for changes in these directories: ['~/ask-a-question']\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [73855] using StatReload\nINFO:     Started server process [73858]\nINFO:     Waiting for application startup.\n07/15/2024 12:29:08 PM          __init__.py  79 : Application started\nINFO:     Application startup complete.\n</code></pre> <p>This will launch the application in \"reload\" mode (i.e., the app will automatically refresh everytime you make a change to one of the files).</p> <p>You can test the endpoint for the API documentation by going to http://localhost:8000/docs (the backend itself runs on http://localhost:8000).</p> </li> <li> <p>To stop the backend app, first exit the running app process in your terminal with <code>ctrl+c</code> and then run:</p> <pre><code>make teardown-dev\n</code></pre> </li> </ol>"},{"location":"develop/setup/#postgresql-database-on-docker","title":"<code>PostgreSQL</code> database on Docker","text":"<p>You can launch a container running <code>PostgreSQL</code> database and run the necessary migrations using:</p> <pre><code>make setup-db\n</code></pre> <p>You can stop and remove the <code>PostgreSQL</code> container using:</p> <pre><code>make teardown-db\n</code></pre>"},{"location":"develop/setup/#litellm-proxy-server","title":"LiteLLM Proxy Server","text":"<pre><code>1. Set models and parameters in `deployment/docker-compose/litellm_proxy_config.yaml`.\n\n2. Set the appropriate API key(s) as environment variables in your terminal:\n\n    export OPENAI_API_KEY=sk... # if using OpenAI\n    export GEMINI_API_KEY=... # if using Gemini\n\n3. Run the Make target to set up the `LiteLLM` proxy server:\n\n    make setup-llm-proxy\n\n4. Once done, teardown the container with:\n\n    make teardown-llm-proxy\n</code></pre>"},{"location":"develop/setup/#step-2-set-up-the-frontend","title":"Step 2: Set up the frontend","text":"<p>NB</p> <p>Ensure that you are in the <code>ask-a-question</code> project directory and that you have activated the <code>aaq</code> virtual environment before proceeding.</p> <ol> <li> <p>In a new terminal:</p> <pre><code>cd admin_app\nnvm use 19\nnpm install\nnpm run dev\n</code></pre> Here's what you should see if the above commands execute successfully <pre><code>&gt; admin-app@0.2.0 dev\n&gt; next dev\n\n\u25b2 Next.js 14.1.3\n- Local:        http://localhost:3000\n\n\u2713 Ready in 1233ms\n</code></pre> <p>This will install the required NodeJS packages for the admin app and start the admin app in <code>dev</code> (i.e., autoreload) mode. The admin app will now be accessible on http://localhost:3000/.</p> <p>You can login with either the default credentials (username: <code>admin</code>, password: <code>fullaccess</code>) or the ones you specified when setting up the login credentials.</p> </li> <li> <p>To stop the admin app, exit the running app process in your terminal with <code>ctrl</code>+<code>c</code>.</p> </li> </ol>"},{"location":"develop/setup/#set-up-docs","title":"Set up docs","text":"<p>NB</p> <p>Ensure that you are in the <code>ask-a-question</code> project directory and that you have activated the <code>aaq</code> virtual environment before proceeding.</p> <ol> <li>To host docs offline so that you can see documentation changes in real-time, run the following from <code>ask-a-question</code> repository root with an altered port (so that it doesn't interfere with the app's server):<pre><code>mkdocs serve -a localhost:8080\n</code></pre> </li> </ol>"},{"location":"develop/setup/#pros-and-cons-of-each-setup-method","title":"Pros and cons of each setup method","text":"Method Pros Cons Set up using docker compose watch <ul><li>Good for end-to-end testing</li><li>Local environment identical to production deployment</li><li>No need to setup local environment</li><li>Set environment variables and configs once</li></ul> <ul><li>Changes take 20-30s to be reflected in the app</li></ul> Set up manually <ul><li>Instant feedback from changes</li></ul> <ul><li>Requires more configuration before each run</li><li>Requires environment and dependencies to be set up correctly</li><ul>"},{"location":"develop/testing/","title":"Writing and running tests","text":"<p>If you are writing new features, you should also add unit tests. Tests are under <code>core_backend/tests</code>.</p>"},{"location":"develop/testing/#running-unit-tests","title":"Running unit tests","text":"<p>You need to have installed Docker</p> Don't run <code>pytest</code> directly <p>Unless you have updated your environment variables and started a testing instance of postrges, the tests will end up writing to your dev environment </p> <p>Run tests using:</p> <pre><code>make tests\n</code></pre> <p>This target starts up new postgres and qdrant containers for testing. It also sets the correct environment variables, runs <code>pytest</code>, and then destroys the containers.</p>"},{"location":"develop/testing/#debugging-unit-tests","title":"Debugging unit tests","text":"<p>Before debugging, run <code>make setup-test-db</code> within <code>core_backend</code> to launch new postgres container for testing and set the correct environment variables.</p> <p>After debugging, clean up the testing resources by calling <code>make teardown-test-db</code>.</p>"},{"location":"develop/testing/#configs-for-visual-studio-code","title":"Configs for Visual Studio Code","text":"<code>.vscode/launch.json</code> <p>Add the following configuration to your <code>.vscode/launch.json</code> file to set environment variables for debugging:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {  // configuration for debugging\n            \"name\": \"Python: Tests in current file\",\n            \"purpose\": [\"debug-test\"],\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"args\": [\"--color=yes\"],\n            \"envFile\": \"${workspaceFolder}/core_backend/tests/api/test.env\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": false\n        }\n    ]\n}\n</code></pre> <code>.vscode/settings.json</code> <p>Add the following configuration to <code>.vscode/settings.json</code> to set the correct pytest working directory and environment variables:</p> <pre><code>{\n    \"python.testing.cwd\": \"${workspaceFolder}/core_backend\",\n    \"python.testing.pytestArgs\": [\n        \"tests\",\n        \"--rootdir=${workspaceFolder}/core_backend\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true,\n    \"python.envFile\": \"${workspaceFolder}/core_backend/tests/api/test.env\"\n}\n</code></pre>"},{"location":"develop/testing/#running-optional-tests","title":"Running optional tests","text":"<p>There are some additional tests that are not run by default. Most of these either make call to OpenAI or depend on other components:</p> <ul> <li>Language Identification: Tests if the the solution is able to identify the language given a short sample text.</li> <li>Test if LLM response aligns with provided context: Tests for hallucinations by checking if the LLM response is supported by the provided context.</li> <li>Test safety: Tests for prompt injection and jailbreaking.</li> </ul> <p>These tests will require the LiteLLM Proxy server to be running (to accept LLM calls). You can run this by going to root and running:</p> <pre><code>make setup-llm-proxy\n</code></pre> <p>Then run the tests using:</p> <pre><code>cd core_backend\nmake setup-test-db\npython -m pytest -m rails\n</code></pre> <p>And when done:</p> <pre><code>make teardown-test-db\n</code></pre>"},{"location":"develop/validation/","title":"Running validation","text":"<p>Currently, there is validation only for retrieval, i.e. <code>POST /search</code> endpoint with <code>\"generate_llm_response\": false</code></p> <p>To evaluate the performance of your model (along with your own configurations and guardrails), run the validation test(s) in <code>core_backend/validation</code>.</p>"},{"location":"develop/validation/#retrieval-search-validation","title":"Retrieval (<code>/search</code>) validation","text":"<p>We evaluate the \"performance\" of retrieval by computing \"Top K Accuracy\", which is defined as proportion of times the best matching answer was present in top K retrieved contents.</p>"},{"location":"develop/validation/#preparing-the-data","title":"Preparing the data","text":"<p>The test assumes the validation data contains a single label representing the best matching content, rather than a ranked list of all relevant content.</p> <p>An example validation data will look like</p> query label \"How?\" 0 \"When?\" 1 \"What year was it?\" 1 \"May I?\" 2 <p>An example content data will look like</p> content_text label \"Here's how.\" 0 \"It was 2024.\" 1 \"Yes\" 2"},{"location":"develop/validation/#setting-up","title":"Setting up","text":"<ol> <li>Create a new python environment:     <pre><code>conda create -n \"aaq-validate\" python=3.10\n</code></pre>     You can also copy the existing <code>aaq</code> environment.</li> <li>Install requirements. This assumes you are in project root <code>ask-a-question</code>.     <pre><code>conda activate aaq-validate\npip install -r core_backend/requirements.txt\npip install -r core_backend/validation/requirements.txt\n</code></pre></li> <li> <p>Set environment variables.</p> <ol> <li> <p>You must export the required environment variables. They are defined with default values in     <code>core_backend/validation/validation.env</code>. To ensure that these env variables are     set every time you activate <code>aaq-validate</code>, you can run the     following command for each variable:     <pre><code>conda env config vars set &lt;VARIABLE&gt;=&lt;VALUE&gt;\n</code></pre></p> </li> <li> <p>For optional ones, check out the defaults in <code>core_backend/app/configs/app_config.py</code>     and modify as per your own requirements. For example:     <pre><code>conda env config vars set LITELLM_MODEL_EMBEDDING=&lt;...&gt;\n</code></pre></p> </li> <li>If you are using an external LLM endpoint, e.g. OpenAI, make sure to export the     API key variable as well.     <pre><code>conda env config vars set OPENAI_API_KEY=&lt;Your OPENAI API key&gt;\n</code></pre></li> </ol> </li> </ol>"},{"location":"develop/validation/#running-retrieval-validation","title":"Running retrieval validation","text":"<p>In project root <code>ask-a-question</code> run the following command. (Perform any necessary    authentication steps you need to do, e.g. for AWS login).     <pre><code>cd ask-a-question\n\npython -m pytest core_backend/validation/validate_retrieval.py \\\n    --validation_data_path &lt;path&gt; \\\n    --content_data_path &lt;path&gt; \\\n    --validation_data_question_col &lt;name&gt; \\\n    --validation_data_label_col &lt;name&gt; \\\n    --content_data_label_col &lt;name&gt; \\\n    --content_data_text_col &lt;name&gt; \\\n    --notification_topic &lt;topic ARN, if using AWS SNS&gt; \\\n    --aws_profile &lt;aws SSO profile name, if required&gt; \\\n    -n auto -s\n</code></pre> <code>-n auto</code> allows multiprocessing to speed up the test, and <code>-s</code> ensures logging by     the test module is shown on your stdout.</p> <pre><code>For details of the command line arguments, see the \"Custom options\" section of the\noutput for the following command:\n```shell\npython -m pytest core_backend/validation/validate_retrieval.py --help\n```\n</code></pre>"},{"location":"integrations/","title":"Integrations","text":"<p>In this section you can find the different integrations AAQ supports so far.</p>"},{"location":"integrations/#chat-managers","title":"Chat Managers","text":"<p>You can use the AAQ endpoints through various chat managers. Below are some examples:</p> <ul> <li> <p></p> <p>How to get Typebot running and connected to AAQ endpoints</p> <p> More info</p> </li> <li> <p> Botpress</p> <p>How to get Botpress v12 (OSS) running and connected to AAQ endpoints</p> <p> More info</p> </li> <li> <p></p> <p>How to connect a Turn.io Journey to AAQ endpoints</p> <p> More info</p> </li> <li> <p></p> <p>How to connect a Glific flow to AAQ endpoints</p> <p> More info</p> </li> </ul>"},{"location":"integrations/chat_managers/botpress_v12/","title":"Botpress v12  Setup Instructions","text":"<p>Below is an example of how to get Botpress v12 (OSS) running and connected to AAQ endpoints using a provided demo flow.</p> <p>Note: Botpress v12 is open-source and available to self-host but Botpress Cloud is a different closed-source product.</p>"},{"location":"integrations/chat_managers/botpress_v12/#demo-aaq-flow","title":"Demo AAQ flow","text":"<ol> <li>Once you have deployed Botpress v12 as per your requirements, go to the URL where the app is running</li> <li>Make an account and login</li> <li>Go to \"Create Bot\" and then \"Import Existing\" (you can set Bot ID to anything you want)</li> <li>Load the <code>.tgz</code> file given under <code>chat_managers/botpress_v12/</code> in the AAQ repo</li> <li> <p>Edit the \"API Call\" cards to reflect the AAQ endpoint URL that you have running *</p> <p>a. Click on the card</p> <p>b. Click on \"Edit skill\"</p> <p>c. Change the base of the URL at the top</p> <p>d. If you've changed the bearer token for the QA endpoints, you'll have to update the headers sections too</p> </li> <li> <p>Test the bot in the emulator</p> </li> </ol> * Errors with using <code>localhost</code> on the API Call skill? <p>If you're having trouble with localhost AAQ calls, try forwarding traffic through <code>ngrok</code> and using that for deployment of AAQ.</p> <ol> <li> <p>Install and configure ngrok</p> </li> <li> <p>Run <code>ngrok http https://localhost</code> to forward traffic</p> </li> <li> <p>In <code>deployment/.env</code> file, ensure you have</p> <pre><code>NEXT_PUBLIC_BACKEND_URL=https://[NGROK URL]/api\nBACKEND_ROOT_PATH=\"/api\"\n</code></pre> </li> <li> <p>In <code>deployment/.env.nginx</code> file ensure you have</p> <pre><code>DOMAIN=[NGROK URL]  # don't add https/http at the front\n</code></pre> </li> <li> <p>Change the base of the API Call skill so it looks like:</p> <pre><code>[NGROK URL]/api/search\n</code></pre> </li> </ol>"},{"location":"integrations/chat_managers/botpress_v12/#self-hosted-deployment","title":"Self-hosted deployment","text":""},{"location":"integrations/chat_managers/botpress_v12/#option-1-via-docker-compose-behind-caddy-with-https","title":"Option 1 - Via Docker Compose (behind Caddy with HTTPS)","text":"<p>Step 1: Navigate to <code>chat_managers/botpress_v12/deployment/</code></p> <p>Step 2: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables</p> <p>Step 3: Run docker compose</p> <pre><code>docker compose -p botpress-stack up -d --build\n</code></pre> <p>You can now access Botpress at <code>https://[DOMAIN]/</code></p> <p>Step 4: Shutdown containers</p> <pre><code>docker compose -p botpress-stack down\n</code></pre>"},{"location":"integrations/chat_managers/botpress_v12/#option-2-via-docker","title":"Option 2 - Via Docker","text":"<p>To install through Docker (recommended), follow the official Botpress v12 docs here. In short:</p> <ol> <li> <p>Get the image</p> <pre><code>docker pull botpress/server\n</code></pre> </li> <li> <p>Run the image</p> <pre><code>docker run -d --name=botpress -p 3000:3000 botpress/server\n</code></pre> </li> </ol>"},{"location":"integrations/chat_managers/botpress_v12/#option-3-via-executables","title":"Option 3 - Via executables","text":"<p>Follow the official docs here to set up Botpress v12 locally as per your OS.</p>"},{"location":"integrations/chat_managers/typebot/","title":"Setup Instructions","text":"<p>Below is a guide on how to connect to AAQ endpoints using a provided demo flow.</p> <p>You can either use the cloud-hosted Typebot service or self-host the application (see \"Deployment\" below).</p>"},{"location":"integrations/chat_managers/typebot/#demo-aaq-flow","title":"Demo AAQ flow","text":"<ol> <li>Go to your Typebot instance</li> <li>Make an account and login</li> <li>Go to \"Create a typebot\" and then \"Import a file\"</li> <li>Load the <code>.json</code> file given under <code>chat_managers/typebot/</code> in the AAQ repo</li> <li> <p>Edit the \"API Call\" cards to reflect the AAQ endpoint URL that you have running</p> <p>a. Click on the card b. Change the base of the URL at the top c. Add the bearer token in the Headers section.</p> </li> <li> <p>Test the bot in the emulator</p> </li> </ol>"},{"location":"integrations/chat_managers/typebot/#deployment","title":"Deployment","text":"<p>For self-hosting, you can either follow the official docs or follow our quick start below:</p> <p>Step 1: Navigate to <code>chat_managers/typebot/deployment/</code></p> <p>Step 2: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables</p> You must configure at least one login option while setting the environment variables. <p>We recommend either Github or Google authentication. See Typebot's docs for details.</p> <p>Step 3: Run docker compose</p> <pre><code>docker compose -p typebot-stack up -d --build\n</code></pre> <p>You can now access Typebot at <code>https://[DOMAIN]/</code></p> <p>Step 4: Shutdown containers</p> <pre><code>docker compose -p typebot-stack down\n</code></pre>"},{"location":"integrations/chat_managers/glific/glific/","title":"Setup Instructions","text":"<p>Below is a tutorial for how to load our FAQ template flow into Glific and connect it to your own AAQ endpoint.</p> <ol> <li> <p>Go to \"Flows\"</p> <p></p> </li> <li> <p>Click \"Import flow\"</p> <p></p> </li> <li> <p>Select a <code>.json</code> file given under <code>chat_managers/glific/</code> in the AAQ repo</p> <p></p> </li> <li> <p>Open the imported flow</p> <p></p> </li> <li> <p>Open the webhook card</p> <p></p> </li> <li> <p>Replace <code>&lt;INSERT_AAQ_URL&gt;</code> with your the URL to your AAQ instance</p> <p></p> </li> <li> <p>Go to headers and replace <code>&lt;INSERT_AAQ_API_KEY&gt;</code> value to your own AAQ API key.</p> <p> </p> </li> <li> <p>Test the flow in the \"Preview\" emulator</p> <p> </p> </li> </ol>"},{"location":"integrations/chat_managers/turn.io/turn/","title":"Setup Instructions","text":"<p>Below is an example of how to connect a Turn.io Journey to AAQ endpoints.</p> <ol> <li> <p>On your Turn.io page, go to the Journey menu.</p> <p></p> </li> <li> <p>Create New Journey.</p> <p></p> </li> <li> <p>Select \"From Scratch\" -&gt; \"Code\".</p> <p></p> </li> <li> <p>Type in your journey title and click \"Next\".</p> <p></p> </li> <li> <p>Copy and paste the contents of    chat_managers/turn.io/llm_response_flow_code_journey.txt in the AAQ repository into the    Journey's code area.</p> <p></p> </li> <li> <p>Replace <code>&lt;INSERT_AAQ_URL&gt;</code> and <code>&lt;INSERT_AAQ_API_KEY&gt;</code> values to your own AAQ URL and    API key.</p> <p></p> </li> <li> <p>Test the bot in the emulator.</p> <p></p> </li> </ol>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/page/2/","title":"Latest updates","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}