{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"","text":"No-code, easy-to-setup and reliable RAG plugin for chatbots <p> Ask A Question is a free and open-source tool created to help non-profit organizations, governments in developing nations, and social sector organizations utilize Large Language Models for responding to citizen inquiries in their native languages.</p>"},{"location":"#features","title":"Features","text":"<ul> <li> <p> Responsible &amp; Safe AI    Implements guardrailed AI that is ethical, transparent, and secure</p> </li> <li> <p> Local Languages (Coming Soon)    Multilingual model support to enhance accessibility and experience</p> </li> <li> <p> Scalable &amp; Easy to Deploy   Containerized app that can be deployed anywhere with minimal setup</p> </li> <li> <p> Voice (Coming Soon)    Ask questions and receive answers using voice memos</p> </li> </ul> <p> LLM-powered search: Answers questions to database content using LLM embeddings.</p> <p> LLM responses : Craft a custom reponse to users using LLM chat</p> <p> Chat manager integration : Integrate with Turn.io, Glific, Typebot and more</p> <p> Manage content : Use the Admin App to add, edit, and delete content in the database</p> <p> Flag urgent messages : Identify messages that are urgent based on your rules</p> <p> See Full Roadmap  </p> <p>Looking for other features?</p> <p>If you are a developing country government, NGO or a social sector organisation, we'd love to hear what features you'd like to see. Raise an issue with <code>[FEATURE REQUEST]</code> before the title to start the conversation.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p> The project is funded by Google.org through the AI for Global Goals grant.</p> <p> Built and powered by IDinsight.</p> <p>IDinsight uses data and evidence to help leaders combat poverty worldwide. Our collaborations deploy a large analytical toolkit to help clients design better policies, rigorously test what works, and use evidence to implement effectively at scale. We place special emphasis on using the right tool for the right question, and tailor our rigorous methods to the real-world constraints of decision-makers. IDinsight works with governments, foundations, NGOs, multilaterals and businesses across Africa and Asia. We work in all major sectors including health, education, agriculture, governance, digital ID, financial access, and sanitation. We have offices in Dakar, Lusaka, Manila, Nairobi, New Delhi, Rabat, and Remote.  www.idinsight.org </p>"},{"location":"contact_us/","title":"Team","text":""},{"location":"contact_us/#point-of-contact","title":"Point of Contact","text":"<ul> <li><p> Tanmay Verma    Product Manager AAQ Help and Support: aaq@idinsight.org tanmay.verma@idinsight.org </p></li> </ul>"},{"location":"contact_us/#full-team","title":"Full Team","text":"<ul> <li> <p><p> Sid Ravinutala      Director, Data Science sid.ravinutala@idinsight.org </p></p> </li> <li> <p><p> Suzin You    Tech Lead, Data Scientist suzin.you@idinsight.org </p></p> </li> <li> <p><p> Carlos Samey     Data Scientist carlos.samey@idinsight.org </p></p> </li> <li> <p><p> Amir Emami     Data Scientist amir.emami@idinsight.org </p></p> </li> </ul> <p>We are part of IDinsight's DSEM team</p> <p>Read more about DSEM and our work here.</p>"},{"location":"roadmap/","title":"Roadmap","text":"Quarter Feature Status Description Q4 2023 FastAPI Refactor Refactored to an all-components-in-one-repo codebase Embeddings-based search Match user questions to content in the database using embeddings Q1 2024 RAG responses Craft a custom response to the question using LLM based on retrieved content in the database Guardrails Keep LLM responses friendly and strictly context-based Q2 2024 Message Triaging Tag user messages with intents &amp; flag urgency Multi-user with Google log-in You can now have 1 AAQ deployment with multiple users, each with their own content DB Support for Turn.io, Glific integration Add AAQ to popular chat flow builders in social sector, like Turn.io and Glific Content Tags Add tags to your content for easy browsing (and more to come!) Q3 2024 Analytics for Feedback and Content See content use, questions that receive poor feedback, missing content, and more Voice notes support Automatic Speech Recognition for audio message to content matching Multi-turn chat Refine or clarify user question through conversation. Engineering Dashboard Monitor uptime, response rates, throughput HTTP response codes Q4 2024 Personalization and contextualization Use contextual information to improve responses Multimedia content Respond with not just text but images and audio as well. A/B Testing Test and decide content that works better for users <p>Key  : Completed  : Under development  : Queued : Yet to be scoped</p> <p>Beyond 2024</p> <ul> <li>Multi-tenant architecture</li> <li>User configurable Guardrails (in Admin app)</li> <li>Feedback-based model/LLM fine tuning</li> <li>Safety tests, fairness reports and ethical reviews</li> </ul>"},{"location":"usage/","title":"Usage","text":"<p>There are two ways to interact with the service:</p> <ol> <li>Accessing the API endpoints</li> <li>The Admin App</li> </ol>"},{"location":"usage/#api-endpoints","title":"API endpoints","text":"<p>To get answers from your database of contents, you can use the <code>/search</code> endpoint. This endpoint returns the following:</p> <ul> <li>Search results: Finds the most similar content in the database using cosine distance between embeddings.</li> <li>(Optionally) LLM generated response: Crafts a custom response using LLM chat using the most similar content.</li> </ul> <p>You can also add your contents programatically using API endpoints. See docs or SwaggerUI at <code>https://&lt;DOMAIN&gt;/api/docs</code> or <code>https://&lt;DOMAIN&gt;/docs</code> for more details and other API endpoints.</p>"},{"location":"usage/#embeddings-search","title":"Embeddings search","text":"<pre><code>curl -X 'POST' \\\n  'https://[DOMAIN]/api/search' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;BEARER TOKEN&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query_text\": \"how are you?\",\n  \"generate_llm_response\": false,\n  \"query_metadata\": {}\n}'\n</code></pre>"},{"location":"usage/#llm-response","title":"LLM response","text":"<pre><code>curl -X 'POST' \\\n  'https://[DOMAIN]/api/search' \\\n  -H 'accept: application/json' \\\n  -H 'Authorization: Bearer &lt;BEARER TOKEN&gt;' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"query_text\": \"this is my question\",\n  \"generate_llm_response\": true,\n  \"query_metadata\": {}\n}'\n</code></pre>"},{"location":"usage/#admin-app","title":"Admin app","text":"<p>You can access the admin console at</p> <pre><code>https://[DOMAIN]/\n</code></pre> <p>On the Admin app, you can:</p> <ul> <li> Manage content and urgency rules</li> <li> Use the test sidebars to test the Question-Answering and Urgency Detection services</li> <li> View dashboards</li> </ul>"},{"location":"blog/","title":"Latest updates","text":""},{"location":"blog/2024/01/12/no-more-hallucinations/","title":"No more hallucinations","text":"<p>Last week we rolled out another safety feature - checking consistency of the response from the LLM with the content it is meant to be using to generate it. This shoud catch hallucinations or when LLM uses it's pre-training to answer a question. But it also catches any prompt injection or jailbreaking - if it somehow got through our other checks.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#checking-alignment-of-llm-response","title":"Checking alignment of LLM Response","text":"<p>Despite clear prompting, LLMs hallucinate. And sometimes use their large training set to answer a question instead of solely using the context provided.</p> <p>One of the endpoints that AAQ presents is Search. The process diagram on the page will be kept up to date on how the service works but here is what it looks like at the time of writing:</p> <p></p> <p>Steps 12 and its response, 13 check if the statements being generated by the LLM are consitent with the content it is meant to use to generate it.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#using-alignscore","title":"Using AlignScore","text":"<p>We can use GPT4-turbo and it does remarkably well on our test data. But there may be reasons - from data governance and privacy rules to costs - for not sending the data to OpenAI. One option is to use your own locally hosted LLM.</p>"},{"location":"blog/2024/01/12/no-more-hallucinations/#doc-references","title":"Doc references","text":"<ul> <li>Locally hosted LLMs</li> <li>Search</li> </ul>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/","title":"Did someone say Dashboard?","text":"<p>Getting insights from your app has been made easy with the introduction of a questions dashboard page.</p>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/#content-dashboard","title":"Content dashboard","text":"<p>On the Admin App, you can now get statistics from your question answering service with the new dashboard displaying information such as:</p> <ul> <li>The number of questions answered during the current month</li> <li>The number of questions with positive feedback</li> <li>The number of questions asked during the past 6 months.</li> </ul> <p>The dashboard is in its first version and we are planning on adding more information such as Urgency Detection related statistics, and content related statistics. </p>"},{"location":"blog/2024/05/08/did-someone-say-dashboard/#doc-references","title":"Doc references","text":"<ul> <li>QA Service</li> </ul>"},{"location":"blog/2024/06/17/new-feature-detected-tags/","title":"New feature detected: Tags","text":"<p>Filtering contents has been made easier with tags.</p>"},{"location":"blog/2024/06/17/new-feature-detected-tags/#manage-tags","title":"Manage tags","text":"<p>On the Admin App, it is now possible to add tags to contents to categorise them.</p> <p>You can create a tag, add it the content, or delete it directly within the edit content page. The \"Tags\" bar shown on the image below will allow you to quickly manage tags.</p> <p></p> <p>We also added the possibility of filtering contents using tags for a seemless content management experience. On the landing page, you can use the Tags bar to filter contents based on tags. </p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/","title":"Revamping the dashboard","text":"<p>In the last month or so, we've had support from Google fellows to pinch hit on a few items. Ahn Mac, a UI/UX designer at Google, helped design a new dashboard that we are very excited about. The first page, \"Overview\", is now ready for you to check out.</p> <p>Here's a screenshot:</p> <p></p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/#why-did-we-build-this","title":"Why did we build this?","text":"<p>The ethos of AAQ is that you care deeply about your users and are continuously refining and adding content to improve user experience. A live dashboard is the first step. It allows you to understand the usage of your solution, your users, and their feedback.</p> <p>Also, it wouldn't be an IDinsight product without a decent dashboard!</p>"},{"location":"blog/2024/07/10/revamping-the-dashboard/#doc-references","title":"Doc references","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"blog/2024/08/14/a-new-ai-powered-dashboard-page/","title":"A new AI-powered dashboard page","text":"<p>Last month we rolled out the \"Overview\" page of the dashboard. Today, we are excited to share the \"Performance\" page. This includes a detailed view of the performance of your content, and AI-powered suggestions on how to improve it.</p> <p>Here's it is in action</p> <p></p>"},{"location":"blog/2024/08/14/a-new-ai-powered-dashboard-page/#why-did-we-build-this","title":"Why did we build this?","text":"<p>In a world with too many \"set-it-and-forget-it\" RAG solutions, we decided to build a solution that is designed to be continuously improved. We believe that a quality service requires deep engagement with the users and their feedback.</p> <p>The \"Performance\" page allows for just that. It lets you see which contents are being shared, and the feedback each is receiving. Using AI, we are able to summarize user feedback and suggest ways to improve the content.</p> <p>As before, a special thanks to Anh Mac from Google for the UI/UX design.</p>"},{"location":"blog/2024/08/14/a-new-ai-powered-dashboard-page/#doc-references","title":"Doc references","text":"<ul> <li>Dashboard</li> </ul>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/","title":"Revamped feedback endpoints","text":"<p>There are now two new endpoints for feedback:</p> <ol> <li><code>POST /response-feedback</code> - Allows you to capture feedback for the overall response returned by either of the Question-Answering APIs.</li> <li><code>POST /content-feedback</code> - Allows you to capture feedback for a specific piece of content.</li> </ol> <p>These can be used in chat managers to collect feedback after answers are shown.</p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#sentiment-and-text","title":"Sentiment and Text","text":"<p>For both of these endpoints, you are able to provide either sentiment (positive, negative) or text feedback, or both.</p> <p>See your deployment's OpenAPI documentation at <code>https://[DOMAIN]/api/docs</code> for more details.</p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#content-cards-show-feedback","title":"Content cards show feedback","text":"<p>The positive and negative feedback captured for the content can also be seen in the \"Read\" modal for each content card in the Admin App.</p> <p></p>"},{"location":"blog/2024/04/20/revamped-feedback-endpoints/#doc-references","title":"Doc references","text":"<ul> <li>Response Feedback</li> <li>Content Feedback</li> </ul>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/","title":"Tracing your AAQ calls with Langfuse","text":"<p>By integrating Langfuse, a popular LLM observability tool with a generous free tier, you can now track all LLM calls made via AAQ.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#whats-in-a-trace","title":"What's in a Trace?","text":"<p>With Langfuse enabled, AAQ is set up to trace each query to the <code>POST /search</code> endpoint.</p> <p></p> <p>Each query is represented as a Trace. If you click on the Trace ID, you can view the details of the trace. Here is an example for a <code>/search</code> call with <code>generate_llm_response</code> set to <code>true</code>:</p> <p></p> <p>On the right, there are Generations associated with this trace. In AAQ, each generation is each call to the LiteLLM Proxy Server. You can view the series of input checks, RAG (\"get_similar_content_async\" and \"openai/generate-response\") and one output check we perform (you can learn more about our Guardrails here). The generation names come from the model names used in your LiteLLM Proxy Server Config.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#why-does-aaq-need-observability","title":"Why does AAQ need observability?","text":"<p>As we begin piloting AAQ in various use cases, we wanted to be able to track LLM calls so that we can debug, analyze, and improve AAQ's question-answering ability. We are using it to test different prompt templates and guardrails. If you are interested in getting your hands dirty with AAQ's codebase, we imagine this will be useful to you. (Langfuse has a generous free tier and is self-hostable!)</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#so-how-do-i-use-it","title":"So how do I use it?","text":"<p>Sign up to Langfuse, and set the following environment variables in the backend app to get started.</p> <pre><code>export LANGFUSE=True\nexport LANGFUSE_PUBLIC_KEY=pk-...\nexport LANGFUSE_SECRET_KEY=sk-...\nexport LANGFUSE_HOST=https://cloud.langfuse.com # optional based on your Langfuse host\n</code></pre> <p>See more in Config options - Tracing with Langfuse.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#whats-next","title":"What's next?","text":"<p>We want to explore the rich set of features that Langfuse offers, such as evaluation and scoring. One concrete next step is to trace AAQ's Feedback endpoint using Langfuse's Scores.</p>"},{"location":"blog/2024/06/06/tracing-your-aaq-calls-with-langfuse/#docs-references","title":"Docs references","text":"<ul> <li>Config options - Tracing with Langfuse</li> <li>Dev setup</li> <li>LiteLLM Proxy Server</li> </ul>"},{"location":"blog/2024/01/12/improved-docs/","title":"Improved docs!","text":"<p>First, we have added this section that you are currently reading. Each week we'll post what we've rolled out - new features, bug fixes, and performance improvements.</p> <p>The rest of the docs have now also been restructured to make it easy to parse.</p>"},{"location":"blog/2024/01/12/improved-docs/#now-with-cards","title":"Now with cards!","text":"<p>A lot of the index pages now show cards like the one shown below. These should make it easy to grasp the content in section in a glance.</p> <p></p>"},{"location":"blog/2024/01/12/improved-docs/#process-flow-diagrams","title":"Process Flow Diagrams","text":"<p>Search page now shows process flow diagrams. It should make it a lot easier to understand what is happening under the hood when you call either of these endopints.</p> <p></p>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/","title":"Adding a model proxy server","text":"<p>Instead of being handled directly in our code, our model calls are now routed through a LiteLLM Proxy server. This lets us change models on the fly and have retries, fallbacks, budget tracking, and more.</p>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#why-change","title":"Why change?","text":"<p>LiteLLM Proxy introduces a streamlined approach for handling various large language models (LLMs) through a single interface. We can now manage model endpoints and configurations via a proxy server, replacing the previous method of hard-coding them into the environment of our app.</p> <p>The benefits of this setup:</p> <ul> <li>Simplifies codebase by centralizing the model configurations</li> <li>Provides the flexibility to switch or update models without altering the core application logic - we can even add and remove models through the proxy's API!</li> <li>Multiple instances of AAQ can use the same model server</li> </ul> <p>We now configure models in a <code>config.yaml</code> file, allowing the proxy to route requests to different LLMs (commercial or self-hosted - full list here). See <code>deployment/docker-compose/litellm_config.yaml</code> for an example.</p> <p>The LiteLLM Proxy server also has some extra useful features:</p> <ul> <li>Consistent input/output formats across different models</li> <li>Fallback mechanisms for error handling</li> <li>Detailed logging and connectivity to Langfuse and others</li> <li>Tracking of token usage and spending.</li> <li>Asynchronous handling of requests and caching.</li> </ul>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#any-downsides","title":"Any downsides?","text":"<p>Potential downsides include:</p> <ul> <li>Dependency on the LiteLLM project for updates for new models and parameters</li> <li>A possible increase in latency</li> <li>A new Docker container in our stack which, despite its name, it not \"lite\"!</li> </ul>"},{"location":"blog/2024/04/05/adding-a-model-proxy-server/#docs-references","title":"Docs references","text":"<ul> <li>LLM Proxy Server</li> <li>Dev setup</li> </ul>"},{"location":"blog/2024/03/22/hello-material-ui/","title":"Hello Material UI","text":"<p>We've switched to MaterialUI: Cleaner, easier to build and maintain, more familiar.</p>"},{"location":"blog/2024/03/22/hello-material-ui/#why-change","title":"Why change?","text":"<p>Design best practices are \"best practices\" for having a proven track record of creating engaging, intuitive, and effective user experiences. Google has encapsulated this in the creation of MaterialUI \u2014 a design language well-infused with the principles of good design. Embracing these principles, we're thrilled to announce a significant shift in our project's Admin App: the transition to using MaterialUI components.</p> <p>This shift is a step towards cleaner maintainable code and a commitment to providing simpler and more familiar user experiences.</p> Old UI MUI Main page Edit page <p>We're already starting to see some benefits of moving to Material UI:</p> <ul> <li> <p>Cleaner Codebase:   MaterialUI components are modular and customizable, and has allowed us to achieve a polished look with less custom CSS.</p> </li> <li> <p>Easier to Build and Maintain:   MaterialUI's extensive component library has significantly reduced our development time. Components like buttons, nav bars, and switches come with a variety of options that are easily customizable.</p> </li> <li> <p>Familiarity and Consistency:   MaterialUI is a design language familiar to millions of users worldwide, thanks to Google's widespread implementation across its products. This is helping us lower the learning curve for new users and is ensuring consistency across devices.</p> </li> </ul>"},{"location":"blog/2024/02/09/nginx-out-caddy-in/","title":"Nginx out, Caddy in","text":"<p>By swapping out Nginx for Caddy, we substantially simplified the deployment steps and the architecture - which means fewer docker containers to run and manage.</p> <p>Previously, we were using NGINX and then manually running a script to issue certificates from Let's Encrypt. We were also running a container to refresh Let\u2019s Encrypt certificates. And then sharing volumes between this container and the nginx container. This article from 2018 shows you the setup. It was a bit of a mess.</p> <p>The other issue was that this process wouldn't work when running locally - for example when you are developing. Your domain would be <code>localhost</code> and Let's Encrypt can't issue certificates for it. So we had to come up with a different process for local dev where we were issuing self-signed certs.</p>"},{"location":"blog/2024/02/09/nginx-out-caddy-in/#welcome-caddy","title":"Welcome Caddy","text":"<p>Oscar, our Google.org AI advisor's first advice when he saw our architecture was to switch to Caddy. Here are the benefits:</p> <ol> <li>It requests and refresh certs from Let's Encrypt.</li> <li>If your domain is localhost, it knows to issue its own certificate.</li> <li>A much smaller and simpler config file.</li> <li>You can use environment variables.</li> </ol> <p>So now our local setup process is the same as prod and requires one fewer containers. Amazing!</p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/","title":"Check out the new Playground","text":"<p>Admin app now has a new Playground page where you can test out the FAQ matching and LLM response endpoints!</p> <p>Here's a screenshot:</p> <p></p>"},{"location":"blog/2024/04/16/check-out-the-new-playground/#why-did-we-build-this","title":"Why did we build this?","text":"<p>Content managers can now test out how the retrieval APIs will perform when new content is added - without leaving the Admin App.</p> <p>By clicking on <code>&lt;json&gt;</code> at the bottom, they can also see the raw JSON response sent back by the server. This include debugging information that may be useful in understanding behaviour.</p> <p></p>"},{"location":"blog/2024/03/19/ditching-qdrant-for-pgvector/","title":"Ditching Qdrant for PgVector","text":"<p>In our latest infrastructure update, we decided to transition from Qdrant to pgvector for managing our vector databases. This move is part of our ongoing effort to reduce cost and simplify AAQ\u2019s architecture.</p> <p>This means: - we no longer require a separate Qdrant server. With the pgvector extension, vector data can now be stored in PostgreSQL along with other transactional data. - Seamless transactions between vector data and transactional data will allow for the integration of new features such as multilingual support.</p>"},{"location":"blog/2024/03/19/ditching-qdrant-for-pgvector/#why-this-change","title":"Why this change?","text":"<p>While Qdrant served us well as a dedicated vector database, integrating it with our existing PostgreSQL setup introduced complexity and maintenance overhead. Operating Qdrant alongside PostgreSQL meant managing two distinct database systems with their own infrastructure, architecture, and requirements. Integrating both databases into our codebase required additional integration layers, complicating our codebase.</p> <p>As we already were using Postgresql, pgvector caught our attention with this article as a promising solution that integrates vector database capabilities directly into our existing database. Here's why we decided to go for pgvector: - Simplified Architecture: By adopting pgvector, we significantly reduced the complexity of our data infrastructure. Vector and relational data now reside within the same database, eliminating the need for separate systems. - Improved Response Time: Direct integration with PostgreSQL enhances performance by eliminating the overhead of communicating between separate databases.</p> <p>Moving to pgvector not only benefits our team in terms of reduced complexity and better resource utilization but also lays the groundwork for future innovations such as the multilingual support which should be coming soon.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/","title":"One Endpoint to Rule Them All","text":"<p>We refactored our two question-answering endpoints into a single one called <code>/search</code> for clarity and ease of use.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#why-change","title":"Why change?","text":"<p>We realised our two <code>/embeddings-search</code> and <code>/llm-response</code> endpoints were a bit confusing, and since they performed very similar tasks we combined them into one for ease of use.</p>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#whats-new","title":"What's new?","text":"<p>We now have a single endpoint called <code>/search</code> which always returns the top contents in the database that most closely matched your query and can optionally return an LLM-generated resposne to your question.</p> <p></p> <p>We've also simplified the parameters in our response model so it's easier to use. We will now respond with something like this:</p> <pre><code>{\n    \"query_id\": 1,\n    \"llm_response\": \"Example LLM response \"\n    \"(null if generate_llm_response is False)\",\n    \"search_results\": {\n        \"0\": {\n            \"title\": \"Example content title\",\n            \"text\": \"Example content text\",\n            \"id\": 23,\n            \"distance\": 0.1,\n        },\n        \"1\": {\n            \"title\": \"Another example content title\",\n            \"text\": \"Another example content text\",\n            \"id\": 12,\n            \"distance\": 0.2,\n        },\n    },\n    \"feedback_secret_key\": \"secret-key-12345-abcde\",\n    \"debug_info\": {\"example\": \"debug-info\"},\n}\n</code></pre>"},{"location":"blog/2024/07/22/one-endpoint-to-rule-them-all/#docs-references","title":"Docs references","text":"<ul> <li>Search</li> </ul>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/","title":"Turn.io Integration Now Available: Your Turn to Play with it","text":"<p>As a question-answering plugin service, Ask A Question requires what we call \"chat managers\" - platforms that help you build end-user chat flows and integrate with channels like WhatsApp.</p> <p>To make this integration process easier, we published a Turn.io Playbook. Playbooks are reusable, pre-built chat flows that can be shared with and remixed by other teams in the chat-for-impact community.</p> <p>Find it here. You may need to sign up to see it.</p>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/#how-do-i-use-it","title":"How do I use it?","text":"<p>Just click on \"Activate Journey\" and you will see this flow imported in your Journeys.</p> <p>Next, replace the <code>&lt;API_KEY&gt;</code> in the code card with your own API key and you should be good to go. If you are using your own deployment, you will also need to change the URL of the API call.</p>"},{"location":"blog/2024/07/29/turnio-integration-now-available-your-turn-to-play-with-it/#stuck","title":"Stuck?","text":"<p>Learn all about Playbooks on Turn's official blog.</p> <p>And don't hesitate to write to us at aaq@idinsight.org!</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/","title":"Introducing Urgency Detection","text":"<p>You may wish to handle urgent messages differently. For example, when deploying a question answering service in a health context, you may wish to refer the user to their nearest health center, or escalate it immediately to a human operator.</p> <p>We introduce a new endpoint and new page in the Admin App to enable this.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#defining-urgency-rules","title":"Defining urgency rules","text":"<p>Using the Admin App, you can define your rules in natural language. For example, here are a few rules borrowed directly from the CDC website on Urgent Maternal Warning Signs:</p> <p></p> <p>It's as simple as that. You don't need to train a model (though you can if you want to. See \"Or write your own\" below).</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#using-the-urgency-detection-endpoint","title":"Using the urgency detection endpoint","text":"<p>You should refer to your Swagger UI/OpenAPI documentation for details but here is a screenshot for us lazy ones:</p> <p></p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#pick-your-method","title":"Pick your method","text":"<p>As of this blog post, there are two ways to determine urgency:</p> <ol> <li>Cosine Distance - It uses the cosine distance between the    input and the urgency rules to determine urgency. It's simple and fast, but may not be    as accurate as the next method.</li> <li>LLM Entailment - This calls an LLM to determine if the message matches the    urgency rules. It's more accurate, but slower. Also, since it's making a call to the LLM, it    is more expensive than the cosine distance method.</li> </ol> <p>See setup sections on how to configure these.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#or-write-your-own","title":"Or write your own","text":"<p>May be you are not happy with either of these and want to try out that new entailment model. All you need to do is define your function with this signature:</p> <pre><code>@urgency_classifier\nasync def your_fancy_method(\n    asession: AsyncSession,\n    urgency_query: UrgencyQuery,\n) -&gt; UrgencyResponse:\n</code></pre> <p>and you can update the <code>URGENCY_CLASSIFIER</code> environment variable to <code>your_fancy_method</code>.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#using-a-different-model","title":"Using a different model","text":"<p>Reminder that we setup a proxy server to make it easy to switch between models. If you want to use a different model, you can host it and update the <code>LITELLM_MODEL_URGENCY_DETECT</code> environment variable to point to your model.</p>"},{"location":"blog/2024/05/02/introducing-urgency-detection/#doc-references","title":"Doc references","text":"<ul> <li>LLM Proxy Server</li> <li>Urgency Detection</li> <li>Managing Urgency Rules</li> </ul>"},{"location":"components/","title":"Components","text":"<p>In this section you can find the different components within AAQ.</p>"},{"location":"components/#user-facing-components","title":"User-facing Components","text":"<p>There are 3 main components in Ask-A-Question.</p> <ul> <li> <p> The Admin App</p> <p>Manage content in the database. Manage urgency detection rules. Test the service's performance. Explore usage dashboards.</p> <p> More info</p> </li> <li> <p> The Question-Answering Service</p> <p>Integrate with these API endpoints to answer questions from your users.</p> <p> More info</p> </li> <li> <p> The Urgency Detection Service</p> <p>Integrate with this API endpoint to tag messages as urgent or not.</p> <p> More info</p> </li> </ul>"},{"location":"components/#internal-components","title":"Internal Components","text":"<ul> <li> <p> Model Proxy Server</p> <p>AAQ uses the LiteLLM Proxy Server for managing LLM and embedding calls, allowing you to use any LiteLLM supported model (including self-hosted ones).</p> <p> More info</p> </li> <li> <p> Self-hosted Hugging Face Embeddings Model</p> <p>(Optional) A dockerised Hugging Face text embeddings model to create vectors   and perform vector search.</p> <p> More info</p> </li> </ul>"},{"location":"components/admin-app/","title":"The Admin App","text":"<ul> <li> <p> Question Answering Contents</p> <p>Allows you to view, create, edit, delete, or test content.</p> <p> More info</p> </li> <li> <p> Urgency Detection Rules</p> <p>Allows you to view, create, edit, delete, or test urgency rules.</p> <p> More info</p> </li> <li> <p> Dashboard</p> <p>Allows you to see statistics like which content is most frequently being used, or the feedback from users on responses provided by the service.</p> <p> More info</p> </li> </ul>"},{"location":"components/admin-app/#accessing-the-admin-app","title":"Accessing the Admin app","text":"<p>If you have the application running, you can access the admin app at:</p> <pre><code>https://[DOMAIN]/\n</code></pre> <p>or if you are using the dev setup:</p> <pre><code>http://localhost:3000/\n</code></pre>"},{"location":"components/admin-app/dashboard/","title":"Dashboard","text":"<p>The Dashboard provides real-time analytics on the performance of your solution. There are four time filters available: Last 24 hours, Last week, Last month, and Last year.</p> <p>The dashboard is divided into three sections: Overview, Performance, and Content Gaps.</p>"},{"location":"components/admin-app/dashboard/#overview","title":"Overview","text":"<p>This landing page of the dashboard provides a high-level summary of the performance of your solution.</p> <p></p>"},{"location":"components/admin-app/dashboard/#performance","title":"Performance","text":"<p>This section show how well your content is performing. You can sort the content by the number of times it was shared with your users, upvotes or downvotes, and even the trend (is it getting more or less popular).</p> <p></p> <p>Clicking on the any of the content in the table opens a detailed view of the content's performance. This also provides an AI generated summary of the feedback users have provided and suggestion on how to improve the content.</p> <p></p>"},{"location":"components/admin-app/dashboard/#content-gaps","title":"Content Gaps","text":"<p> Stay tuned for the \"Content Gaps\" section.</p>"},{"location":"components/admin-app/dashboard/#want-to-see-more","title":"Want to see more?","text":"<p>Is there a metric or a feature you'd like to see on the dashboard? We'd love to hear from you. Contact us!</p>"},{"location":"components/admin-app/question-answering/","title":"Managing Content used for Question Answering","text":"<p>The Admin app allows you to view, add, edit, or delete content in the database. It also allows you to test the service with questions to ensure the correct responses are fetched before releasing the service to your users.</p> <p>Once logged in, you should see the following screen:</p> <p></p> <p>When you click the \"Test\" button, you can use the sidebar to try your questions.</p> <p></p>"},{"location":"components/admin-app/question-answering/#upcoming-features","title":"Upcoming features","text":"<ul> <li> Add multiple languages for each content</li> <li> Allow metadata to be captured for each content</li> </ul>"},{"location":"components/admin-app/urgency-rules/","title":"Managing Urgency Rules","text":"<p>The Admin app allows you to view, add, edit, or delete  in the database. It also allows you to test the service with messages to ensure the classifications seem correct.</p> <p>Once logged in, navigate to \"Urgency Detection\" from the menu and you should see the following screen</p> <p></p> <p>You can add, edit, delete, or test urgency rules from this screen.</p> <p>When you click the \"Test\" button, you can use the sidebar to try your messages.</p> <p></p>"},{"location":"components/huggingface-embeddings/","title":"Hugging Face Embeddings","text":"<p>Get more control of your data by using open-source embeddings from Hugging Face</p> <p>By default, AAQ uses OpenAI text embeddings to generate text vectors. But we give the option of deploying and hosting custom Hugging Face embedding models.</p> <p>Note</p> <p>Hugging Face has an extensive list of embeddings models, some lightweight and some heavy. Please carefully choose the model that will fit your use case best.</p> <ul> <li> <p> How to use</p> <p>Deploying the custom Hugging Face embeddings model and configuring AAQ to use Hugging Face embeddings.</p> <p> More info</p> </li> </ul>"},{"location":"components/huggingface-embeddings/how-to-use/","title":"How to use Hugging Face embeddings","text":"<p>To host Hugging Face embeddings we use text-embeddings-inference image by Hugging Face.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#prerequisite-steps","title":"Prerequisite steps","text":""},{"location":"components/huggingface-embeddings/how-to-use/#step-0-update-your-litellm-proxy-config","title":"Step 0. Update your LiteLLM Proxy config","text":"<p>To use Hugging Face embeddings instead of OpenAI embeddings, you can replace OpenAI embeddings in <code>litellm_proxy_config.yaml</code>.</p> <p>This can be done by uncommenting the second embeddings model:</p> <pre><code># - model_name: embeddings\n#   litellm_params:\n#     model: huggingface/huggingface-embeddings # model name not important\n#     api_key: \"os.environ/HUGGINGFACE_EMBEDDINGS_API_KEY\" #pragma: allowlist secret\n#     api_base: \"os.environ/HUGGINGFACE_EMBEDDINGS_ENDPOINT\"\n</code></pre> <p>The first embeddings model should be commented out unless using Hugging Face embeddings as a back up to OpenAI embeddings.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#step-1-set-pgvector_vector_size-environment-variable","title":"Step 1. Set <code>PGVECTOR_VECTOR_SIZE</code> environment variable","text":"<p>Make sure that <code>PGVECTOR_VECTOR_SIZE</code> is set to be the vector size generated by your Hugging Face embedding of choice. This should be set in <code>.core_backend.env</code> (cf. Configuring AAQ).</p> <p>Note that if the database is already set up using a different <code>PGVECTOR_VECTOR_SIZE</code> value, this will not work unless the database is destroyed and created again.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#deploying-hugging-face-embeddings","title":"Deploying Hugging Face Embeddings","text":"<p>Make sure you've performed the prerequisite steps before proceeding.</p> <p>To deploy Hugging Face embeddings, follow the deployment instructions in Quick Setup with the following additional steps</p> <p>On Step 4: Configure LiteLLM Proxy server, edit <code>.litellm_proxy.env</code> by setting the following variables:</p> <ul> <li><code>HUGGINGFACE_MODEL</code>: your Hugging Face model of choice.</li> <li><code>HUGGINGFACE_EMBEDDINGS_API_KEY</code>: API key for Hugging Face Embeddings API</li> <li><code>HUGGINGFACE_EMBEDDINGS_ENDPOINT</code>: API endpoint URL for Hugging Face Embeddings     container. The default value should work with docker compose.</li> </ul> If using an arm64 device, a docker image should be built locally before deployment. <p>This can be done by running the make command: <code>make build-embeddings-arm</code>. Also, the variable <code>EMBEDDINGS_IMAGE_NAME</code> should be uncommented in <code>.core_backend.env</code>.</p> <p>On Step 6: Run docker-compose, add <code>--profile huggingface-embeddings</code> to the docker compose command:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    --profile huggingface-embeddings -p aaq-stack up -d --build\n</code></pre>"},{"location":"components/huggingface-embeddings/how-to-use/#setting-up-hugging-face-embeddings-for-development","title":"Setting up Hugging Face embeddings for development","text":"<p>Make sure you've performed the prerequisite steps before proceeding.</p> <p>To set up your development environment with Hugging Face embeddings, you can start the container manually by navigating to <code>ask-a-question</code> repository root and executing the following make command:</p> <pre><code>make setup-embeddings\n</code></pre> <p>If you are using an arm device, you can first build the image using:</p> <pre><code>make build-embeddings-arm\n</code></pre> <p>then:</p> <pre><code>make setup-embeddings-arm\n</code></pre> <p>Before running the commands above, you must export environment variables <code>HUGGINGFACE_MODEL</code> and <code>HUGGINGFACE_EMBEDDINGS_API_KEY</code>.</p> <p>The embeddings API endpoint by default is at: http://localhost:5000.</p>"},{"location":"components/huggingface-embeddings/how-to-use/#also-see","title":"Also see","text":"<ol> <li>Quick Setup</li> <li>Configuring AAQ</li> </ol>"},{"location":"components/litellm-proxy/","title":"LLM Proxy Server","text":""},{"location":"components/litellm-proxy/#what-is-it","title":"What is it?","text":"<p>AAQ uses the LiteLLM Proxy Server for managing LLM calls, allowing you to use any LiteLLM supported model including self-hosted ones.</p> <p>This proxy server runs as a separate Docker container with configs read from a <code>config.yaml</code> file, where you can set the appropriate model names and endpoints for each LLM task.</p>"},{"location":"components/litellm-proxy/#example-config","title":"Example config","text":"<p>You can see an example <code>litellm_proxy_config.yaml</code> file below. In our backend code, we refer to the models by their custom task <code>model_name</code> (e.g. \"generate-response\"), but which actual LLM model each call is routed to is set here.</p> <pre><code>model_list:\n  - model_name: embeddings\n    litellm_params:\n      model: text-embedding-ada-002\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: default\n    litellm_params:\n      model: gpt-4-0125-preview\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: generate-response\n    litellm_params:\n      model: gpt-4-0125-preview\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: detect-language\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: translate\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: paraphrase\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: safety\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\n  - model_name: alignscore\n    litellm_params:\n      model: gpt-3.5-turbo-1106\n      api_key: \"os.environ/OPENAI_API_KEY\"\nlitellm_settings:\n  num_retries: 3\n  request_timeout: 100\n  telemetry: False\n</code></pre> <p>See the Contributing Setup and Docker Compose Setup for how this service is run in our stack.</p>"},{"location":"components/litellm-proxy/#also-see","title":"Also see","text":"<ul> <li>Latest Updates: Adding a model proxy server</li> </ul>"},{"location":"components/qa-service/","title":"Question-Answering Service","text":"<p><code>/search</code> is the flagship endpoint that your application can integrate with:</p> <ul> <li> <p> Search</p> <p>Allows searching through the content database using embeddings similarity and optionally creates an LLM-generated answer to the user's question.</p> <p> More info</p> </li> </ul>"},{"location":"components/qa-service/#capturing-feedback","title":"Capturing Feedback","text":"<p>The service also provides two endpoint to capture feedback from users. One to capture feedback for the response returned and the other for the content items retrieved.</p> <p>For both of these, you can provide feedback as sentiment (\"positive\", \"negative\"), as text, or as both.</p> <ul> <li> <p> Response Feedback</p> <p>Allows users to provide feedback on the response generated by the either Semantic Search or LLM Response.</p> <p> More info</p> </li> <li> <p> Content Feedback</p> <p>Allows users to provide feedback on the content items returned by the Semantic Search.</p> <p> More info</p> </li> </ul>"},{"location":"components/qa-service/#swaggerui","title":"SwaggerUI","text":"<p>If you have the application running, you can access the SwaggerUI at</p> <pre><code>https://[DOMAIN]/api/docs\n</code></pre> <p>or if you are using the dev setup:</p> <pre><code>http://localhost:8000/docs\n</code></pre>"},{"location":"components/qa-service/#upcoming","title":"Upcoming","text":"<ul> <li> Chat capability</li> </ul>"},{"location":"components/qa-service/content-feedback/","title":"Content Feedback","text":"<p>This service allows you to provide feedback for individual content items in the database. The feedback can be used to improve the quality of the content and the search results.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/response-feedback/","title":"Response Feedback","text":"<p>This service captures feedback for the response return by Semantic Search or LLM Response.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/search/","title":"Search","text":"<p>This service returns the contents from the database with the most similar vector embeddings to the question and optionally also uses an LLM to construct a custom answer to the user's question using the retrieved contents.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/qa-service/search/#process-flow-without-llm-response-generation","title":"Process flow without LLM response generation","text":"<pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Translate text\n  LLM-&gt;&gt;AAQ: &lt;Translated text&gt;\n  AAQ-&gt;&gt;LLM: Paraphrase question\n  LLM-&gt;&gt;AAQ: &lt;Paraphrased question&gt;\n  AAQ-&gt;&gt;Vector DB: Request M most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;M contents with similarity score&gt;\n  AAQ-&gt;&gt;Cross-encoder: Re-rank to get top N contents\n  Cross-encoder-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;User: Return JSON of N contents\n</code></pre>"},{"location":"components/qa-service/search/#process-flow-with-llm-response-generation","title":"Process flow with LLM response generation","text":"<pre><code>sequenceDiagram\n  autonumber\n  User-&gt;&gt;AAQ: User's question\n  AAQ-&gt;&gt;LLM: Identify language\n  LLM-&gt;&gt;AAQ: &lt;Language&gt;\n  AAQ-&gt;&gt;LLM: Check for safety\n  LLM-&gt;&gt;AAQ: &lt;Safety Classification&gt;\n  AAQ-&gt;&gt;Vector DB: Request N most similar contents in DB\n  Vector DB-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;Cross-encoder: Re-rank to get top N contents\n  Cross-encoder-&gt;&gt;AAQ: &lt;N contents with similarity score&gt;\n  AAQ-&gt;&gt;LLM: Given contents, construct response in user's language to question\n  LLM-&gt;&gt;AAQ: &lt;LLM response&gt;\n  AAQ-&gt;&gt;LLM: Check if LLM response is consistent with contents\n  LLM-&gt;&gt;AAQ: &lt;Consistency score&gt;\n  AAQ-&gt;&gt;User: Return JSON of LLM response and N contents\n</code></pre>"},{"location":"components/urgency-detection/","title":"Urgency Detection","text":"<p>This service returns if the the message is urgent or not. There are currently two methods available to do this.</p>"},{"location":"components/urgency-detection/#method-1-cosine-distance","title":"Method 1: Cosine distance","text":"<ul> <li>Cost: </li> <li>Accuracy: </li> <li>Latency: </li> </ul> <p>This method uses the cosine distance between the input message and the urgency rules in the database. Since it only uses embeddings, it is fast and cheap to run.</p>"},{"location":"components/urgency-detection/#setup","title":"Setup","text":"<p>Set the following environment variables.</p> <ol> <li>Set <code>URGENCY_CLASSIFIER</code> environment variable to <code>cosine_distance_classifier</code>.</li> <li>Set <code>URGENCY_DETECTION_MAX_DISTANCE</code> environment variable. Any message with a cosine distance greater than this value will be tagged as urgent.</li> </ol> <p>You can do this either in the <code>.env</code> file or under <code>core_backend/app/urgency_detection/config.py</code>. See Configuring AAQ for more details.</p>"},{"location":"components/urgency-detection/#method-2-llm-entailment-classifier","title":"Method 2: LLM entailment classifier","text":"<ul> <li>Cost: </li> <li>Accuracy: </li> <li>Latency: </li> </ul> <p>This method calls an LLM to score the message against each of the urgency rules in the database.</p>"},{"location":"components/urgency-detection/#setup_1","title":"Setup","text":"<p>Set the following environment variables.</p> <ol> <li>Set <code>URGENCY_CLASSIFIER</code> environment variable to <code>llm_entailment_classifier</code>.</li> <li>Set <code>URGENCY_DETECTION_MIN_PROBABILITY</code> environment variable. The LLM returns the probability of a message being urgent. Any message with a probability greater than this value will be tagged as urgent.</li> </ol> <p>You can do this either in the <code>.env</code> file or under <code>core_backend/app/urgency_detection/config.py</code>. See Configuring AAQ for more details.</p> <p>See OpenAPI specification or SwaggerUI for more details on how to call the service.</p>"},{"location":"components/urgency-detection/#more-details","title":"More details","text":"<ul> <li>Blog post on Urgency Detection.</li> <li>SwaggerUI</li> </ul>"},{"location":"deployment/architecture/","title":"Architecture","text":"<p>We use docker-compose to orchestrate containers with a reverse proxy that manages all incoming traffic to the service. The database and LiteLLM proxy are only accessed by the core app.</p> <p> </p>"},{"location":"deployment/config-options/","title":"Configuring AAQ","text":"<p>There are several aspects of AAQ you can configure:</p> <ol> <li> <p>Application configs through environment variables</p> <p>All required and optional environment variables are defined in <code>deployment/docker-compose/template.*.env</code> files. You will need to copy the templates into <code>.*.env</code> files.</p> <pre><code>cp template.base.env .base.env\ncp template.core_backend.env .core_backend.env\ncp template.litellm_proxy.env .litellm_proxy.env\n</code></pre> <p>To get a local setup running with docker compose, you won't need to change any values except for LLM credentials in <code>.litellm_proxy.env</code>.</p> <p>See the rest of this page for more information on the environment variables.</p> </li> <li> <p>LLM models in    <code>litellm_proxy_config.yaml</code></p> <p>This defines which LLM to use for which task. You may want to change the LLMs and specific calling parameters based on your needs.</p> </li> <li> <p>LLM prompts in    <code>llm_prompts.py</code></p> <p>While all prompts have been carefully selected to perform each task well, you can customize them to your need here.</p> </li> </ol> <p></p> <p>Understanding the template environment files <code>template.*.env</code></p> <p>For local testing and development, the values shoud work as is, except for LLM API credentials in <code>.litellm_proxy.env</code></p> <p>For production, make sure you confirm or update the ones marked \"change for production\" at the least.</p> <ol> <li>Secrets have been marked with \ud83d\udd12.</li> <li>All optional values have been commented out. Uncomment to customize for your own case.</li> </ol>"},{"location":"deployment/config-options/#aaq-wide-configurations","title":"AAQ-wide configurations","text":"<p>The base environment variables are shared by <code>caddy</code> (reverse proxy), <code>core_backend</code>, and <code>admin_app</code> during run time.</p> <p>If not done already, copy the template environment file to <code>.base.env</code></p> <pre><code>cd deployment/docker-compose/\ncp template.base.env .base.env\n</code></pre> <p>Then, edit the environment variables according to your need (guide on updating the template):</p> <code>deployment/docker-compose/template.base.env</code><pre><code>#### AAQ domain -- change for production ######################################\nDOMAIN=\"localhost\"\n# Example value: `example.domain.com`\n# This is the domain that admin_app will be hosted on. core_backend will be\n# hosted on ${DOMAIN}/${BACKEND_ROOT_PATH}.\n\nBACKEND_ROOT_PATH=\"/api\"\n# This is the path that core_backend will be hosted on.\n# Only change if you want to use a different backend root path.\n\n#### Google OAuth Client ID ###################################################\n# NEXT_PUBLIC_GOOGLE_LOGIN_CLIENT_ID=\"update-me\"\n# If you want to use Google OAuth, set the correct value for your production.\n# This value is used by core_backend and admin_app.\n\n#### Backend URL ##############################################################\nNEXT_PUBLIC_BACKEND_URL=\"https://${DOMAIN}${BACKEND_ROOT_PATH}\"\n# Do not change this value. This value is used by admin_app.\n# If not set, it will default to \"http://localhost:8000\" in the admin_app.\n</code></pre>"},{"location":"deployment/config-options/#configuring-the-backend-core_backend","title":"Configuring the backend (<code>core_backend</code>)","text":""},{"location":"deployment/config-options/#environment-variables-for-the-backend","title":"Environment variables for the backend","text":"<p>If not done already, copy the template environment file to <code>.core_backend.env</code> (guide on updating the template):</p> <pre><code>cd deployment/docker-compose/\ncp template.core_backend.env .core_backend.env\n</code></pre> <p>The <code>core_backend</code> uses the following required and optional (commented out) environment variables.</p> <code>deployment/docker-compose/template.core_backend.env</code><pre><code># If not set, default values are loaded from core_backend/app/**/config.py files\n\n#### \ud83d\udd12 Postgres variables -- change for production ###########################\nPOSTGRES_USER=postgres\nPOSTGRES_PASSWORD=postgres  #pragma: allowlist secret\nPOSTGRES_HOST=localhost\nPOSTGRES_PORT=5432\nPOSTGRES_DB=postgres\n\n#### \ud83d\udd12 Admin user -- change for production ###################################\nADMIN_USERNAME=\"admin\"\nADMIN_PASSWORD=\"fullaccess\" #pragma: allowlist secret\nADMIN_API_KEY=\"admin-key\"   #pragma: allowlist secret\n\n#### Admin user rate limits ###################################################\n# ADMIN_CONTENT_QUOTA=1000\n# ADMIN_API_DAILY_QUOTA=100\n\n#### \ud83d\udd12 JWT -- change for production ###########################################\nJWT_SECRET=\"jwt-secret\"    #pragma: allowlist secret\n\n#### Redis  -- change for production ##########################################\nREDIS_HOST=\"redis://localhost:6379\"\n# For docker compose, use \"redis://redis:6379\"\n\n#### LiteLLM Proxy Server -- change for production ############################\nLITELLM_ENDPOINT=\"http://localhost:4000\"\n# For docker compose, use \"http://litellm_proxy:4000\"\n\n#### Variables for Huggingface embeddings container ###########################\n# If on ARM, you need to build the embeddings image manually using\n# `make build-embeddings-arm` from repository root and set the following variables\n#EMBEDDINGS_IMAGE_NAME=text-embeddings-inference-arm\n#PGVECTOR_VECTOR_SIZE=1024\n\n#### Speech APIs ###############################################################\n# CUSTOM_SPEECH_ENDPOINT=http://speech_service:8001/transcribe\n\n#### Temporary folder for prometheus gunicorn multiprocess ####################\nPROMETHEUS_MULTIPROC_DIR=\"/tmp\"\n\n#### Application-wide content limits ##########################################\n# CHECK_CONTENT_LIMIT=True\n# DEFAULT_CONTENT_QUOTA=50\n\n#### Number of top content to return for /search. #############################\n# N_TOP_CONTENT=5\n\n#### Urgency detection variables ##############################################\n# URGENCY_CLASSIFIER=\"cosine_distance_classifier\"\n# Choose between `cosine_distance_classifier` and `llm_entailment_classifier`\n\n# URGENCY_DETECTION_MAX_DISTANCE=0.5\n# Only used if URGENCY_CLASSIFIER=cosine_distance_classifier\n\n# URGENCY_DETECTION_MIN_PROBABILITY=0.5\n# Only used if URGENCY_CLASSIFIER=llm_entailment_classifier\n\n#### LLM response alignment scoring ###########################################\n# ALIGN_SCORE_THRESHOLD=0.7\n\n#### LiteLLM tracing ##########################################################\nLANGFUSE=False\n\n# \ud83d\udd12 Keys\n# LANGFUSE_PUBLIC_KEY=\"pk-...\"\n# LANGFUSE_SECRET_KEY=\"sk-...\"  #pragma: allowlist secret\n# Set LANGFUSE=True to enable Langfuse logging, and set the keys.\n# See https://docs.litellm.ai/docs/observability/langfuse_integration for more\n# information.\n\n# Optional based on your Langfuse host:\n# LANGFUSE_HOST=\"https://cloud.langfuse.com\"\n\n##### Google Cloud Storage Variables#############################################\n# GCS_SPEECH_BUCKET=\"aaq-speech-test\"\n# Set this variable up to your specific GCS bucket for storage and retrieval for Speech Workflow.\n</code></pre>"},{"location":"deployment/config-options/#other-configurations-for-the-backend","title":"Other configurations for the backend","text":"<p>You can view all configurations that <code>core_backend</code> uses in <code>core_backend/app/*/config.py</code> files -- for example, <code>core_backend/app/config.py</code>.</p> Environment variables take precedence over the config file. <p>You'll see in the config files that we get parameters from the environment and if not found, we fall back on defaults provided. So any environment variables set will override any defaults you have set in the config file.</p>"},{"location":"deployment/config-options/#configuring-litellm-proxy-server-litellm_proxy","title":"Configuring LiteLLM Proxy Server (<code>litellm_proxy</code>)","text":""},{"location":"deployment/config-options/#litellm-proxy-server-configurations","title":"LiteLLM Proxy Server configurations","text":"<p>You can edit the default LiteLLM Proxy Server settings by updating <code>litellm_proxy_config.yaml</code>. Learn more about the server configuration in LiteLLM Proxy Server.</p>"},{"location":"deployment/config-options/#authenticating-litellm-proxy-server-to-llms","title":"Authenticating LiteLLM Proxy Server to LLMs","text":"<p>The <code>litellm_proxy</code> server uses the following required and optional (commented out) environment variables for authenticating to external LLM APIs (guide on updating the template).</p> <p>You will need to set up the correct credentials (API keys, etc.) for all LLM APIs declared in <code>litellm_proxy_config.yaml</code>. See LiteLLM's documentation for more information about authentication for different LLMs.</p> <code>deployment/docker-compose/template.litellm_proxy.env</code><pre><code># For every LLM API you decide to use, defined in litellm_proxy_config.yaml,\n# ensure you set up the correct authentication(s) here.\n\n#### \ud83d\udd12 Vertex AI auth -- change for production ###############################\n# Must be set if using VertexAI models\nGOOGLE_APPLICATION_CREDENTIALS=\"/app/credentials.json\"\n# Path to the GCP credentials file *within* litellm_proxy container.\n# This default value should work with docker compose.\n\nVERTEXAI_PROJECT=\"gcp-project-id-12345\"\nVERTEXAI_LOCATION=\"us-central1\"\nVERTEXAI_ENDPOINT=\"https://us-central1-aiplatform.googleapis.com\"\n# Vertex AI endpoint. Note that you may want to increase the request quota from\n# GCP's APIs console.\n\n#### \ud83d\udd12 OpenAI auth -- change for production ##################################\n# Must be set if using OpenAI APIs.\nOPENAI_API_KEY=\"sk-...\"\n\n#### \ud83d\udd12 Huggingface embeddings -- change for production #######################\n# HUGGINGFACE_MODEL=\"Alibaba-NLP/gte-large-en-v1.5\"\n# HUGGINGFACE_EMBEDDINGS_API_KEY=\"embeddings\"  #pragma: allowlist secret\n# HUGGINGFACE_EMBEDDINGS_ENDPOINT=\"http://huggingface-embeddings\"\n# This default endpoint value should work with docker compose.\n\n#### \ud83d\udd12 LiteLLM Proxy UI -- change for production #############################\n# UI_USERNAME=\"admin\"\n# UI_PASSWORD=\"admin\"\n</code></pre>"},{"location":"deployment/config-options/#configuring-optional-components","title":"Configuring optional components","text":"<p>See instructions for setting these in the documentation for the specific optional component at Optional components.</p>"},{"location":"deployment/quick-setup/","title":"Quick Setup with Docker Compose","text":""},{"location":"deployment/quick-setup/#quick-setup","title":"Quick setup","text":"<p>You need to have installed Docker</p> <p>Step 1: Clone the AAQ repository.</p> <pre><code>git clone git@github.com:IDinsight/ask-a-question.git\n</code></pre> <p>Step 2: Navigate to the <code>deployment/docker-compose/</code> subfolder.</p> <pre><code>cd deployment/docker-compose/\n</code></pre> <p>Step 3: Copy <code>template.*.env</code> files to <code>.*.env</code>:</p> <pre><code>cp template.base.env .base.env\ncp template.core_backend.env .core_backend.env\ncp template.litellm_proxy.env .litellm_proxy.env\n</code></pre> <p>Step 4: Configure LiteLLM Proxy server</p> <ol> <li>(optional) Edit <code>litellm_proxy_config.yaml</code> with LLM services you want to use. See    LiteLLM Proxy Server for more details.</li> <li>Update the API key(s) and authentication information in    <code>.litellm_proxy.env</code>. Make sure you set up the correct authentication for the LLM    services defined in <code>litellm_proxy_config.yaml</code>.</li> </ol> <p>Step 5: Run docker-compose</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack up -d --build\n</code></pre> <p>You can now view the AAQ admin app at <code>https://$DOMAIN/</code> (by default, this should be https://localhost/) and the API documentation at <code>https://$DOMAIN/api/docs</code> (you can also test the endpoints here).</p> <p>Step 6: Shutdown containers</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml -p aaq-stack down\n</code></pre>"},{"location":"deployment/quick-setup/#ready-to-deploy","title":"Ready to deploy?","text":"<p>See Configuring AAQ to configure your app.</p>"},{"location":"develop/contributing/","title":"Contributing to AAQ","text":"<p>Thank you for your interest in contributing to AAQ!</p> <p>AAQ is an open source project started by data scientists at IDinsight and sponsored by Google.org. Everyone is welcome to contribute! </p> <p>Note</p> <p>If you want to set up the complete development environment for AAQ first, you can follow the setup instructions here. Otherwise, this page provides general guidelines on how to contribute to the project with a minimal setup.</p>"},{"location":"develop/contributing/#pull-requests-guide","title":"Pull requests guide","text":"<p>This section shows you how to raise a pull request to the project.</p>"},{"location":"develop/contributing/#make-a-fork","title":"Make a fork","text":"<ol> <li>Fork the project repository by clicking on the \"Fork\" button.</li> <li> <p>Clone the repo using:</p> <pre><code>git clone git@github.com:&lt;your GitHub handle&gt;/ask-a-question.git\n</code></pre> </li> <li> <p>Navigate to the project directory.</p> <pre><code>cd ask-a-question\n</code></pre> </li> </ol>"},{"location":"develop/contributing/#install-prerequisites","title":"Install prerequisites","text":"<p>Install conda.</p>"},{"location":"develop/contributing/#setup-your-virtual-python-environment","title":"Setup your virtual Python environment","text":"<p>You can automatically create a ready-to-go <code>aaq</code> conda environment with:</p> <pre><code>make fresh-env\nconda activate aaq\n</code></pre> <p>If you encounter errors while installing <code>psycopg2</code>, see here for troubleshooting.</p> <p>If you would like to set up your Python environment manually, you can follow the steps here.</p>"},{"location":"develop/contributing/#make-your-changes","title":"Make your changes","text":"<ol> <li> <p>Create a <code>feature</code> branch for your development changes:</p> <pre><code>git checkout -b feature\n</code></pre> </li> <li> <p>Run <code>mypy</code> with <code>mypy core_backend/app</code> (1)</p> </li> <li> <p>Then <code>git add</code> and <code>git commit</code> your changes:</p> <pre><code>git add modified_files\ngit commit\n</code></pre> </li> <li> <p>And then push the changes to your fork in GitHub</p> <pre><code>git push -u origin feature\n</code></pre> </li> <li> <p>Go to the GitHub web page of your fork of the AAQ repo. Click the <code>Pull request</code> button to send your changes to the project\u2019s maintainers for review. This will send a notification to the committers.</p> </li> </ol> <ol> <li><code>pre-commit</code> runs in its own virtual environment. Since <code>mypy</code> needs all the    packages installed, this would mean keeping a whole separate copy of your    environment just for it. That's too bulky so the pre-commit only checks    a few high-level typing things. You still need to run <code>mypy</code> directly to catch    all the typing issues.    If you forget, GitHub Actions 'Linting' workflow will pick up all the mypy errors.</li> </ol>"},{"location":"develop/contributing/#next-steps","title":"Next steps","text":"<p>If you haven't already, see Setup on how to set up the complete development environment for AAQ. Otherwise, you can check out how to test the AAQ codebase.</p>"},{"location":"develop/setup/","title":"Setting up your development environment","text":""},{"location":"develop/setup/#step-1-fork-the-repository","title":"Step 1: Fork the repository","text":"<p>Please fork the project repository by clicking on the \"Fork\" button. Then, clone the repo using your own GitHub handle:</p> <pre><code>git clone git@github.com:&lt;your GitHub handle&gt;/ask-a-question.git\n</code></pre> <p>For questions related to setup, please contact AAQ Support</p>"},{"location":"develop/setup/#step-2-configure-environment-variables","title":"Step 2: Configure environment variables","text":"<ol> <li> <p>Navigate to the <code>deployment/docker-compose</code> directory.</p> <pre><code>cd ask-a-question/deployment/docker-compose\n</code></pre> </li> <li> <p>Copy <code>template.*.env</code> into new files named <code>.*.env</code> within the same directory:</p> <pre><code>cp template.base.env .base.env\ncp template.core_backend.env .core_backend.env\ncp template.litellm_proxy.env .litellm_proxy.env\n</code></pre> </li> <li> <p>Update <code>.litellm_proxy.env</code> with LLM service credentials. This will be used by    LiteLLM Proxy Server to authenticate to LLM services.</p> <p>For local development setup, this is the only file you need to update to get started. For more information on the variables used here and other template environment files, see Configuring AAQ.</p> </li> <li> <p>(optional) Edit which LLMs are used in the    <code>litellm_proxy_config.yaml</code>.</p> </li> </ol>"},{"location":"develop/setup/#step-3-set-up-your-development-environment","title":"Step 3: Set up your development environment","text":"<p>Once you are done with steps 1 &amp; 2, there are two ways to set up your development environment for AAQ:</p> <ol> <li>Docker Compose Watch</li> <li>Manual</li> </ol> <p>You can view the pros and cons of each method at the bottom.</p>"},{"location":"develop/setup/#set-up-using-docker-compose-watch","title":"Set up using Docker Compose Watch","text":""},{"location":"develop/setup/#install-prerequisites","title":"Install prerequisites","text":"<ol> <li>Install Docker.</li> <li>If you are not using Docker Desktop, install Docker Compose with version &gt;=2.22 to use the <code>watch</code> command.</li> </ol>"},{"location":"develop/setup/#run-docker-compose-watch","title":"Run <code>docker compose watch</code>","text":"<p>If not done already, configure the environment variables in Step 2.</p> <ol> <li> <p>In the <code>deployment/docker-compose</code> directory, run</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack watch\n</code></pre> Here's what you should see if the above command executes successfully <pre><code>\u2714 Network aaq-stack_default            Created\n\u2714 Volume \"aaq-stack_db_volume\"         Created\n\u2714 Volume \"aaq-stack_caddy_data\"        Created\n\u2714 Volume \"aaq-stack_caddy_config\"      Created\n\u2714 Container aaq-stack-caddy-1          Started\n\u2714 Container aaq-stack-litellm_proxy-1  Started\n\u2714 Container aaq-stack-relational_db-1  Started\n\u2714 Container aaq-stack-core_backend-1   Started\n\u2714 Container aaq-stack-admin_app-1      Started\nWatch enabled\n</code></pre> <p>The app will now run and update with any changes made to the <code>core_backend</code> or <code>admin_app</code> folders.</p> <p>The admin app will be available on https://localhost and the backend API testing UI on https://localhost/api/docs.</p> </li> <li> <p>To stop AAQ, first exit the running app process in your terminal with <code>ctrl+c</code> and then run:</p> <pre><code>docker compose -f docker-compose.yml -f docker-compose.dev.yml \\\n    -p aaq-stack down\n</code></pre> </li> </ol>"},{"location":"develop/setup/#set-up-manually","title":"Set up manually","text":""},{"location":"develop/setup/#install-prerequisites_1","title":"Install prerequisites","text":"<ol> <li>Install    conda.</li> <li> <p>Install Node.js v19.</p> Setting up a different NodeJS version <p>If you have a different NodeJS version installed already, you can switch to a different version by installing Node Version Manager and then executing</p> <pre><code>nvm install 19\nnvm use 19\n</code></pre> </li> <li> <p>Install Docker.</p> </li> <li>(optional) Install GNU Make. <code>make</code> is used to run commands (targets) in <code>Makefile</code>s and can be used to automate various setup procedures.</li> </ol>"},{"location":"develop/setup/#set-up-the-backend","title":"Set up the backend","text":"<ol> <li> <p>Navigate to <code>ask-a-question</code> repository root.</p> <pre><code>cd ask-a-question\n</code></pre> </li> <li> <p>Set up your Python environment by creating a new conda environment using <code>make</code>.</p> <pre><code>make fresh-env\n</code></pre> <p>This command will remove any existing <code>aaq</code> conda environment and create a new <code>aaq</code> conda environment with the required Python packages.</p> <p></p> Errors with <code>psycopg2</code>? <p><code>psycopg2</code> vs <code>psycopg2-binary</code>: For production use cases we should use <code>psycopg2</code> but for local development, <code>psycopg2-binary</code> suffices and is often easier to install. If you are getting errors from <code>psycopg2</code>, try installing <code>psycopg2-binary</code> instead.</p> <p>If you would like <code>psycopg2-binary</code> instead of <code>psycopg2</code>, run <code>make fresh-env psycopg_binary=true</code> instead (see below for why you may want this).</p> <p>See here for more details.</p> <p></p> Setting up the Python environment manually <p>Create virtual environment</p> <pre><code>conda create --name aaq python=3.10\n</code></pre> <p>Install Python packages</p> <pre><code>pip install -r core_backend/requirements.txt\npip install -r requirements-dev.txt\n</code></pre> <p>Install pre-commit</p> <p><code>pre-commit</code> is used to ensure that code changes  are formatted correctly. It is only necessary if you are planning on making  changes to the codebase.</p> <pre><code>pre-commit install\n</code></pre> </li> <li> <p>Activate your <code>aaq</code> conda environment.</p> <pre><code>conda activate aaq\n</code></pre> </li> <li> <p>Set up the required Docker containers by running</p> <pre><code>make setup-dev\n</code></pre> <p>The command will start the following containers:</p> <ul> <li>PostgreSQL with pgvector extension</li> <li>LiteLLM Proxy Server</li> <li>Redis</li> </ul> Setting up the DB, LiteLLM Proxy Server, and Redis seprately <p>If you would like to set up each of these dependencies separately, check out the Makefile at repository root.</p> </li> <li> <p>Export the environment variables you defined earlier in Step    2 by running</p> <pre><code>set -a\nsource deployment/docker-compose/.base.env\nsource deployment/docker-compose/.core_backend.env\nset +a\n</code></pre> Saving environment variables in <code>aaq</code> conda environment <p>You can set environment variables by either running <code>conda env config vars set &lt;NAME&gt;=&lt;VALUE&gt;</code> for each required environment variable, or save them in the environment activation script.</p> </li> <li> <p>Start the backend app.</p> <pre><code>python core_backend/main.py\n</code></pre> Here's what you should see if the above command executes successfully <pre><code>INFO:     Will watch for changes in these directories: ['~/ask-a-question']\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [73855] using StatReload\nINFO:     Started server process [73858]\nINFO:     Waiting for application startup.\n07/15/2024 12:29:08 PM          __init__.py  79 : Application started\nINFO:     Application startup complete.\n</code></pre> <p>This will launch the application in \"reload\" mode (i.e., the app will automatically refresh everytime you make a change to one of the files).</p> <p>You can test the endpoint for the API documentation by going to http://localhost:8000/docs (the backend itself runs on http://localhost:8000).</p> </li> <li> <p>To stop the backend app, first exit the running app process in your terminal with <code>ctrl+c</code> and then run:</p> <pre><code>make teardown-dev\n</code></pre> </li> </ol>"},{"location":"develop/setup/#set-up-the-frontend","title":"Set up the frontend","text":"<ol> <li> <p>Navigate to <code>ask-a-question</code> repository root and activate the <code>aaq</code> virtual environment:</p> <pre><code>cd ask-a-question\n</code></pre> <pre><code>conda activate aaq\n</code></pre> </li> <li> <p>In a new terminal:</p> <pre><code>cd admin_app\nnvm use 19\nnpm install\nnpm run dev\n</code></pre> Here's what you should see if the above commands execute successfully <pre><code>&gt; admin-app@0.2.0 dev\n&gt; next dev\n\n\u25b2 Next.js 14.1.3\n- Local:        http://localhost:3000\n\n\u2713 Ready in 1233ms\n</code></pre> <p>This will install the required NodeJS packages for the admin app and start the admin app in <code>dev</code> (i.e., autoreload) mode. The admin app will now be accessible on http://localhost:3000/.</p> <p>You can login with either the default credentials (username: <code>admin</code>, password: <code>fullaccess</code>) or the ones you specified in <code>.core_backend.env</code>.</p> </li> <li> <p>To stop the admin app, exit the running app process in your terminal with <code>ctrl</code>+<code>c</code>.</p> </li> </ol>"},{"location":"develop/setup/#pros-and-cons-of-each-setup-method","title":"Pros and cons of each setup method","text":"Method Pros Cons Set up using docker compose watch <ul><li>Good for end-to-end testing</li><li>Local environment identical to production deployment</li><li>No need to setup local environment</li><li>Set environment variables and configs once</li></ul> <ul><li>Changes take 20-30s to be reflected in the app</li></ul> Set up manually <ul><li>Instant feedback from changes</li></ul> <ul><li>Requires more configuration before each run</li><li>Requires environment and dependencies to be set up correctly</li><ul>"},{"location":"develop/setup/#step-4-set-up-docs","title":"Step 4: Set up docs","text":"<p>Note</p> <p>Ensure that you are in the <code>ask-a-question</code> project directory and that you have activated the <code>aaq</code> virtual environment before proceeding.</p> <p>To host docs offline so that you can see documentation changes in real-time, run the following from <code>ask-a-question</code> repository root with an altered port (so that it doesn't interfere with the app's server):</p> <pre><code>mkdocs serve -a localhost:8080\n</code></pre>"},{"location":"develop/testing/","title":"Writing and running tests","text":"<p>If you are writing new features, you should also add unit tests. Tests are under <code>core_backend/tests</code>.</p>"},{"location":"develop/testing/#running-unit-tests","title":"Running unit tests","text":"<p>You need to have installed Docker</p> Don't run <code>pytest</code> directly <p>Unless you have updated your environment variables and started a testing instance of postrges, the tests will end up writing to your dev environment </p> <p>Run tests using:</p> <pre><code>make tests\n</code></pre> <p>This target starts up new postgres and qdrant containers for testing. It also sets the correct environment variables, runs <code>pytest</code>, and then destroys the containers.</p>"},{"location":"develop/testing/#debugging-unit-tests","title":"Debugging unit tests","text":"<p>Before debugging, run <code>make setup-test-db</code> within <code>core_backend</code> to launch new postgres container for testing and set the correct environment variables.</p> <p>After debugging, clean up the testing resources by calling <code>make teardown-test-db</code>.</p>"},{"location":"develop/testing/#configs-for-visual-studio-code","title":"Configs for Visual Studio Code","text":"<code>.vscode/launch.json</code> <p>Add the following configuration to your <code>.vscode/launch.json</code> file to set environment variables for debugging:</p> <pre><code>{\n    \"version\": \"0.2.0\",\n    \"configurations\": [\n        {  // configuration for debugging\n            \"name\": \"Python: Tests in current file\",\n            \"purpose\": [\"debug-test\"],\n            \"type\": \"python\",\n            \"request\": \"launch\",\n            \"program\": \"${file}\",\n            \"args\": [\"--color=yes\"],\n            \"envFile\": \"${workspaceFolder}/core_backend/tests/api/test.env\",\n            \"console\": \"integratedTerminal\",\n            \"justMyCode\": false\n        }\n    ]\n}\n</code></pre> <code>.vscode/settings.json</code> <p>Add the following configuration to <code>.vscode/settings.json</code> to set the correct pytest working directory and environment variables:</p> <pre><code>{\n    \"python.testing.cwd\": \"${workspaceFolder}/core_backend\",\n    \"python.testing.pytestArgs\": [\n        \"tests\",\n        \"--rootdir=${workspaceFolder}/core_backend\"\n    ],\n    \"python.testing.unittestEnabled\": false,\n    \"python.testing.pytestEnabled\": true,\n    \"python.envFile\": \"${workspaceFolder}/core_backend/tests/api/test.env\"\n}\n</code></pre>"},{"location":"develop/testing/#running-optional-tests","title":"Running optional tests","text":"<p>There are some additional tests that are not run by default. Most of these either make call to OpenAI or depend on other components:</p> <ul> <li>Language Identification: Tests if the the solution is able to identify the language given a short sample text.</li> <li>Test if LLM response aligns with provided context: Tests for hallucinations by checking if the LLM response is supported by the provided context.</li> <li>Test safety: Tests for prompt injection and jailbreaking.</li> </ul> <p>These tests will require the LiteLLM Proxy server to be running (to accept LLM calls). You can run this by going to root and running:</p> <pre><code>make setup-llm-proxy\n</code></pre> <p>Then run the tests using:</p> <pre><code>cd core_backend\nmake setup-test-db\npython -m pytest -m rails\n</code></pre> <p>And when done:</p> <pre><code>make teardown-test-db\n</code></pre>"},{"location":"develop/validation/","title":"Running validation","text":"<p>Currently, there is validation only for retrieval, i.e. <code>POST /search</code> endpoint with <code>\"generate_llm_response\": false</code></p> <p>To evaluate the performance of your model (along with your own configurations and guardrails), run the validation test(s) in <code>core_backend/validation</code>.</p>"},{"location":"develop/validation/#retrieval-search-validation","title":"Retrieval (<code>/search</code>) validation","text":"<p>We evaluate the \"performance\" of retrieval by computing \"Top K Accuracy\", which is defined as proportion of times the best matching answer was present in top K retrieved contents.</p>"},{"location":"develop/validation/#preparing-the-data","title":"Preparing the data","text":"<p>The test assumes the validation data contains a single label representing the best matching content, rather than a ranked list of all relevant content.</p> <p>An example validation data will look like</p> query label \"How?\" 0 \"When?\" 1 \"What year was it?\" 1 \"May I?\" 2 <p>An example content data will look like</p> content_text label \"Here's how.\" 0 \"It was 2024.\" 1 \"Yes\" 2"},{"location":"develop/validation/#setting-up","title":"Setting up","text":"<ol> <li>Create a new python environment:     <pre><code>conda create -n \"aaq-validate\" python=3.10\n</code></pre>     You can also copy the existing <code>aaq</code> environment.</li> <li>Install requirements. This assumes you are in project root <code>ask-a-question</code>.     <pre><code>conda activate aaq-validate\npip install -r core_backend/requirements.txt\npip install -r core_backend/validation/requirements.txt\n</code></pre></li> <li> <p>Set environment variables.</p> <ol> <li> <p>You must export the required environment variables. They are defined with default values in     <code>core_backend/validation/validation.env</code>. To ensure that these env variables are     set every time you activate <code>aaq-validate</code>, you can run the     following command for each variable:     <pre><code>conda env config vars set &lt;VARIABLE&gt;=&lt;VALUE&gt;\n</code></pre></p> </li> <li> <p>For optional ones, check out the defaults in <code>core_backend/app/configs/app_config.py</code>     and modify as per your own requirements. For example:     <pre><code>conda env config vars set LITELLM_MODEL_EMBEDDING=&lt;...&gt;\n</code></pre></p> </li> <li>If you are using an external LLM endpoint, e.g. OpenAI, make sure to export the     API key variable as well.     <pre><code>conda env config vars set OPENAI_API_KEY=&lt;Your OPENAI API key&gt;\n</code></pre></li> </ol> </li> </ol>"},{"location":"develop/validation/#running-retrieval-validation","title":"Running retrieval validation","text":"<p>In project root <code>ask-a-question</code> run the following command. (Perform any necessary    authentication steps you need to do, e.g. for AWS login).     <pre><code>cd ask-a-question\n\npython -m pytest core_backend/validation/validate_retrieval.py \\\n    --validation_data_path &lt;path&gt; \\\n    --content_data_path &lt;path&gt; \\\n    --validation_data_question_col &lt;name&gt; \\\n    --validation_data_label_col &lt;name&gt; \\\n    --content_data_label_col &lt;name&gt; \\\n    --content_data_text_col &lt;name&gt; \\\n    --notification_topic &lt;topic ARN, if using AWS SNS&gt; \\\n    --aws_profile &lt;aws SSO profile name, if required&gt; \\\n    -n auto -s\n</code></pre> <code>-n auto</code> allows multiprocessing to speed up the test, and <code>-s</code> ensures logging by     the test module is shown on your stdout.</p> <pre><code>For details of the command line arguments, see the \"Custom options\" section of the\noutput for the following command:\n```shell\npython -m pytest core_backend/validation/validate_retrieval.py --help\n```\n</code></pre>"},{"location":"integrations/","title":"Integrations","text":"<p>In this section you can find the different integrations AAQ supports so far.</p>"},{"location":"integrations/#chat-managers","title":"Chat Managers","text":"<p>You can use the AAQ endpoints through various chat managers. Below are some examples:</p> <ul> <li> <p></p> <p>How to get Typebot running and connected to AAQ endpoints</p> <p> More info</p> </li> <li> <p> Botpress</p> <p>How to get Botpress v12 (OSS) running and connected to AAQ endpoints</p> <p> More info</p> </li> <li> <p></p> <p>How to connect a Turn.io Journey to AAQ endpoints</p> <p> More info</p> </li> <li> <p></p> <p>How to connect a Glific flow to AAQ endpoints</p> <p> More info</p> </li> </ul>"},{"location":"integrations/chat_managers/botpress_v12/","title":"Botpress v12  Setup Instructions","text":"<p>Below is an example of how to get Botpress v12 (OSS) running and connected to AAQ endpoints using a provided demo flow.</p> <p>Note: Botpress v12 is open-source and available to self-host but Botpress Cloud is a different closed-source product.</p>"},{"location":"integrations/chat_managers/botpress_v12/#demo-aaq-flow","title":"Demo AAQ flow","text":"<ol> <li>Once you have deployed Botpress v12 as per your requirements, go to the URL where the app is running</li> <li>Make an account and login</li> <li>Go to \"Create Bot\" and then \"Import Existing\" (you can set Bot ID to anything you want)</li> <li>Load the <code>.tgz</code> file given under <code>chat_managers/botpress_v12/</code> in the AAQ repo</li> <li> <p>Edit the \"API Call\" cards to reflect the AAQ endpoint URL that you have running *</p> <p>a. Click on the card</p> <p>b. Click on \"Edit skill\"</p> <p>c. Change the base of the URL at the top</p> <p>d. If you've changed the bearer token for the QA endpoints, you'll have to update the headers sections too</p> </li> <li> <p>Test the bot in the emulator</p> </li> </ol> * Errors with using <code>localhost</code> on the API Call skill? <p>If you're having trouble with localhost AAQ calls, try forwarding traffic through <code>ngrok</code> and using that for deployment of AAQ.</p> <ol> <li> <p>Install and configure ngrok</p> </li> <li> <p>Run <code>ngrok http https://localhost</code> to forward traffic</p> </li> <li> <p>In <code>deployment/.env</code> file, ensure you have</p> <pre><code>NEXT_PUBLIC_BACKEND_URL=https://[NGROK URL]/api\nBACKEND_ROOT_PATH=\"/api\"\n</code></pre> </li> <li> <p>In <code>deployment/.env.nginx</code> file ensure you have</p> <pre><code>DOMAIN=[NGROK URL]  # don't add https/http at the front\n</code></pre> </li> <li> <p>Change the base of the API Call skill so it looks like:</p> <pre><code>[NGROK URL]/api/search\n</code></pre> </li> </ol>"},{"location":"integrations/chat_managers/botpress_v12/#self-hosted-deployment","title":"Self-hosted deployment","text":""},{"location":"integrations/chat_managers/botpress_v12/#option-1-via-docker-compose-behind-caddy-with-https","title":"Option 1 - Via Docker Compose (behind Caddy with HTTPS)","text":"<p>Step 1: Navigate to <code>chat_managers/botpress_v12/deployment/</code></p> <p>Step 2: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables</p> <p>Step 3: Run docker compose</p> <pre><code>docker compose -p botpress-stack up -d --build\n</code></pre> <p>You can now access Botpress at <code>https://[DOMAIN]/</code></p> <p>Step 4: Shutdown containers</p> <pre><code>docker compose -p botpress-stack down\n</code></pre>"},{"location":"integrations/chat_managers/botpress_v12/#option-2-via-docker","title":"Option 2 - Via Docker","text":"<p>To install through Docker (recommended), follow the official Botpress v12 docs here. In short:</p> <ol> <li> <p>Get the image</p> <pre><code>docker pull botpress/server\n</code></pre> </li> <li> <p>Run the image</p> <pre><code>docker run -d --name=botpress -p 3000:3000 botpress/server\n</code></pre> </li> </ol>"},{"location":"integrations/chat_managers/botpress_v12/#option-3-via-executables","title":"Option 3 - Via executables","text":"<p>Follow the official docs here to set up Botpress v12 locally as per your OS.</p>"},{"location":"integrations/chat_managers/typebot/","title":"Setup Instructions","text":"<p>Below is a guide on how to connect to AAQ endpoints using a provided demo flow.</p> <p>You can either use the cloud-hosted Typebot service or self-host the application (see \"Deployment\" below).</p>"},{"location":"integrations/chat_managers/typebot/#demo-aaq-flow","title":"Demo AAQ flow","text":"<ol> <li>Go to your Typebot instance</li> <li>Make an account and login</li> <li>Go to \"Create a typebot\" and then \"Import a file\"</li> <li>Load the <code>.json</code> file given under <code>chat_managers/typebot/</code> in the AAQ repo</li> <li> <p>Edit the \"API Call\" cards to reflect the AAQ endpoint URL that you have running</p> <p>a. Click on the card b. Change the base of the URL at the top c. Add the bearer token in the Headers section.</p> </li> <li> <p>Test the bot in the emulator</p> </li> </ol>"},{"location":"integrations/chat_managers/typebot/#deployment","title":"Deployment","text":"<p>For self-hosting, you can either follow the official docs or follow our quick start below:</p> <p>Step 1: Navigate to <code>chat_managers/typebot/deployment/</code></p> <p>Step 2: Copy <code>template.env</code> to <code>.env</code> and edit it to set the variables</p> You must configure at least one login option while setting the environment variables. <p>We recommend either Github or Google authentication. See Typebot's docs for details.</p> <p>Step 3: Run docker compose</p> <pre><code>docker compose -p typebot-stack up -d --build\n</code></pre> <p>You can now access Typebot at <code>https://[DOMAIN]/</code></p> <p>Step 4: Shutdown containers</p> <pre><code>docker compose -p typebot-stack down\n</code></pre>"},{"location":"integrations/chat_managers/glific/glific/","title":"Setup Instructions","text":"<p>Below is a tutorial for how to load our FAQ template flow into Glific and connect it to your own AAQ endpoint.</p> <ol> <li> <p>Go to \"Flows\"</p> <p></p> </li> <li> <p>Click \"Import flow\"</p> <p></p> </li> <li> <p>Select a <code>.json</code> file given under <code>chat_managers/glific/</code> in the AAQ repo</p> <p></p> </li> <li> <p>Open the imported flow</p> <p></p> </li> <li> <p>Open the webhook card</p> <p></p> </li> <li> <p>Replace <code>&lt;INSERT_AAQ_URL&gt;</code> with your the URL to your AAQ instance</p> <p></p> </li> <li> <p>Go to headers and replace <code>&lt;INSERT_AAQ_API_KEY&gt;</code> value to your own AAQ API key.</p> <p> </p> </li> <li> <p>Test the flow in the \"Preview\" emulator</p> <p> </p> </li> </ol>"},{"location":"integrations/chat_managers/turn.io/turn/","title":"Setup Instructions","text":"<p>Below is an example of how to connect a Turn.io Journey to AAQ endpoints.</p> <ol> <li> <p>On your Turn.io page, go to the Journey menu.</p> <p></p> </li> <li> <p>Create New Journey.</p> <p></p> </li> <li> <p>Select \"From Scratch\" -&gt; \"Code\".</p> <p></p> </li> <li> <p>Type in your journey title and click \"Next\".</p> <p></p> </li> <li> <p>Copy and paste the contents of    chat_managers/turn.io/llm_response_flow_code_journey.txt in the AAQ repository into the    Journey's code area.</p> <p></p> </li> <li> <p>Replace <code>&lt;INSERT_AAQ_URL&gt;</code> and <code>&lt;INSERT_AAQ_API_KEY&gt;</code> values to your own AAQ URL and    API key.</p> <p></p> </li> <li> <p>Test the bot in the emulator.</p> <p></p> </li> </ol>"},{"location":"blog/archive/2024/","title":"2024","text":""},{"location":"blog/page/2/","title":"Latest updates","text":""},{"location":"blog/archive/2024/page/2/","title":"2024","text":""}]}